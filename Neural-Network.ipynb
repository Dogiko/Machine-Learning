{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def array_sign(array):\n",
    "    # return +1, 0, -1 respect to positive, zero, negtive\n",
    "    return 1.*(array>0) - 1.*(array<0)\n",
    "\n",
    "def column_operate(matrix, threshold = 0.00001):\n",
    "    rm = np.array(matrix) # reduced matrix\n",
    "    fm = np.array(matrix) # filtered matrix\n",
    "    ms = matrix.shape # matrix size\n",
    "    mk = np.ones(matrix.shape) # mask\n",
    "    pv = -1*np.ones((ms[1]), dtype = np.int) # pivots\n",
    "    for t in range(ms[1]):\n",
    "        fm = rm*mk # filtered matrix\n",
    "        if np.abs(fm).max() < threshold:\n",
    "            break\n",
    "        \n",
    "        pr, pc = np.unravel_index(np.abs(fm).argmax(), ms) # pivot row, pivot column\n",
    "        rm[:,pc] /= rm[pr][pc]\n",
    "        multi = np.array(rm[pr])\n",
    "        multi[pc] = 0.\n",
    "        rm -= np.dot(rm[:,pc].reshape((ms[0], 1)), multi.reshape((1, ms[1])))\n",
    "        mk[pr] = 0.\n",
    "        mk[:,pc] = 0.\n",
    "        pv[pc] = pr\n",
    "    \n",
    "    rm = rm[:, pv != -1]\n",
    "    pv = pv[pv != -1]\n",
    "    \n",
    "    return rm, pv\n",
    "\n",
    "def mcmc_normal(targets, drop_t = 10, mean=0., std=1.):\n",
    "    output = np.random.normal(mean, std, targets.shape[1:])\n",
    "    if drop_t>1:\n",
    "        for t in range(1, drop_t):\n",
    "            c = np.random.normal(mean, std, targets.shape[1:]) # candicate\n",
    "            cd = np.sqrt(np.square(np.subtract(targets, c)).sum(axis=tuple(np.arange(1,len(targets.shape)))).min())\n",
    "            # distance of candicate to target\n",
    "            od = np.sqrt(np.square(np.subtract(targets, output)).sum(axis=tuple(np.arange(1,len(targets.shape)))).min())\n",
    "            # distance of currently output to target\n",
    "            if np.random.rand()*od < cd:\n",
    "                output = np.array(c)\n",
    "    \n",
    "    return output\n",
    "\n",
    "class VariableArray():\n",
    "    def __init__(self, size, cs_initial=0.1):\n",
    "        self.v = np.random.normal(0., 1., size) # array values\n",
    "        self.td = np.zeros(self.v.shape) # total derivative, used to descent\n",
    "        self.ltd = None # last total derivative\n",
    "        self.m = np.zeros(self.v.shape) # moving array\n",
    "        self.cs = cs_initial*np.ones(self.v.shape) # component-wise step\n",
    "        self.work = np.ones(self.v.shape) # working components, defult to be fully connected\n",
    "    \n",
    "    def assign_values(self, values, cs_initial=0.1):\n",
    "        self.v = np.array(values)\n",
    "        self.td = np.zeros(self.v.shape)\n",
    "        self.ltd = None\n",
    "        self.m = np.zeros(self.v.shape)\n",
    "        self.cs = cs_initial*np.ones(self.v.shape)\n",
    "        self.work = np.ones(self.v.shape)\n",
    "    \n",
    "    def derivative_assign(self, values):\n",
    "        if values.shape != self.td.shape:\n",
    "            raise ValueError(\"values shape error\")\n",
    "        \n",
    "        self.ltd = np.array(self.td)\n",
    "        self.td = np.array(values)\n",
    "    \n",
    "    def add_row(self, new_row, cs_initial=0.1):\n",
    "        self.v = np.append(self.v, np.array([new_row]), axis = 0)\n",
    "        self.td = np.append(self.td, np.zeros((1,)+new_row.shape), axis = 0)\n",
    "        self.ltd = None\n",
    "        self.m = np.zeros(self.v.shape)\n",
    "        self.cs = np.append(self.cs, cs_initial*np.ones((1,)+new_row.shape), axis = 0)\n",
    "        self.work = np.ones(self.v.shape)\n",
    "    \n",
    "    def add_column(self, new_column, cs_initial=0.1):\n",
    "        self.v = np.append(self.v, np.array([new_column]).T, axis = 1)\n",
    "        self.td = np.append(self.td, np.zeros(new_column.shape + (1,)), axis = 1)\n",
    "        self.ltd = None\n",
    "        self.m = np.zeros(self.v.shape)\n",
    "        self.cs = np.append(self.cs, cs_initial*np.ones(new_column.shape + (1,)), axis = 1)\n",
    "        self.work = np.ones(self.v.shape)\n",
    "    \n",
    "    def max_cs(self):\n",
    "        return self.cs.max()\n",
    "    \n",
    "    def reset_cs(self, new_cs):\n",
    "        self.cs = new_cs*np.ones(self.cs.shape)\n",
    "    \n",
    "    def descent(self, step = 1., descent_method = \"normal\", regularizer = (\"None\",), td_max = 0.1, move_max=1., move_min=0.000001):\n",
    "        if regularizer[0] == \"r_square\":\n",
    "            self.td += regularizer[1] * self.v\n",
    "        \n",
    "        if regularizer[0] == \"rs_extend\":\n",
    "            self.td += regularizer[1] * ((self.v>regularizer[2])*(self.v-regularizer[2]) + (self.v < -regularizer[2])*(self.v+regularizer[2]))\n",
    "        \n",
    "        if descent_method == \"normal\":\n",
    "            self.m = self.td * (np.abs(self.td) < td_max) + array_sign(self.td)*(np.abs(self.td) >= td_max)\n",
    "            self.v -= step * self.m * self.work\n",
    "        elif descent_method == \"Rprop\":\n",
    "            self.m = array_sign(self.td)\n",
    "            self.cs *= 1.2*(self.td*self.ltd>0) + 1.*(self.td*self.ltd==0) + 0.5*(self.td*self.ltd<0)\n",
    "            self.cs = self.cs * (self.cs < move_max) * (self.cs > move_min)+ move_max*(self.cs >= move_max) + move_min*(self.cs <= move_min)\n",
    "            self.v -= self.cs * self.m * self.work\n",
    "        elif descent_method == \"Dogiko Rprop\":\n",
    "            self.m = array_sign(self.td)\n",
    "            step_change = 1.2*(self.td*self.ltd>0.) + 1.*(self.td*self.ltd==0.) + 1.*(self.td==self.ltd)\n",
    "            step_change[step_change == 0.] = self.td[step_change == 0.]/(self.ltd-self.td)[step_change == 0.]\n",
    "            step_change[step_change < 0.1] = 0.1\n",
    "            self.cs *= step_change\n",
    "            self.cs = self.cs * (self.cs < move_max) * (self.cs > move_min)+ move_max*(self.cs >= move_max) + move_min*(self.cs <= move_min)\n",
    "            self.v -= self.cs * self.m * self.work\n",
    "\n",
    "# Activation functions defined start\n",
    "\n",
    "class Identity():\n",
    "    def trans(self, x):\n",
    "        return x\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return np.ones(x.shape, dtype = np.float64)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Sigmoid():\n",
    "    def trans(self, x):\n",
    "        return expit(x)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return expit(x)*expit(-x)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Hypertan():\n",
    "    def trans(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1. / np.square(np.cosh(x))\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Relu():\n",
    "    def trans(self, x):\n",
    "        return x*(x>0)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1.*(x>0)\n",
    "    \n",
    "    def backward(self, x, _input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class LeakyRelu():\n",
    "    def __init__(self, alpha = 0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def trans(self, x):\n",
    "        return x*(x>0) + self.alpha*x*(x<0)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1.*(x>0) + self.alpha*(x<0)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class SoftPlus():\n",
    "    def trans(self, x):\n",
    "        return np.log(1. + np.exp(x))\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return expit(x)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Selu():\n",
    "    def __init__(self):\n",
    "        self.ahpha = 1.05071\n",
    "        self.beta = 1.67326\n",
    "    \n",
    "    def trans(self, x):\n",
    "        return self.ahpha*(x*(x>=0) + self.beta*(np.exp(x) - 1)*(x<0))\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return self.ahpha*(1.*(x>=0) + self.beta*np.exp(x)*(x<0))\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Softmax():\n",
    "    def trans(self, x):\n",
    "        output = x - x.max(axis=0)\n",
    "        output = np.exp(output)\n",
    "        output /= output.sum(axis=0)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, x, _input):\n",
    "        tr = self.trans(x) # result of self.trans\n",
    "        return tr*_input - tr*((tr*_input).sum(axis=0))\n",
    "\n",
    "# Activation functions defined end\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, neuron_n, activation_function):\n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        self.nn = neuron_n\n",
    "        self.af = activation_function\n",
    "        self.w = VariableArray((self.nn, 0)) # linear weights working before active function\n",
    "        self.b = VariableArray((self.nn, 1)) # bias working before active function\n",
    "        self.x = np.zeros((0, self.nn))\n",
    "        self.y = np.zeros((0, self.nn))\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        self.x = np.dot(self.w.v, _input) + self.b.v\n",
    "        self.y = self.af.trans(self.x)\n",
    "    \n",
    "    def backward(self, _input, source):\n",
    "        derivative = self.af.backward(self.x, _input)\n",
    "        self.w.derivative_assign(np.dot(derivative, source.T))\n",
    "        self.b.derivative_assign(np.sum(derivative, axis=1).reshape(derivative.shape[0], 1))\n",
    "        derivative = np.dot(derivative.T, self.w.v)\n",
    "        return derivative.T\n",
    "    \n",
    "    def descent(self, step, descent_method, regularizer):\n",
    "        self.w.descent(step, descent_method, regularizer)\n",
    "        self.b.descent(step, descent_method, regularizer)\n",
    "    \n",
    "    def reset_cs(self, new_cs):\n",
    "        self.w.reset_cs(new_cs)\n",
    "        self.b.reset_cs(new_cs)\n",
    "    \n",
    "    def dimension(self):\n",
    "        return self.w.v.size + self.b.v.size\n",
    "\n",
    "class DogikoLearn():\n",
    "    def __init__(self, loss_function = \"r2\"):\n",
    "        self.lf = loss_function # loss function type\n",
    "        self.ly = [] # layers list\n",
    "        self.rg = (\"None\",) # Regularizetion method\n",
    "        self.csi = 0.1 # initial component-wise step when claim new weights and bias\n",
    "    \n",
    "    def r_square_regularizer(self, alpha):\n",
    "        # Assign regularization method as radius square\n",
    "        # i.e Error += alpha*0.5*sum(weight**2) when descent\n",
    "        if alpha <= 0:\n",
    "            raise ValueError(\"Input should be positive\")\n",
    "        \n",
    "        self.rg = (\"r_square\", alpha)\n",
    "    \n",
    "    def rs_extend_regularizer(self, alpha, beta):\n",
    "        # Assign regularization method as radius square\n",
    "        # i.e Error += alpha*0.5sum(weight**2) when descent\n",
    "        if (alpha <= 0) or (alpha <= 0):\n",
    "            raise ValueError(\"All input should be positive\")\n",
    "        \n",
    "        self.rg = (\"rs_extend\", alpha, beta)\n",
    "    \n",
    "    def set_training_data(self, training_input, training_labels):\n",
    "        self.tx = np.array(training_input) # training data input\n",
    "        self.ty = np.array(training_labels) # training data lables(answers)\n",
    "        if self.tx.shape[0] != self.ty.shape[0]:\n",
    "            temp_min = min(self.tx.shape[0], self.ty.shape[0])\n",
    "            self.tx = self.tx[:temp_min]\n",
    "            self.ty = self.ty[:temp_min]\n",
    "            print(\"training data #input != #output, took the minimun size automatically\")\n",
    "        \n",
    "        self.xs = self.tx.shape[1] # size of each datum input\n",
    "        self.ys = self.ty.shape[1] # size of each datum output\n",
    "    \n",
    "    def set_validation_data(self, validation_input, validation_labels):\n",
    "        self.vx = np.array(validation_input) # validation data input\n",
    "        self.vy = np.array(validation_labels) # validation data lables(answers)\n",
    "        if self.vx.shape[1] != self.xs:\n",
    "            raise ValueError(\"validation data input size should be equal to training data\")\n",
    "        \n",
    "        if self.vy.shape[1] != self.ys:\n",
    "            raise ValueError(\"validation data lables size should be equal to training data\")\n",
    "    \n",
    "    def add_layer(self, new_layer):\n",
    "        if type(new_layer) != Layer:\n",
    "            raise TypeError(\"new_layer should be a Layer (class). eg: 'Layer(30, Sigmoid())'\")\n",
    "        \n",
    "        self.ly.append(new_layer)\n",
    "    \n",
    "    def build(self):\n",
    "        self.ln = len(self.ly) # amount of layers\n",
    "        self.ly[0].w.assign_values(np.random.normal(0., 1., (self.ly[0].nn, self.xs)), self.csi)\n",
    "        self.ly[0].b.assign_values(np.random.normal(0., 1., (self.ly[0].nn, 1)), self.csi)\n",
    "        for l in range(1,self.ln):\n",
    "            self.ly[l].w.assign_values(np.random.normal(0., 1., (self.ly[l].nn, self.ly[l-1].nn)), self.csi)\n",
    "            self.ly[l].b.assign_values(np.random.normal(0., 1., (self.ly[l].nn, 1)), self.csi)\n",
    "        \n",
    "        if self.ly[-1].nn != self.ys: # cheak output size\n",
    "            raise ValueError(\"output layer must has the same size with datum lables(answer)\")\n",
    "    \n",
    "    def prediction(self, data_input):\n",
    "        self.px = np.array(data_input) # prediction data input of last time predic\n",
    "        if self.px.shape[1] != self.xs:\n",
    "            raise ValueError(\"datum size error\")\n",
    "        \n",
    "        self.ly[0].forward(self.px.T)\n",
    "        for l in range(1,self.ln):\n",
    "            self.ly[l].forward(self.ly[l-1].y)\n",
    "        \n",
    "        self.py = np.array(self.ly[-1].y.T) # prediction result of last time predict\n",
    "        \n",
    "        return self.py\n",
    "    \n",
    "    def descent(self, step = 1., descent_method = \"normal\"):\n",
    "        for l in range(self.ln):\n",
    "            self.ly[l].descent(step, descent_method, self.rg)\n",
    "        \n",
    "        if descent_method in [\"Rprop\", \"Dogiko Rprop\"]:\n",
    "            self.max_cs = 0.\n",
    "            for l in range(self.ln):\n",
    "                self.max_cs = max(self.max_cs, self.ly[l].w.max_cs(), self.ly[l].b.max_cs())\n",
    "    \n",
    "    def evaluate(self, _input, labels):\n",
    "        self.prediction(_input)\n",
    "        if self.lf == \"r2\":\n",
    "            return np.square(self.py - labels).mean()/labels.var(axis=0).mean()\n",
    "        elif self.lf == \"ce\":\n",
    "            return (-1*labels*np.log(self.py+0.0001)).sum(axis=1).mean()\n",
    "        else:\n",
    "            raise ValueError(\"loss function should be 'r2' or 'ce'\")\n",
    "    \n",
    "    def gradient_get(self, _input, labels):\n",
    "        self.prediction(_input)\n",
    "        if self.lf == \"r2\":\n",
    "            temp_derivative = 2*(self.py - labels).T/(labels.shape[0]*labels.var(axis=0).sum())\n",
    "        elif self.lf == \"ce\":\n",
    "            temp_derivative = -1*(labels/(self.py + 0.0001)).T/labels.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"loss function should be 'r2' or 'ce'\")\n",
    "        \n",
    "        for l in range(self.ln-1, 0, -1):\n",
    "            temp_derivative = self.ly[l].backward(temp_derivative, self.ly[l-1].y)\n",
    "        \n",
    "        self.ly[0].backward(temp_derivative, _input.T)\n",
    "    \n",
    "    def batch_fit(self, batch_input, batch_labels, step = 1., descent_method = \"normal\"):\n",
    "        self.gradient_get(batch_input, batch_labels)\n",
    "        self.descent(step, descent_method)\n",
    "    \n",
    "    def epoch_fit(self, batch_size = None, step = 1., descent_method = \"normal\"):\n",
    "        if type(batch_size) == type(None):\n",
    "            self.batch_fit(self.tx, self.ty, step, descent_method)\n",
    "        elif type(batch_size) == int:\n",
    "            if batch_size > 0:\n",
    "                for b in range(np.ceil(self.tx.shape[0]/ batch_size)):\n",
    "                    self.batch_fit(self.tx[b*batch_size: (b+1)*batch_size],\n",
    "                                   self.ty[b*batch_size: (b+1)*batch_size],\n",
    "                                   step,\n",
    "                                   descent_method\n",
    "                                  )\n",
    "            else:\n",
    "                raise ValueError(\"batch_size should be positive int\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"batch_size should be positive int\")\n",
    "    \n",
    "    def train(self, times, batch_size = None, step = 1., descent_method = \"normal\"):\n",
    "        for t in range(times):\n",
    "            self.epoch_fit(batch_size, step, descent_method)\n",
    "    \n",
    "    def training_error(self):\n",
    "        return self.evaluate(self.tx, self.ty)\n",
    "    \n",
    "    def validation_error(self):\n",
    "        return self.evaluate(self.vx, self.vy)\n",
    "    \n",
    "    def neuron_refined(self, l, reference_data = None, threshold = 0.01):\n",
    "        # l : the no. of layer\n",
    "        # threshold : threshold for information contained of dimension be remaind\n",
    "        if type(l) != int:\n",
    "            raise TypeError(\"l should be the layer no. of hidden layer, an int between 0 to (neural_number - 2)\")\n",
    "        elif (l >= self.ln - 1) or (l < 0):\n",
    "            raise ValueError(\"l should be the layer no. of hidden layer, an int between 0 to (neural_number - 2)\")\n",
    "        \n",
    "        if type(reference_data) == type(None):\n",
    "            self.prediction(self.tx)\n",
    "        else:\n",
    "            self.prediction(reference_data)\n",
    "        \n",
    "        ym = NN.ly[l].y.mean(axis=1).reshape((NN.ly[l].nn,1)) # y (output of Layer) mean of each neurons\n",
    "        yn = NN.ly[l].y - ym # centralized y\n",
    "        ab = np.dot(NN.ly[l+1].w.v, ym) # Adjusted bias\n",
    "        ev, em = np.linalg.eigh(np.dot(yn, yn.T)) # eigenvalues and eigenmatrix(with eigenvectors as columns)\n",
    "        ir = ev/ev.sum() # info ratio for each eigenvector\n",
    "        # op, pv :column operator result and pivots\n",
    "        if (threshold< 1) and (threshold>0):\n",
    "            op, pv = column_operate(em[:,ir > threshold])\n",
    "        elif (type(threshold) == int) and (threshold != 0):\n",
    "            op, pv = column_operate(em[:,ir > ir[ir.argsort(threshold)]])\n",
    "        else:\n",
    "            raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "            \n",
    "        nw = np.dot(NN.ly[l+1].w.v, op) # new weight\n",
    "        NN.ly[l+1].b.assign_values(NN.ly[l+1].b.v + (np.dot(NN.ly[l+1].w.v, ym) -np.dot(nw, ym[pv])))\n",
    "        NN.ly[l+1].w.assign_values(nw) # l+1 weight should be rewrite after l+1 bias have been rewrite\n",
    "        NN.ly[l].w.assign_values(NN.ly[l].w.v[pv])\n",
    "        NN.ly[l].b.assign_values(NN.ly[l].b.v[pv])\n",
    "        NN.ly[l].nn = len(pv)\n",
    "    \n",
    "    def neuron_proliferate(self, proliferating_layer, proliferating_n = 1, output_weight_bound = 1.):\n",
    "        if proliferating_layer not in range(self.ln):\n",
    "            raise ValueError(\"proliferating_layer should be an int from 0 to (#layer-1)\")\n",
    "            \n",
    "        if type(proliferating_n) != int:\n",
    "            raise ValueError(\"proliferating_n should be int\")\n",
    "        \n",
    "        if proliferating_n <= 0:\n",
    "            raise ValueError(\"proliferating_n should be postive\")\n",
    "            \n",
    "        if output_weight_bound < 0.:\n",
    "            raise ValueError(\"output_weight_bound should be non-negative\")\n",
    "            \n",
    "        l = proliferating_layer\n",
    "        for t in range(proliferating_n):\n",
    "            NN.ly[l].w.add_row(mcmc_normal(NN.ly[l].w.v))\n",
    "            NN.ly[l].b.add_row(mcmc_normal(NN.ly[l].b.v))\n",
    "            NN.ly[l+1].w.add_column(output_weight_bound*(2*np.random.rand((NN.ly[l+1].nn))-1.))\n",
    "            NN.ly[l].nn += 1\n",
    "    \n",
    "    def reset_cs(self, new_cs):\n",
    "        for l in range(self.ln):\n",
    "            self.ly[l].reset_cs(new_cs)\n",
    "    \n",
    "    def inter_layer_linear_regression(self, layer_interval):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                if ls == 0:\n",
    "                    ri = np.array(self.px.T) # regression input\n",
    "                else:\n",
    "                    ri = np.array(self.ly[ls-1].y)\n",
    "                \n",
    "                ri = np.append(ri, np.ones((1, ri.shape[1])), axis=0) # append 1. for each datum as bias\n",
    "                ro = np.array(self.ly[le].x)\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        rr = np.linalg.lstsq(ri.T, ro.T) # regression result (matrix, residuals, rank of ri, singuler values of ri)\n",
    "        if len(rr[1]) == 0:\n",
    "            raise ValueError(\"output data of layer\" + str(ls-1) + \"(= -1, for input data) should be full rank, try self.nruron_refine first\")\n",
    "        \n",
    "        return rr[0], rr[1]/ri.shape[1]\n",
    "    \n",
    "    def find_linearist_layers(self, reference_data = None):\n",
    "        output = (0, 0, np.inf, np.array([[]]), np.zeros((0,0)))\n",
    "        if type(reference_data) == type(None):\n",
    "            self.prediction(self.tx)\n",
    "        else:\n",
    "            self.prediction(reference_data)\n",
    "        \n",
    "        for l1 in range(NN.ln-1):\n",
    "            for l2 in range(i+1, NN.ln):\n",
    "                rr = NN.inter_layer_linear_regression((l1,l2))\n",
    "                if np.sqrt(rr[1].sum()) < output[2]:\n",
    "                    output = (l1, l2, np.sqrt(rr[1].sum()), rr[0])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def layer_filled(self, layer_interval, weights, bias):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        if weights.shape[0] != bias.shape[0]:\n",
    "            raise ValueError(\"weights.shape[0] doesn't match bias.shape[0]\")\n",
    "        \n",
    "        if weights.shape[0] != self.ly[le].nn:\n",
    "            raise ValueError(\"weights.shape[0] doesn't match #neuron of layer at end of layer_interval\")\n",
    "        \n",
    "        self.ly[le].w.assign_values(weights)\n",
    "        self.ly[le].b.assign_values(bias)\n",
    "        self.ly = self.ly[:ls] + self.ly[le:]\n",
    "        self.ln = len(self.ly)\n",
    "    \n",
    "    def linear_filled(self, layer_interval):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "            \n",
    "        rr = NN.inter_layer_linear_regression(layer_interval)\n",
    "        NN.layer_filled(layer_interval, rr[0].T[:,:-1], rr[0].T[:,-1:])\n",
    "    \n",
    "    def insert_layer(self, position, weights, bias, activation_function, next_layer_weights, next_layer_bias):\n",
    "        if type(position) == int:\n",
    "            if position in range(self.ln):\n",
    "                pass\n",
    "        else:\n",
    "            raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        \n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        ilo, ili = weights.shape # input and output size of inserted layer\n",
    "        nlo, nli = next_layer_weights.shape # input and output size of next layer\n",
    "        \n",
    "        if position == 0:\n",
    "            if ili != self.xs:\n",
    "                raise ValueError(\"weights.shape error, cheak input and output size for this new layer\")\n",
    "        else:\n",
    "            if ili != self.ly[position-1].nn:\n",
    "                raise ValueError(\"weights.shape error, cheak input and output size for this new layer\")\n",
    "        \n",
    "        if (ilo != bias.shape[0]) or (ilo != nli):\n",
    "            raise ValueError(\"to define #neuron of new layer, all related weighs and bias size should be consistent\")\n",
    "        \n",
    "        if nlo != self.ly[position].nn:\n",
    "            raise ValueError(\"next_layer_weights.shape error, cheak #neuron of next layer\")\n",
    "        \n",
    "        if next_layer_bias.shape[0] != self.ly[position].nn:\n",
    "            raise ValueError(\"next_layer_bias.shape error, cheak #neuron of next layer\")\n",
    "        \n",
    "        if (bias.shape[1] != 1) or (next_layer_bias.shape[1] != 1):\n",
    "            raise ValueError(\"bias shape should be (#neuron, 1)\")\n",
    "        \n",
    "        l = position\n",
    "        \n",
    "        self.ly.insert(l, Layer(ilo, activation_function))\n",
    "        self.ly[l].w.assign_values(weights)\n",
    "        self.ly[l].b.assign_values(bias)\n",
    "        self.ly[l+1].w.assign_values(next_layer_weights)\n",
    "        self.ly[l+1].b.assign_values(next_layer_bias)\n",
    "        \n",
    "        self.ln = len(self.ly)\n",
    "    \n",
    "    def identity_dig(self, position, activation_function):\n",
    "        if type(position) == int:\n",
    "            if position in range(self.ln):\n",
    "                pass\n",
    "        else:\n",
    "            raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        \n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        l = position\n",
    "        # ids : size of identity transform, input size of new layer\n",
    "        if l == 0:\n",
    "            ids = self.xs\n",
    "        else:\n",
    "            ids = self.ly[l-1].nn\n",
    "        \n",
    "        if type(activation_function) in [Relu, SoftPlus]:\n",
    "            liw = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 0)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 1)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) == LeakyRelu:\n",
    "            liw = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 0)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 1) / (1.+activation_function.alpha)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) == Identity:\n",
    "            liw = np.identity(ids)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.identity(ids)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) in [Sigmoid, Hypertan, Selu]:\n",
    "            # li : input of new layer\n",
    "            if l == 0:\n",
    "                li = np.array(self.tx.T)\n",
    "            else:\n",
    "                li = np.array(self.ly[l-1].y)\n",
    "            \n",
    "            lim = li.mean(axis=1)\n",
    "            lis = li.std(axis=1) + 1.\n",
    "            \n",
    "            liw = np.diag(1./lis)\n",
    "            if type(activation_function) == Selu:\n",
    "                lib = 1.-(lim/lis).reshape(-1,1) # let mean become one before transform by activation function\n",
    "            else:\n",
    "                lib = -(lim/lis).reshape(-1,1) # let mean become zero before transform by activation function\n",
    "            \n",
    "            lo = activation_function.trans(np.dot(liw, li)+lib)\n",
    "            lo = np.append(lo, np.ones((1, lo.shape[1])), axis=0) # append 1. for each datum as bias\n",
    "            rr = np.linalg.lstsq(lo.T, li.T) # regression result (matrix, residuals, rank of ri, singuler values of ri)\n",
    "            # since the goal is construct identity, try to find linear transform form layer output to layer input\n",
    "            low = rr[0].T[:,:-1]\n",
    "            lob = rr[0].T[:,-1:]\n",
    "        else:\n",
    "            raise TypeError(\"activation_function type error\")\n",
    "        \n",
    "        nlw = np.dot(self.ly[l].w.v, low)\n",
    "        nlb = np.dot(self.ly[l].w.v, lob) + self.ly[l].b.v\n",
    "        \n",
    "        self.insert_layer(l,\n",
    "                          liw,\n",
    "                          lib,\n",
    "                          activation_function,\n",
    "                          nlw,\n",
    "                          nlb\n",
    "                         )\n",
    "    \n",
    "    def dimension(self):\n",
    "        output = 0\n",
    "        for l in range(self.ln):\n",
    "            output += self.ly[l].dimension()\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient cheak : case regression\n",
    "\n",
    "For the regression case, using r2 error\n",
    "\n",
    "Cheak backward propagation by computing difference between gradient got from backward propagation and numerical method\n",
    "\n",
    "The difference may become large when r2 error large by this property\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x} x^2 = 2x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.normal(0.,1., (1000,2))\n",
    "Y = np.sqrt((X**2).sum(axis=1)).reshape(-1,1)\n",
    "\n",
    "NN = DogikoLearn(loss_function=\"r2\")\n",
    "NN.set_training_data(X, Y)\n",
    "NN.set_validation_data(X, Y)\n",
    "NN.add_layer(Layer(4, Identity()))\n",
    "NN.add_layer(Layer(4, Sigmoid()))\n",
    "NN.add_layer(Layer(4, Hypertan()))\n",
    "NN.add_layer(Layer(4, Relu()))\n",
    "NN.add_layer(Layer(4, LeakyRelu()))\n",
    "NN.add_layer(Layer(4, SoftPlus()))\n",
    "NN.add_layer(Layer(4, Selu()))\n",
    "NN.add_layer(Layer(1, Identity()))\n",
    "NN.build()\n",
    "\n",
    "threshold = 10**(-4)\n",
    "epsilon = 10**(-8)\n",
    "\n",
    "for l in range(NN.ln):\n",
    "    for wi in range(NN.ly[l].w.v.shape[0]):\n",
    "        for wj in range(NN.ly[l].w.v.shape[1]):\n",
    "            NN.build() # reset weights randomly\n",
    "            NN.gradient_get(X,Y)\n",
    "            pd = NN.ly[l].w.td[wi][wj] # partial derivative by backward propagation\n",
    "            ea = NN.evaluate(X,Y) # error before slightly moving\n",
    "            NN.ly[l].w.v[wi][wj] += epsilon # slightly moving\n",
    "            eb = NN.evaluate(X,Y) # error after slightly moving\n",
    "            if (pd - (eb-ea)/epsilon) > threshold: # if difference large (than threshold), print.\n",
    "                print(pd - ((eb-ea)/epsilon), NN.evaluate(X,Y))\n",
    "            \n",
    "    for bi in range(NN.ly[l].b.v.shape[0]): # bias part\n",
    "        NN.build()\n",
    "        NN.gradient_get(X,Y)\n",
    "        pd = NN.ly[l].b.td[bi][0]\n",
    "        ea = NN.evaluate(X,Y)\n",
    "        NN.ly[l].b.v[bi][0] += epsilon\n",
    "        eb = NN.evaluate(X,Y)\n",
    "        if (pd - (eb-ea)/epsilon) > threshold:\n",
    "            print(pd - ((eb-ea)/epsilon), NN.evaluate(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient cheak : case classification\n",
    "\n",
    "For the case classification, using cross entropy\n",
    "\n",
    "Cheak backward propagation by computing difference between gradient got from backward propagation and numerical method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.normal(0.,1., (1000,2))\n",
    "Y = np.zeros((1000,2))\n",
    "Y[:,0] = (1.*np.sqrt((X**2).sum(axis=1)) > 1.)\n",
    "Y[:,1] = 1. - Y[:,0]\n",
    "\n",
    "NN = DogikoLearn(loss_function=\"ce\")\n",
    "NN.set_training_data(X, Y)\n",
    "NN.set_validation_data(X, Y)\n",
    "NN.add_layer(Layer(4, Identity()))\n",
    "NN.add_layer(Layer(4, Sigmoid()))\n",
    "NN.add_layer(Layer(4, Hypertan()))\n",
    "NN.add_layer(Layer(4, Relu()))\n",
    "NN.add_layer(Layer(4, LeakyRelu()))\n",
    "NN.add_layer(Layer(4, SoftPlus()))\n",
    "NN.add_layer(Layer(4, Selu()))\n",
    "NN.add_layer(Layer(2, Softmax()))\n",
    "NN.build()\n",
    "\n",
    "threshold = 10**(-4)\n",
    "epsilon = 10**(-8)\n",
    "\n",
    "for l in range(NN.ln):\n",
    "    for wi in range(NN.ly[l].w.v.shape[0]):\n",
    "        for wj in range(NN.ly[l].w.v.shape[1]):\n",
    "            NN.build() # reset weights randomly\n",
    "            NN.gradient_get(X,Y)\n",
    "            pd = NN.ly[l].w.td[wi][wj] # partial derivative by backward propagation\n",
    "            ea = NN.evaluate(X,Y) # error before slightly moving\n",
    "            NN.ly[l].w.v[wi][wj] += epsilon # slightly moving\n",
    "            eb = NN.evaluate(X,Y) # error after slightly moving\n",
    "            if (pd - (eb-ea)/epsilon) > threshold: # if difference large (than threshold), print.\n",
    "                print(pd - ((eb-ea)/epsilon), NN.evaluate(X,Y))\n",
    "            \n",
    "    for bi in range(NN.ly[l].b.v.shape[0]): # bias part\n",
    "        NN.build()\n",
    "        NN.gradient_get(X,Y)\n",
    "        pd = NN.ly[l].b.td[bi][0]\n",
    "        ea = NN.evaluate(X,Y)\n",
    "        NN.ly[l].b.v[bi][0] += epsilon\n",
    "        eb = NN.evaluate(X,Y)\n",
    "        if (pd - (eb-ea)/epsilon) > threshold:\n",
    "            print(pd - ((eb-ea)/epsilon), NN.evaluate(X,Y))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "Fit $y = \\sin{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = (np.arange(201)/50) - 2\n",
    "X = X.reshape((201,1))\n",
    "Y = np.sin(2*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfv4/15x70IX1tRkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11063de80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN = DogikoLearn()\n",
    "NN.r_square_regularizer(0.000001)\n",
    "NN.set_training_data(X, Y)\n",
    "NN.set_validation_data(X, Y)\n",
    "NN.add_layer(Layer(30, Hypertan()))\n",
    "NN.add_layer(Layer(1,Identity()))\n",
    "NN.build()\n",
    "\n",
    "NN.train(100, descent_method=\"Rprop\")\n",
    "\n",
    "plt.plot(X.reshape((201)), NN.py.reshape((201)), \"b\", X.reshape((201)), Y.reshape((201)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example 2\n",
    "\n",
    "Net-shape classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAGfCAYAAAAgfbd6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK/5/4bm/515tfMwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10741e710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([np.arange(81).repeat(81), np.arange(81*81) % 81]).astype(np.float64)\n",
    "X -= 40.\n",
    "X /= 40.\n",
    "Y_temp = (((X[0] + X[1] - 0.5) % 2.) > 1.) ^ (((X[0] - X[1] - 0.5) % 2.) > 1.)\n",
    "Y = np.zeros(Y_temp.shape + (2,))\n",
    "Y[:,0] = Y_temp.astype(np.float64)\n",
    "Y[:,1] = 1 - Y[:,0]\n",
    "X = X.T\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(X.T[0][Y[:,0]==1], X.T[1][Y[:,0]==1], \"bp\")\n",
    "plt.plot(X.T[0][Y[:,1]==1], X.T[1][Y[:,1]==1], \"rp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permu = np.random.permutation(np.arange(81*81))\n",
    "\n",
    "X_train = X[permu[:5000]]\n",
    "Y_train = Y[permu[:5000]]\n",
    "X_valid = X[permu[5000:]]\n",
    "Y_valid = Y[permu[5000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0827472841908 0.0916636174127\n"
     ]
    }
   ],
   "source": [
    "NN = DogikoLearn(loss_function=\"ce\")\n",
    "NN.rs_extend_regularizer(0.001,3.)\n",
    "NN.set_training_data(X_train, Y_train)\n",
    "NN.set_validation_data(X_valid, Y_valid)\n",
    "NN.add_layer(Layer(8, Relu()))\n",
    "NN.add_layer(Layer(8, Relu()))\n",
    "NN.add_layer(Layer(2, Softmax()))\n",
    "NN.build()\n",
    "\n",
    "NN.train(10000, descent_method=\"Rprop\")\n",
    "\n",
    "print(NN.training_error(), NN.validation_error())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043810838607703601"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.validation_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, 12, 188 0.092 0.103\n",
      "11, 13, 217 0.059 0.077\n",
      "11, 12, 203 0.051 0.068\n",
      "12, 13, 233 0.039 0.053\n",
      "10, 14, 214 0.038 0.051\n",
      "10, 14, 214 0.035 0.05\n",
      "10, 14, 214 0.036 0.048\n",
      "10, 14, 214 0.034 0.045\n",
      "10, 14, 214 0.03 0.04\n",
      "10, 14, 214 0.028 0.039\n",
      "9, 13, 185 0.035 0.046\n",
      "9, 14, 197 0.033 0.046\n",
      "9, 14, 197 0.033 0.048\n",
      "8, 12, 158 0.045 0.058\n",
      "9, 13, 185 0.043 0.055\n",
      "8, 14, 180 0.041 0.054\n",
      "8, 14, 180 0.038 0.052\n",
      "8, 13, 169 0.04 0.054\n",
      "8, 13, 169 0.037 0.05\n",
      "8, 14, 180 0.035 0.047\n",
      "8, 15, 191 0.034 0.049\n",
      "8, 13, 169 0.035 0.048\n",
      "8, 12, 158 0.041 0.055\n",
      "8, 13, 169 0.036 0.049\n",
      "8, 13, 169 0.033 0.045\n",
      "8, 14, 180 0.032 0.045\n",
      "8, 14, 180 0.03 0.046\n",
      "8, 13, 169 0.034 0.045\n",
      "8, 13, 169 0.032 0.047\n",
      "8, 12, 158 0.03 0.046\n",
      "8, 12, 158 0.03 0.045\n",
      "8, 12, 158 0.028 0.04\n",
      "8, 12, 158 0.027 0.038\n",
      "8, 12, 158 0.027 0.042\n",
      "8, 11, 147 0.03 0.044\n",
      "8, 11, 147 0.029 0.042\n",
      "8, 12, 158 0.028 0.04\n",
      "8, 12, 158 0.027 0.041\n",
      "8, 12, 158 0.028 0.041\n",
      "8, 13, 169 0.026 0.043\n",
      "8, 11, 147 0.027 0.044\n",
      "8, 11, 147 0.024 0.043\n",
      "7, 10, 123 0.04 0.058\n",
      "7, 10, 123 0.038 0.055\n",
      "7, 11, 133 0.036 0.054\n",
      "7, 11, 133 0.035 0.056\n",
      "7, 12, 143 0.035 0.054\n",
      "7, 12, 143 0.034 0.058\n",
      "7, 10, 123 0.034 0.054\n",
      "7, 10, 123 0.034 0.057\n",
      "7, 10, 123 0.034 0.057\n",
      "7, 10, 123 0.034 0.053\n",
      "7, 10, 123 0.034 0.055\n",
      "7, 11, 133 0.033 0.054\n",
      "7, 12, 143 0.032 0.052\n",
      "7, 12, 143 0.03 0.049\n",
      "7, 11, 133 0.031 0.049\n",
      "7, 11, 133 0.029 0.046\n",
      "7, 12, 143 0.029 0.047\n",
      "7, 12, 143 0.029 0.049\n",
      "7, 11, 133 0.031 0.046\n",
      "8, 12, 158 0.027 0.043\n",
      "7, 11, 133 0.03 0.042\n",
      "7, 12, 143 0.027 0.044\n",
      "7, 11, 133 0.027 0.041\n",
      "7, 11, 133 0.027 0.041\n",
      "7, 12, 143 0.026 0.041\n",
      "7, 12, 143 0.026 0.038\n",
      "7, 12, 143 0.025 0.04\n",
      "7, 12, 143 0.026 0.04\n",
      "7, 12, 143 0.025 0.04\n",
      "7, 12, 143 0.025 0.04\n",
      "7, 12, 143 0.025 0.04\n",
      "7, 12, 143 0.025 0.041\n",
      "7, 12, 143 0.025 0.041\n",
      "7, 12, 143 0.026 0.044\n",
      "7, 12, 143 0.025 0.043\n",
      "7, 12, 143 0.025 0.039\n",
      "7, 12, 143 0.025 0.039\n",
      "7, 12, 143 0.025 0.039\n",
      "7, 12, 143 0.024 0.041\n",
      "7, 11, 133 0.024 0.038\n",
      "7, 11, 133 0.024 0.039\n",
      "7, 11, 133 0.025 0.039\n",
      "7, 12, 143 0.025 0.039\n",
      "7, 12, 143 0.025 0.039\n",
      "7, 11, 133 0.024 0.038\n",
      "7, 11, 133 0.023 0.037\n",
      "7, 11, 133 0.023 0.035\n",
      "7, 12, 143 0.024 0.038\n",
      "7, 11, 133 0.023 0.037\n",
      "7, 11, 133 0.023 0.036\n",
      "7, 11, 133 0.023 0.035\n",
      "7, 11, 133 0.025 0.041\n",
      "7, 11, 133 0.024 0.038\n",
      "7, 11, 133 0.023 0.039\n",
      "7, 11, 133 0.023 0.036\n",
      "7, 11, 133 0.023 0.037\n",
      "7, 11, 133 0.025 0.039\n",
      "7, 12, 143 0.024 0.038\n",
      "7, 12, 143 0.023 0.039\n",
      "7, 11, 133 0.023 0.035\n",
      "7, 12, 143 0.023 0.039\n",
      "7, 11, 133 0.023 0.037\n",
      "7, 11, 133 0.025 0.034\n",
      "8, 12, 158 0.022 0.038\n",
      "7, 11, 133 0.023 0.037\n",
      "7, 11, 133 0.022 0.036\n",
      "7, 11, 133 0.022 0.035\n",
      "7, 11, 133 0.022 0.036\n",
      "7, 11, 133 0.023 0.039\n",
      "7, 11, 133 0.023 0.037\n",
      "7, 11, 133 0.023 0.038\n",
      "7, 11, 133 0.022 0.035\n",
      "7, 11, 133 0.022 0.036\n",
      "7, 11, 133 0.023 0.035\n",
      "7, 11, 133 0.023 0.041\n",
      "7, 11, 133 0.023 0.036\n",
      "7, 12, 143 0.022 0.037\n",
      "7, 11, 133 0.023 0.038\n",
      "7, 11, 133 0.023 0.039\n",
      "7, 11, 133 0.023 0.039\n",
      "7, 11, 133 0.023 0.035\n",
      "7, 12, 143 0.023 0.038\n",
      "7, 11, 133 0.022 0.039\n",
      "7, 10, 123 0.023 0.044\n",
      "7, 10, 123 0.023 0.039\n",
      "7, 11, 133 0.023 0.036\n",
      "7, 11, 133 0.023 0.036\n",
      "7, 11, 133 0.023 0.035\n",
      "7, 11, 133 0.022 0.034\n",
      "7, 11, 133 0.022 0.035\n",
      "7, 10, 123 0.022 0.035\n",
      "7, 10, 123 0.024 0.036\n",
      "7, 10, 123 0.022 0.035\n",
      "7, 10, 123 0.023 0.036\n",
      "7, 10, 123 0.022 0.036\n",
      "7, 10, 123 0.022 0.035\n",
      "7, 10, 123 0.022 0.035\n",
      "7, 10, 123 0.022 0.037\n",
      "7, 10, 123 0.023 0.038\n",
      "7, 10, 123 0.021 0.034\n",
      "7, 10, 123 0.022 0.036\n",
      "7, 10, 123 0.022 0.034\n",
      "7, 11, 133 0.022 0.036\n",
      "7, 10, 123 0.022 0.036\n",
      "7, 10, 123 0.022 0.035\n",
      "7, 10, 123 0.022 0.036\n",
      "7, 11, 133 0.022 0.037\n",
      "7, 10, 123 0.021 0.035\n",
      "7, 10, 123 0.022 0.038\n",
      "7, 10, 123 0.022 0.035\n",
      "7, 10, 123 0.022 0.037\n",
      "7, 10, 123 0.023 0.035\n",
      "7, 10, 123 0.023 0.037\n",
      "7, 10, 123 0.022 0.035\n",
      "7, 11, 133 0.022 0.038\n",
      "7, 11, 133 0.022 0.037\n",
      "7, 11, 133 0.022 0.035\n",
      "7, 11, 133 0.023 0.036\n",
      "7, 11, 133 0.022 0.035\n",
      "7, 11, 133 0.022 0.036\n",
      "7, 10, 123 0.022 0.036\n",
      "7, 10, 123 0.022 0.036\n",
      "7, 10, 123 0.022 0.035\n",
      "7, 11, 133 0.022 0.038\n",
      "7, 10, 123 0.022 0.033\n",
      "7, 10, 123 0.022 0.035\n",
      "7, 10, 123 0.021 0.033\n",
      "7, 10, 123 0.022 0.034\n",
      "7, 11, 133 0.022 0.033\n",
      "7, 11, 133 0.021 0.035\n",
      "7, 11, 133 0.022 0.034\n",
      "7, 11, 133 0.024 0.039\n",
      "7, 10, 123 0.022 0.036\n",
      "7, 10, 123 0.022 0.033\n",
      "7, 10, 123 0.022 0.031\n",
      "7, 10, 123 0.022 0.032\n",
      "8, 11, 147 0.017 0.03\n",
      "7, 10, 123 0.021 0.036\n",
      "8, 11, 147 0.022 0.037\n",
      "8, 11, 147 0.023 0.036\n",
      "8, 11, 147 0.022 0.037\n",
      "8, 10, 136 0.021 0.035\n",
      "8, 11, 147 0.022 0.038\n",
      "8, 11, 147 0.021 0.033\n",
      "8, 11, 147 0.021 0.031\n",
      "8, 11, 147 0.021 0.032\n",
      "8, 11, 147 0.02 0.031\n",
      "8, 11, 147 0.02 0.03\n",
      "8, 12, 158 0.02 0.034\n",
      "7, 12, 143 0.021 0.035\n",
      "7, 12, 143 0.022 0.033\n",
      "7, 12, 143 0.021 0.033\n",
      "7, 12, 143 0.021 0.034\n",
      "7, 10, 123 0.021 0.033\n",
      "7, 10, 123 0.02 0.033\n",
      "7, 10, 123 0.02 0.034\n",
      "7, 10, 123 0.02 0.034\n",
      "7, 11, 133 0.022 0.036\n",
      "7, 11, 133 0.022 0.036\n",
      "7, 11, 133 0.021 0.033\n",
      "7, 11, 133 0.02 0.034\n",
      "7, 11, 133 0.021 0.034\n",
      "7, 11, 133 0.021 0.035\n",
      "7, 12, 143 0.022 0.035\n",
      "7, 10, 123 0.021 0.035\n",
      "7, 10, 123 0.021 0.039\n",
      "7, 10, 123 0.021 0.038\n",
      "7, 10, 123 0.021 0.036\n",
      "7, 10, 123 0.022 0.035\n",
      "7, 10, 123 0.021 0.035\n",
      "7, 10, 123 0.021 0.035\n",
      "7, 10, 123 0.041 0.058\n",
      "8, 10, 136 0.032 0.044\n",
      "9, 11, 161 0.026 0.036\n",
      "9, 11, 161 0.024 0.038\n",
      "7, 11, 133 0.025 0.039\n",
      "7, 11, 133 0.025 0.036\n",
      "8, 11, 147 0.02 0.031\n",
      "7, 11, 133 0.024 0.038\n",
      "7, 11, 133 0.025 0.035\n",
      "7, 11, 133 0.024 0.039\n",
      "7, 11, 133 0.026 0.037\n",
      "7, 11, 133 0.024 0.033\n",
      "7, 12, 143 0.023 0.037\n",
      "7, 12, 143 0.023 0.036\n",
      "7, 12, 143 0.023 0.036\n",
      "7, 11, 133 0.022 0.035\n",
      "7, 11, 133 0.022 0.034\n",
      "7, 12, 143 0.023 0.034\n",
      "7, 12, 143 0.022 0.033\n",
      "8, 13, 169 0.022 0.039\n",
      "7, 11, 133 0.031 0.05\n",
      "7, 12, 143 0.033 0.058\n",
      "7, 11, 133 0.034 0.062\n",
      "7, 11, 133 0.034 0.059\n",
      "7, 11, 133 0.036 0.061\n",
      "7, 11, 133 0.036 0.062\n",
      "7, 11, 133 0.036 0.064\n",
      "7, 11, 133 0.034 0.062\n",
      "6, 9, 101 0.214 0.233\n",
      "7, 10, 123 0.091 0.109\n",
      "7, 11, 133 0.07 0.099\n",
      "8, 12, 158 0.048 0.076\n",
      "7, 11, 133 0.048 0.082\n",
      "8, 11, 147 0.042 0.076\n",
      "7, 10, 123 0.039 0.069\n",
      "7, 10, 123 0.036 0.07\n",
      "7, 9, 113 0.046 0.078\n",
      "7, 10, 123 0.043 0.073\n",
      "7, 10, 123 0.041 0.068\n",
      "7, 11, 133 0.038 0.064\n",
      "7, 11, 133 0.038 0.063\n",
      "7, 11, 133 0.037 0.058\n",
      "8, 11, 147 0.031 0.052\n",
      "7, 9, 113 0.041 0.065\n",
      "7, 10, 123 0.036 0.059\n",
      "7, 10, 123 0.035 0.056\n",
      "7, 10, 123 0.034 0.053\n",
      "7, 10, 123 0.033 0.055\n",
      "7, 11, 133 0.033 0.054\n",
      "7, 10, 123 0.035 0.062\n",
      "7, 10, 123 0.031 0.053\n",
      "7, 10, 123 0.032 0.053\n",
      "7, 10, 123 0.031 0.053\n",
      "7, 10, 123 0.032 0.052\n",
      "7, 10, 123 0.031 0.052\n",
      "7, 11, 133 0.03 0.052\n",
      "7, 10, 123 0.031 0.053\n",
      "7, 10, 123 0.031 0.058\n",
      "7, 9, 113 0.032 0.049\n",
      "7, 10, 123 0.034 0.059\n",
      "7, 10, 123 0.034 0.06\n",
      "7, 11, 133 0.034 0.055\n",
      "7, 11, 133 0.032 0.051\n",
      "7, 11, 133 0.029 0.049\n",
      "7, 9, 113 0.035 0.064\n",
      "7, 8, 103 0.138 0.202\n",
      "7, 9, 113 0.105 0.154\n",
      "8, 10, 136 0.055 0.095\n",
      "7, 9, 113 0.081 0.115\n",
      "8, 10, 136 0.054 0.076\n",
      "8, 10, 136 0.054 0.075\n",
      "9, 11, 161 0.041 0.065\n",
      "8, 10, 136 0.044 0.065\n",
      "8, 10, 136 0.044 0.066\n",
      "9, 10, 149 0.038 0.057\n",
      "8, 10, 136 0.041 0.06\n",
      "8, 11, 147 0.038 0.059\n",
      "8, 11, 147 0.038 0.059\n",
      "8, 10, 136 0.035 0.054\n",
      "8, 10, 136 0.034 0.052\n",
      "8, 10, 136 0.036 0.055\n",
      "8, 10, 136 0.036 0.056\n",
      "8, 10, 136 0.035 0.057\n",
      "8, 10, 136 0.048 0.074\n",
      "8, 11, 147 0.042 0.067\n",
      "7, 10, 123 0.05 0.085\n",
      "7, 10, 123 0.048 0.079\n",
      "7, 11, 133 0.044 0.075\n",
      "7, 9, 113 0.047 0.074\n",
      "7, 9, 113 0.045 0.073\n",
      "7, 9, 113 0.044 0.074\n",
      "7, 9, 113 0.041 0.071\n",
      "7, 9, 113 0.04 0.072\n",
      "7, 9, 113 0.04 0.072\n",
      "7, 9, 113 0.04 0.069\n",
      "7, 9, 113 0.053 0.086\n",
      "8, 9, 125 0.043 0.073\n",
      "7, 8, 103 0.046 0.08\n",
      "7, 9, 113 0.044 0.079\n",
      "7, 8, 103 0.043 0.077\n",
      "7, 8, 103 0.044 0.078\n",
      "7, 8, 103 0.045 0.077\n",
      "7, 8, 103 0.044 0.078\n",
      "7, 8, 103 0.045 0.08\n",
      "7, 8, 103 0.059 0.095\n",
      "7, 8, 103 0.045 0.081\n",
      "7, 8, 103 0.044 0.076\n",
      "7, 8, 103 0.057 0.09\n",
      "7, 8, 103 0.056 0.092\n",
      "7, 8, 103 0.054 0.091\n",
      "7, 8, 103 0.042 0.076\n",
      "7, 8, 103 0.042 0.076\n",
      "7, 9, 113 0.04 0.074\n",
      "7, 8, 103 0.043 0.075\n",
      "7, 8, 103 0.04 0.074\n",
      "7, 8, 103 0.039 0.071\n",
      "7, 8, 103 0.049 0.088\n",
      "7, 8, 103 0.052 0.085\n",
      "8, 9, 125 0.047 0.077\n",
      "7, 9, 113 0.046 0.073\n",
      "8, 9, 125 0.039 0.067\n",
      "7, 9, 113 0.043 0.071\n",
      "7, 9, 113 0.037 0.066\n",
      "7, 9, 113 0.038 0.067\n",
      "7, 9, 113 0.036 0.063\n",
      "7, 9, 113 0.035 0.063\n",
      "7, 8, 103 0.051 0.087\n",
      "7, 7, 93 0.053 0.09\n",
      "7, 7, 93 0.051 0.077\n",
      "7, 8, 103 0.04 0.065\n",
      "7, 8, 103 0.04 0.067\n",
      "7, 8, 103 0.038 0.062\n",
      "7, 9, 113 0.036 0.054\n",
      "7, 9, 113 0.035 0.054\n",
      "8, 10, 136 0.031 0.05\n",
      "9, 10, 149 0.028 0.044\n",
      "9, 10, 149 0.027 0.046\n",
      "7, 9, 113 0.032 0.062\n",
      "7, 9, 113 0.032 0.057\n",
      "7, 9, 113 0.033 0.052\n",
      "7, 9, 113 0.031 0.051\n",
      "7, 9, 113 0.032 0.053\n",
      "7, 9, 113 0.03 0.052\n",
      "7, 9, 113 0.03 0.048\n",
      "7, 9, 113 0.029 0.046\n",
      "7, 9, 113 0.029 0.044\n",
      "7, 9, 113 0.029 0.047\n",
      "7, 9, 113 0.029 0.048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7, 10, 123 0.03 0.048\n",
      "7, 10, 123 0.028 0.048\n",
      "7, 11, 133 0.027 0.043\n",
      "7, 11, 133 0.027 0.042\n",
      "7, 11, 133 0.028 0.044\n",
      "7, 11, 133 0.028 0.044\n",
      "7, 10, 123 0.027 0.05\n",
      "7, 9, 113 0.029 0.051\n",
      "7, 8, 103 0.055 0.085\n",
      "7, 9, 113 0.046 0.078\n",
      "7, 9, 113 0.042 0.072\n",
      "7, 9, 113 0.042 0.072\n",
      "7, 8, 103 0.049 0.082\n",
      "7, 8, 103 0.046 0.076\n",
      "8, 8, 114 0.044 0.073\n",
      "8, 8, 114 0.041 0.069\n",
      "7, 8, 103 0.042 0.073\n",
      "7, 9, 113 0.037 0.067\n",
      "7, 8, 103 0.037 0.066\n",
      "7, 8, 103 0.038 0.068\n",
      "7, 8, 103 0.039 0.066\n",
      "7, 8, 103 0.038 0.064\n",
      "7, 9, 113 0.038 0.064\n",
      "7, 9, 113 0.038 0.064\n",
      "7, 9, 113 0.038 0.066\n",
      "7, 9, 113 0.038 0.068\n",
      "7, 9, 113 0.038 0.07\n",
      "7, 9, 113 0.038 0.065\n",
      "7, 10, 123 0.037 0.062\n",
      "7, 9, 113 0.036 0.059\n",
      "7, 10, 123 0.034 0.058\n",
      "7, 10, 123 0.033 0.059\n",
      "7, 9, 113 0.033 0.056\n",
      "7, 10, 123 0.033 0.06\n",
      "7, 9, 113 0.032 0.056\n",
      "8, 9, 125 0.031 0.052\n",
      "7, 8, 103 0.044 0.069\n",
      "8, 9, 125 0.035 0.054\n",
      "8, 9, 125 0.034 0.055\n",
      "8, 9, 125 0.032 0.053\n",
      "7, 9, 113 0.034 0.056\n",
      "7, 9, 113 0.032 0.054\n",
      "7, 9, 113 0.033 0.052\n",
      "7, 10, 123 0.031 0.055\n",
      "7, 9, 113 0.042 0.064\n",
      "7, 10, 123 0.039 0.06\n",
      "7, 9, 113 0.041 0.062\n",
      "7, 10, 123 0.038 0.057\n",
      "7, 11, 133 0.036 0.054\n",
      "7, 10, 123 0.036 0.055\n",
      "7, 9, 113 0.038 0.059\n",
      "7, 9, 113 0.037 0.056\n",
      "7, 10, 123 0.037 0.055\n",
      "7, 11, 133 0.036 0.055\n",
      "8, 12, 158 0.032 0.048\n",
      "7, 12, 143 0.032 0.05\n",
      "7, 11, 133 0.032 0.052\n",
      "7, 11, 133 0.03 0.051\n",
      "7, 10, 123 0.031 0.053\n",
      "7, 10, 123 0.031 0.051\n",
      "7, 11, 133 0.03 0.048\n",
      "7, 11, 133 0.03 0.047\n",
      "7, 11, 133 0.03 0.047\n",
      "7, 12, 143 0.03 0.047\n",
      "7, 12, 143 0.029 0.047\n",
      "7, 12, 143 0.027 0.046\n",
      "7, 12, 143 0.026 0.045\n",
      "7, 11, 133 0.062 0.106\n",
      "7, 11, 133 0.049 0.078\n",
      "7, 11, 133 0.043 0.073\n",
      "7, 11, 133 0.042 0.07\n",
      "7, 11, 133 0.039 0.066\n",
      "7, 10, 123 0.046 0.075\n",
      "7, 10, 123 0.039 0.064\n",
      "7, 10, 123 0.039 0.063\n",
      "7, 11, 133 0.034 0.058\n",
      "7, 9, 113 0.04 0.07\n",
      "8, 9, 125 0.034 0.054\n",
      "7, 9, 113 0.033 0.053\n",
      "7, 9, 113 0.032 0.053\n",
      "7, 9, 113 0.033 0.054\n",
      "7, 9, 113 0.031 0.05\n",
      "7, 10, 123 0.03 0.05\n",
      "7, 10, 123 0.03 0.049\n",
      "7, 10, 123 0.029 0.048\n",
      "7, 10, 123 0.028 0.049\n",
      "7, 9, 113 0.029 0.05\n",
      "7, 9, 113 0.029 0.05\n",
      "7, 9, 113 0.028 0.05\n",
      "7, 9, 113 0.028 0.051\n",
      "7, 9, 113 0.028 0.05\n",
      "7, 9, 113 0.029 0.053\n",
      "7, 9, 113 0.028 0.049\n",
      "7, 9, 113 0.027 0.048\n",
      "7, 8, 103 0.034 0.058\n",
      "7, 8, 103 0.03 0.053\n",
      "7, 8, 103 0.03 0.053\n",
      "7, 8, 103 0.029 0.053\n",
      "7, 8, 103 0.029 0.049\n",
      "7, 8, 103 0.032 0.051\n",
      "8, 9, 125 0.028 0.047\n",
      "8, 8, 114 0.028 0.045\n",
      "8, 8, 114 0.027 0.046\n",
      "8, 9, 125 0.027 0.045\n",
      "8, 10, 136 0.026 0.044\n",
      "8, 10, 136 0.04 0.065\n",
      "7, 9, 113 0.038 0.069\n",
      "7, 9, 113 0.038 0.064\n",
      "7, 10, 123 0.036 0.063\n",
      "7, 10, 123 0.035 0.062\n",
      "7, 9, 113 0.041 0.071\n",
      "7, 9, 113 0.035 0.059\n",
      "7, 9, 113 0.034 0.058\n",
      "7, 9, 113 0.034 0.056\n",
      "7, 9, 113 0.034 0.057\n",
      "7, 9, 113 0.034 0.058\n",
      "7, 10, 123 0.035 0.058\n",
      "8, 11, 147 0.031 0.049\n",
      "7, 10, 123 0.034 0.053\n",
      "7, 10, 123 0.031 0.05\n",
      "7, 10, 123 0.03 0.049\n",
      "7, 10, 123 0.03 0.05\n",
      "7, 10, 123 0.03 0.048\n",
      "7, 9, 113 0.03 0.051\n",
      "8, 10, 136 0.035 0.053\n",
      "9, 11, 161 0.03 0.046\n",
      "8, 10, 136 0.028 0.055\n",
      "6, 6, 74 0.067 0.092\n",
      "7, 7, 93 0.037 0.057\n",
      "8, 8, 114 0.03 0.048\n",
      "7, 9, 113 0.034 0.05\n",
      "7, 10, 123 0.032 0.05\n",
      "6, 11, 119 0.03 0.046\n",
      "7, 12, 143 0.027 0.043\n",
      "7, 11, 133 0.026 0.04\n",
      "7, 11, 133 0.027 0.042\n",
      "7, 10, 123 0.029 0.045\n",
      "7, 11, 133 0.027 0.041\n",
      "7, 11, 133 0.026 0.039\n",
      "7, 11, 133 0.026 0.039\n",
      "7, 12, 143 0.025 0.037\n",
      "7, 12, 143 0.025 0.038\n",
      "8, 11, 147 0.024 0.036\n",
      "9, 11, 161 0.023 0.034\n",
      "9, 11, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3f70789651f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Rprop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ee28578b59fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, times, batch_size, step, descent_method)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ee28578b59fe>\u001b[0m in \u001b[0;36mepoch_fit\u001b[0;34m(self, batch_size, step, descent_method)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mepoch_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ee28578b59fe>\u001b[0m in \u001b[0;36mbatch_fit\u001b[0;34m(self, batch_input, batch_labels, step, descent_method)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ee28578b59fe>\u001b[0m in \u001b[0;36mgradient_get\u001b[0;34m(self, _input, labels)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mtemp_derivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_derivative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_derivative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ee28578b59fe>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, _input, source)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mderivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderivative_assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mderivative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderivative_assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mderivative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mderivative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ee28578b59fe>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, _input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSoftPlus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NN = DogikoLearn(loss_function=\"ce\")\n",
    "NN.rs_extend_regularizer(0.001, 3.)\n",
    "NN.set_training_data(X_train, Y_train)\n",
    "NN.set_validation_data(X_valid, Y_valid)\n",
    "NN.add_layer(Layer(20, LeakyRelu()))\n",
    "NN.add_layer(Layer(20, LeakyRelu()))\n",
    "NN.add_layer(Layer(2, Softmax()))\n",
    "NN.build()\n",
    "\n",
    "counter = np.zeros((5, 1000))\n",
    "\n",
    "for t in range(1000):\n",
    "    for l in range(NN.ln-1):\n",
    "        NN.neuron_proliferate(l, 1, 0.01)\n",
    "\n",
    "    NN.train(500, descent_method=\"Rprop\")\n",
    "    \n",
    "    dr = NN.dimension()/NN.tx.shape[0]\n",
    "    er = NN.validation_error()/NN.training_error()\n",
    "    \n",
    "    for l in range(NN.ln-1):\n",
    "        NN.neuron_refined(l, X_valid, min(1., (er**10)*(dr**3))/(NN.ly[l].nn))\n",
    "        print(NN.ly[l].nn, end=\", \")\n",
    "\n",
    "    NN.train(500, descent_method=\"Rprop\")\n",
    "    \n",
    "    counter[0][t] = NN.ly[0].nn\n",
    "    counter[1][t] = NN.ly[1].nn\n",
    "    counter[2][t] = NN.dimension()\n",
    "    counter[3][t] = np.log(NN.training_error())\n",
    "    counter[4][t] = np.log(NN.validation_error())\n",
    "    \n",
    "\n",
    "    print(NN.dimension(), round(NN.training_error(), 3), round(NN.validation_error(), 3))\n",
    "\n",
    "NN.prediction(X_valid)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(X_valid.T[0][NN.py[:,0]>0.5], X_valid.T[1][NN.py[:,0]>0.5], \"bp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "accuracys_t = np.zeros((100))\n",
    "accuracys_v = np.zeros((100))\n",
    "for t in range(100):\n",
    "    NN = DogikoLearn(loss_function=\"ce\")\n",
    "    NN.rs_extend_regularizer(0.001,3.)\n",
    "    NN.set_training_data(X_train, Y_train)\n",
    "    NN.set_validation_data(X_valid, Y_valid)\n",
    "    NN.add_layer(Layer(10,LeakyRelu()))\n",
    "    NN.add_layer(Layer(10,LeakyRelu()))\n",
    "    NN.add_layer(Layer(2,Softmax()))\n",
    "    NN.build()\n",
    "    \n",
    "    NN.train(1000, descent_method=\"Rprop\")\n",
    "    \n",
    "    accuracys_t[t] = NN.training_error()\n",
    "    accuracys_v[t] = NN.validation_error()\n",
    "    \n",
    "    if (t+1) % 5 ==0:\n",
    "        print(t+1)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.055  0.058  0.058  0.067  0.069  0.071  0.072  0.075  0.076  0.076\n",
      "  0.077  0.079  0.079  0.079  0.08   0.08   0.081  0.081  0.082  0.083\n",
      "  0.084  0.084  0.084  0.085  0.085  0.086  0.087  0.087  0.087  0.088\n",
      "  0.088  0.088  0.089  0.089  0.089  0.09   0.09   0.09   0.091  0.092\n",
      "  0.093  0.094  0.094  0.095  0.095  0.096  0.097  0.098  0.098  0.098\n",
      "  0.099  0.099  0.099  0.101  0.102  0.102  0.102  0.103  0.103  0.104\n",
      "  0.104  0.104  0.104  0.105  0.105  0.107  0.108  0.108  0.109  0.111\n",
      "  0.112  0.113  0.115  0.118  0.119  0.119  0.12   0.12   0.125  0.127\n",
      "  0.127  0.128  0.129  0.131  0.131  0.132  0.133  0.133  0.134  0.134\n",
      "  0.137  0.137  0.138  0.141  0.149  0.174  0.206  0.327  0.384  0.441]\n",
      "0.055 0.086 0.099 0.119 0.441\n"
     ]
    }
   ],
   "source": [
    "accuracys_t.sort()\n",
    "accuracys_t = np.round(accuracys_t, 3)\n",
    "print(accuracys_t)\n",
    "print(accuracys_t.min(), accuracys_t[25], accuracys_t[50], accuracys_t[75], accuracys_t.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.057  0.061  0.063  0.071  0.071  0.072  0.074  0.078  0.078  0.078\n",
      "  0.08   0.081  0.082  0.082  0.082  0.083  0.083  0.084  0.085  0.087\n",
      "  0.087  0.087  0.088  0.088  0.088  0.088  0.089  0.089  0.089  0.09   0.09\n",
      "  0.09   0.09   0.09   0.091  0.091  0.092  0.092  0.092  0.093  0.094\n",
      "  0.094  0.095  0.098  0.098  0.098  0.098  0.098  0.099  0.101  0.101\n",
      "  0.102  0.103  0.103  0.103  0.105  0.106  0.106  0.107  0.107  0.107\n",
      "  0.108  0.108  0.108  0.109  0.109  0.111  0.112  0.113  0.114  0.116\n",
      "  0.116  0.117  0.118  0.121  0.124  0.124  0.125  0.125  0.126  0.126\n",
      "  0.132  0.132  0.133  0.135  0.135  0.135  0.138  0.139  0.139  0.139\n",
      "  0.142  0.146  0.148  0.152  0.18   0.207  0.342  0.396  0.461]\n",
      "0.057 0.088 0.101 0.124 0.461\n"
     ]
    }
   ],
   "source": [
    "accuracys_v.sort()\n",
    "accuracys_v = np.round(accuracys_v, 3)\n",
    "print(accuracys_v)\n",
    "print(accuracys_v.min(), accuracys_v[25], accuracys_v[50], accuracys_v[75], accuracys_v.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 (3, 10) 0.0789749677881\n",
      "0 2 (3, 10) 0.151427476744\n",
      "0 3 (3, 2) 0.128019887719\n",
      "1 2 (11, 10) 0.0163064692087\n",
      "1 3 (11, 2) 0.0221946229495\n",
      "2 3 (11, 2) 0.00461351269551\n"
     ]
    }
   ],
   "source": [
    "NN.validation_error()\n",
    "for i in range(NN.ln-1):\n",
    "    for j in range(i+1, NN.ln):\n",
    "        rr = NN.inter_layer_linear_regression((i,j))\n",
    "        print(i, j, rr[0].shape, (rr[1].sum()/(NN.ly[j].x.var(axis=1).sum()+0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examlple 3\n",
    "Linear case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.normal(0,1, (10000, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.random.normal(0,1, (5,5))\n",
    "while np.linalg.det(A) < 0.5:\n",
    "    A = np.random.normal(0,1, (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.dot(X,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = DogikoLearn(loss_function=\"r2\")\n",
    "NN.rs_extend_regularizer(0.001,10.)\n",
    "NN.set_training_data(X, Y)\n",
    "NN.set_validation_data(X, Y)\n",
    "NN.add_layer(Layer(50,LeakyRelu()))\n",
    "NN.add_layer(Layer(50,LeakyRelu()))\n",
    "NN.add_layer(Layer(50,LeakyRelu()))\n",
    "NN.add_layer(Layer(5,Identity()))\n",
    "NN.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9, 12, 27, 0.0493435386694\n",
      "9, 9, 12, 0.0113445106801\n",
      "10, 8, 12, 0.00755824874162\n",
      "10, 8, 10, 0.00444346400938\n",
      "10, 9, 10, 0.00263462290757\n",
      "12, 9, 10, 0.000664910727598\n",
      "9, 8, 8, 0.00237638763757\n",
      "8, 8, 7, 0.00076685471248\n",
      "7, 8, 7, 0.00224576035833\n",
      "7, 8, 8, 0.00104895855289\n",
      "7, 7, 7, 0.00155323345726\n",
      "8, 7, 7, 0.000591984296887\n",
      "7, 7, 6, 8.92301498747e-05\n",
      "6, 7, 7, 0.00106030334232\n",
      "7, 8, 7, 0.000243928801053\n",
      "7, 8, 7, 0.000579466981622\n",
      "7, 6, 6, 0.00053306100836\n",
      "6, 6, 6, 0.00145979593393\n",
      "6, 7, 6, 0.000504371044989\n",
      "7, 7, 6, 0.000187245110848\n"
     ]
    }
   ],
   "source": [
    "for t in range(20):\n",
    "    for l in range(NN.ln-1):\n",
    "        NN.neuron_proliferate(l, int(0.1*NN.ly[l].nn) + 1)\n",
    "\n",
    "    for i in range(100):\n",
    "        NN.batch_fit(X,Y,step=0.1, descent_method=\"Rprop\")\n",
    "        if NN.max_cs < 0.0001:\n",
    "            break\n",
    "\n",
    "    for l in range(NN.ln-1):\n",
    "        NN.neuron_refined(l, X,((NN.dimension()/(10000))**2)/(NN.ly[l].nn))\n",
    "        print(NN.ly[l].nn, end=\", \")\n",
    "\n",
    "    for i in range(100):\n",
    "        NN.batch_fit(X,Y,step=0.1, descent_method=\"Rprop\")\n",
    "        if NN.max_cs < 0.0001:\n",
    "            break\n",
    "\n",
    "    print(NN.validate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    NN.batch_fit(X,Y,step=0.5, descent_method=\"Rprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, 10\n"
     ]
    }
   ],
   "source": [
    "for l in range(NN.ln-1):\n",
    "    print(NN.ly[l].nn, end= \", \")\n",
    "    NN.neuron_refined(l, None, 0.0001)\n",
    "    print(NN.ly[l].nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1.97488787516\n",
      "0 2 0.198389442508\n",
      "0 3 0.0773856446697\n",
      "1 2 0.0856913391978\n",
      "1 3 0.0664770708499\n",
      "2 3 0.0453495495103\n"
     ]
    }
   ],
   "source": [
    "for i in range(NN.ln-1):\n",
    "    for j in range(i+1, NN.ln):\n",
    "        rr = NN.inter_layer_linear_regression((i,j))\n",
    "        print(i, j, np.sqrt(rr[1].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
