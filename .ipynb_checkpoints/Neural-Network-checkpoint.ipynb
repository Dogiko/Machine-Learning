{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def array_sign(array):\n",
    "    # return +1, 0, -1 respect to positive, zero, negtive\n",
    "    return 1.*(array>0) - 1.*(array<0)\n",
    "\n",
    "class VariableArray():\n",
    "    def __init__(self, size, cs_initial=0.01):\n",
    "        self.v = np.random.normal(0., 1., size) # array values\n",
    "        self.td = np.zeros(self.v.shape) # total derivative, used to descent\n",
    "        self.ltd = None # last total derivative\n",
    "        self.m = np.zeros(self.v.shape) # moving array\n",
    "        self.cs = cs_initial*np.ones(self.v.shape) # component-wise step\n",
    "        self.work = np.ones(self.v.shape) # working components, defult to be fully connected\n",
    "    \n",
    "    def assign_values(self, values, cs_initial=0.01):\n",
    "        self.v = values\n",
    "        self.td = np.zeros(self.v.shape)\n",
    "        self.ltd = None\n",
    "        self.m = np.zeros(self.v.shape)\n",
    "        self.cs = cs_initial*np.ones(self.v.shape)\n",
    "        self.work = np.ones(self.v.shape)\n",
    "    \n",
    "    def derivative_assign(self, values):\n",
    "        if values.shape != self.td.shape:\n",
    "            raise ValueError(\"values shape error\")\n",
    "        \n",
    "        self.ltd = np.array(self.td)\n",
    "        self.td = values\n",
    "    \n",
    "    def max_cs(self):\n",
    "        return self.cs.max()\n",
    "    \n",
    "    def descent(self, step = 1., descent_method = \"normal\", regularizer = (\"None\",),step_max=1., step_min=0.000001):\n",
    "        if regularizer[0] == \"r_square\":\n",
    "            self.td += regularizer[1] * self.v\n",
    "        \n",
    "        if descent_method == \"normal\":\n",
    "            self.m = np.array(self.td)\n",
    "            self.v -= step * self.m * self.work\n",
    "        elif descent_method == \"Rprop\":\n",
    "            self.m = array_sign(self.td)\n",
    "            self.cs *= 1.2*(self.td*self.ltd>0) + 1.*(self.td*self.ltd==0) + 0.5*(self.td*self.ltd<0)\n",
    "            self.cs = self.cs * (self.cs < step_max) * (self.cs > step_min)+ step_max*(self.cs >= step_max) + step_min*(self.cs <= step_min)\n",
    "            self.v -= self.cs * self.m * self.work\n",
    "        elif descent_method == \"Dogiko Rprop\":\n",
    "            self.m = array_sign(self.td)\n",
    "            step_change = 1.2*(self.td*self.ltd>0.) + 1.*(self.td*self.ltd==0.) + 1.*(self.td==self.ltd)\n",
    "            step_change[step_change == 0.] = self.td[step_change == 0.]/(self.ltd-self.td)[step_change == 0.]\n",
    "            step_change[step_change < 0.1] = 0.1\n",
    "            self.cs *= step_change\n",
    "            \n",
    "            self.cs = self.cs * (self.cs < step_max) * (self.cs > step_min)+ step_max*(self.cs >= step_max) + step_min*(self.cs <= step_min)\n",
    "            self.v -= self.cs * self.m * self.work\n",
    "\n",
    "# Activation functions defined start\n",
    "\n",
    "class Identity():\n",
    "    def trans(self, x):\n",
    "        return x\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return np.ones(x.shape, dtype = np.float64)\n",
    "\n",
    "class Sigmoid():\n",
    "    def trans(self, x):\n",
    "        return expit(x)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return expit(x)*expit(-x)\n",
    "\n",
    "class Hypertan():\n",
    "    def trans(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1. / np.square(np.cosh(x))\n",
    "\n",
    "class Relu():\n",
    "    def trans(self, x):\n",
    "        return x*(x>0)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1.*(x>0)\n",
    "\n",
    "class LeakyRelu():\n",
    "    def __init__(self, alpha = 0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def trans(self, x):\n",
    "        return x*(x>0) + self.alpha*x*(x<0)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1.*(x>0) + self.alpha*(x<0)\n",
    "\n",
    "class SoftPlus():\n",
    "    def trans(self, x):\n",
    "        return np.log(1. + np.exp(x))\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return expit(x)\n",
    "\n",
    "class Selu():\n",
    "    def __init__(self):\n",
    "        self.ahpha = 1.05071\n",
    "        self.beta = 1.67326\n",
    "    \n",
    "    def trans(self, x):\n",
    "        return self.ahpha*(x*(x>0) + self.beta*(np.exp(x) - 1)*(x<0))\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return self.ahpha*((x>0) + self.beta*np.exp(x)*(x<0))\n",
    "\n",
    "# Activation functions defined end\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, neuron_n, activation_function):\n",
    "        self.nn = neuron_n\n",
    "        self.af = activation_function\n",
    "        self.w = VariableArray((self.nn, 0)) # linear weights working before active function\n",
    "        self.b = VariableArray((self.nn)) # bias working before active function\n",
    "        self.x = np.zeros((0, self.nn))\n",
    "        self.y = np.zeros((0, self.nn))\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        temp_dn = _input.shape[1] # _input datum n\n",
    "        self.x = np.dot(self.w.v, _input) + self.b.v.repeat(temp_dn).reshape((self.nn, temp_dn))\n",
    "        self.y = self.af.trans(self.x)\n",
    "    \n",
    "    def backward(self, _input, source):\n",
    "        derivative = self.af.diff(self.x)*_input\n",
    "        self.w.derivative_assign(np.dot(derivative, source.T))\n",
    "        self.b.derivative_assign(np.sum(derivative, axis=1))\n",
    "        derivative = np.dot(derivative.T, self.w.v)\n",
    "        return derivative.T\n",
    "    \n",
    "    def descent(self, step, descent_method, regularizer):\n",
    "        self.w.descent(step, descent_method, regularizer)\n",
    "        self.b.descent(step, descent_method, regularizer)\n",
    "\n",
    "class DogikoLearn():\n",
    "    def __init__(self, loss_function = \"r2\"):\n",
    "        self.lf = loss_function # loss function type\n",
    "        self.ly = [] # layers list\n",
    "        self.rg = (\"None\",) # Regularizetion method\n",
    "    \n",
    "    def r_square_regularizer(self, alpha):\n",
    "        # Assign regularization method as radius square\n",
    "        # i.e Error += alpha*0.5sum(weight**2) when descent\n",
    "        self.rg = (\"r_square\", alpha)\n",
    "    \n",
    "    def set_training_data(self, training_input, training_labels):\n",
    "        self.tx = np.array(training_input) # training data input\n",
    "        self.ty = np.array(training_labels) # training data lables(answers)\n",
    "        if self.tx.shape[0] != self.ty.shape[0]:\n",
    "            temp_min = min(self.tx.shape[0], self.ty.shape[0])\n",
    "            self.tx = self.tx[:temp_min]\n",
    "            self.ty = self.ty[:temp_min]\n",
    "            print(\"training data #input != #output, took the minimun size automatically\")\n",
    "        \n",
    "        self.xs = self.tx.shape[1] # size of each datum input\n",
    "        self.ys = self.ty.shape[1] # size of each datum output\n",
    "    \n",
    "    def set_validating_data(self, validating_input, validating_labels):\n",
    "        self.vx = np.array(validating_input) # validating data input\n",
    "        self.vy = np.array(validating_labels) # validating data lables(answers)\n",
    "        if self.vx.shape[1] != self.xs:\n",
    "            raise ValueError(\"validating data input size should be equal to training data\")\n",
    "        \n",
    "        if self.vy.shape[1] != self.ys:\n",
    "            raise ValueError(\"validating data lables size should be equal to training data\")\n",
    "    \n",
    "    def add_layer(self, new_layer):\n",
    "        self.ly.append(new_layer)\n",
    "    \n",
    "    def build(self):\n",
    "        self.ln = len(self.ly) # amount of layers\n",
    "        self.ly[0].w.assign_values(np.random.normal(0., 1., (self.ly[0].nn, self.xs)))\n",
    "        self.ly[0].b.assign_values(np.random.normal(0., 1., (self.ly[0].nn)))\n",
    "        for l in range(1,self.ln):\n",
    "            self.ly[l].w.assign_values(np.random.normal(0., 1., (self.ly[l].nn, self.ly[l-1].nn)))\n",
    "            self.ly[l].b.assign_values(np.random.normal(0., 1., (self.ly[l].nn)))\n",
    "        \n",
    "        if self.ly[-1].nn != self.ys: # cheak output size\n",
    "            raise ValueError(\"output layer must has the same size with datum lables(answer)\")\n",
    "    \n",
    "    def prediction(self, data_input):\n",
    "        self.px = np.array(data_input) # prediction data input of last time predic\n",
    "        if self.px.shape[1] != self.xs:\n",
    "            raise ValueError(\"datum size error\")\n",
    "        \n",
    "        self.ly[0].forward(self.px.T)\n",
    "        for l in range(1,self.ln):\n",
    "            self.ly[l].forward(self.ly[l-1].y)\n",
    "        \n",
    "        self.py = self.ly[l].y.T # prediction result of last time predict\n",
    "    \n",
    "    def descent(self, step = 1., descent_method = \"normal\"):\n",
    "        for l in range(self.ln):\n",
    "            self.ly[l].descent(step, descent_method, self.rg)\n",
    "        \n",
    "        if descent_method in [\"Rprop\", \"Dogiko Rprop\"]:\n",
    "            self.max_cs = 0.\n",
    "            for l in range(self.ln):\n",
    "                self.max_cs = max(self.max_cs, self.ly[l].w.max_cs(), self.ly[l].b.max_cs())\n",
    "    \n",
    "    def batch_fit(self, batch_input, batch_labels, step = 1., descent_method = \"normal\"):\n",
    "        self.prediction(batch_input)\n",
    "        if self.lf == \"r2\":\n",
    "            temp_derivative = 2*(self.py - batch_labels).T/(batch_labels.shape[0]*batch_labels.var(axis=0).sum())\n",
    "        \n",
    "        for l in range(self.ln-1, 0, -1):\n",
    "            temp_derivative = self.ly[l].backward(temp_derivative, self.ly[l-1].y)\n",
    "        \n",
    "        self.ly[0].backward(temp_derivative, batch_input.T)\n",
    "        self.descent(step, descent_method)\n",
    "    \n",
    "    def validate(self):\n",
    "        self.prediction(self.vx)\n",
    "        if self.lf == \"r2\":\n",
    "            return np.square(self.py - self.vy).mean()/self.vy.var(axis=0).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "Fit $y = \\sin{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = (np.arange(81)/20) - 2\n",
    "X = X.reshape((81,1))\n",
    "Y = np.sin(2*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.272735597249\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3362 into shape (81,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9c2d7d4bfa1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m81\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m81\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m81\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m81\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3362 into shape (81,)"
     ]
    }
   ],
   "source": [
    "NN = DogikoLearn()\n",
    "NN.r_square_regularizer(0.001)\n",
    "NN.set_training_data(X, Y)\n",
    "NN.set_validating_data(X, Y)\n",
    "NN.add_layer(Layer(100,Relu()))\n",
    "NN.add_layer(Layer(1,Identity()))\n",
    "NN.build()\n",
    "\n",
    "counter = 0\n",
    "for i in range(1000):\n",
    "    NN.batch_fit(X,Y,step=0.02, descent_method=\"Dogiko Rprop\")\n",
    "    if NN.max_cs < 0.001:\n",
    "        counter +=1\n",
    "        if counter ==10:\n",
    "            print(i)\n",
    "            break\n",
    "    else:\n",
    "        counter =0\n",
    "\n",
    "print(NN.validate())\n",
    "\n",
    "plt.plot(X.reshape((81)), NN.py.reshape((81)), \"b\", X.reshape((81)), Y.reshape((81)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example 2\n",
    "\n",
    "Net-shape classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAEyCAYAAACs1IiIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a5811d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([np.arange(41).repeat(41), np.arange(41*41) % 41]).astype(np.float64)\n",
    "X -= 20.\n",
    "X /= 20.\n",
    "Y = (((X[0] + X[1] - 0.5) % 2.) > 1.) ^ (((X[0] - X[1] - 0.5) % 2.) > 1.)\n",
    "Y = Y.reshape((Y.shape + (1,))).astype(np.float64)\n",
    "X = X.T\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(X.T[0][Y.T[0] == 1], X.T[1][Y.T[0] == 1], \"bp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.231129767502\n",
      "0.0645040762391\n",
      "0.0811404206686\n",
      "0.0198180553774\n",
      "0.0738413379759\n",
      "0.0157255571232\n",
      "0.0135676722807\n",
      "0.0264586864482\n",
      "0.00526645726273\n",
      "0.0259227415441\n",
      "0.010785871702\n",
      "0.00399392238243\n",
      "0.00868147369314\n",
      "0.00563475143532\n",
      "0.0130235878917\n",
      "0.0113767508729\n",
      "0.00658376786627\n",
      "0.0270057118523\n",
      "0.0108529899098\n",
      "0.00523388788088\n",
      "0.00438871438561\n",
      "0.00381005084854\n",
      "0.0135676722807\n",
      "0.0157255571232\n",
      "0.0381005084854\n",
      "0.00868147369314\n",
      "0.00868147369314\n",
      "0.00785166219947\n",
      "0.00358982786706\n",
      "0.00910043815\n",
      "0.00948062572743\n",
      "0.00317504237378\n",
      "0.00290740797444\n",
      "3366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13670066202535941"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = DogikoLearn()\n",
    "NN.r_square_regularizer(0.01)\n",
    "NN.set_training_data(X, Y)\n",
    "NN.set_validating_data(X, Y)\n",
    "NN.add_layer(Layer(50,Selu()))\n",
    "NN.add_layer(Layer(50,Selu()))\n",
    "NN.add_layer(Layer(20,Selu()))\n",
    "NN.add_layer(Layer(1,Sigmoid()))\n",
    "NN.build()\n",
    "\n",
    "for i in range(10000):\n",
    "    NN.batch_fit(X,Y,step=0.1, descent_method=\"Dogiko Rprop\")\n",
    "    if NN.max_cs < 0.001:\n",
    "        print(i)\n",
    "        break\n",
    "    \n",
    "    if (i+1) % 100 == 0:\n",
    "        print(NN.max_cs)\n",
    "\n",
    "NN.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
