{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def IsNone(target):\n",
    "    return (type(target) == type(None))\n",
    "\n",
    "def ArraySign(input_array):\n",
    "    # return +1, 0, -1 respect to positive, zero, negtive\n",
    "    return 1.*(input_array>0) - 1.*(input_array<0)\n",
    "\n",
    "def CutValue(input_array, cut_value):\n",
    "    output = np.abs(input_array)\n",
    "    output = ArraySign(input_array) * (output * (output < cut_value) + cut_value * (output >= cut_value))\n",
    "    return output\n",
    "\n",
    "def WeightedSum(input_array, weight):\n",
    "    try:\n",
    "        return (input_array.reshape(-1)*weight).sum()\n",
    "    except:\n",
    "        raise ValueError(\"weight should be an 1-d array with the same size with input_array\")\n",
    "\n",
    "def WeightedRow(input_array, weight):\n",
    "    try:\n",
    "        return input_array*weight.reshape(-1,1)\n",
    "    except:\n",
    "        raise ValueError(\"weight should be an 1-d array with the same length with first shape of input_array\")\n",
    "\n",
    "def OverPenalty(input_value, rate = 0.1, threshold=0.):\n",
    "    output = np.abs(input_value) - threshold\n",
    "    output *= (output > 0)\n",
    "    output *= rate * ArraySign(input_value)\n",
    "    return output\n",
    "\n",
    "def RowOperate(matrix, threshold = 0.000000000000001, large_element_alert=False):\n",
    "    # matrix : matrix with rows > columns\n",
    "    reduced_matrix = np.array(matrix)\n",
    "    filtered_matrix = np.array(matrix)\n",
    "    shape = matrix.shape # matrix size\n",
    "    mask = np.ones(shape)\n",
    "    pivots = -1*np.ones((min(shape)), dtype = np.int) # store pivots, (# of pivots) <= min(rows, columns)\n",
    "    for p in range(len(pivots)):\n",
    "        filtered_matrix = reduced_matrix * mask # filter\n",
    "        if np.abs(filtered_matrix).max() < threshold:\n",
    "            print(\"WARNING : input rows not independ for threshold %d when apply row operation\" %(threshold))\n",
    "            break\n",
    "        \n",
    "        pivot_row, pivot_col = np.unravel_index(np.abs(filtered_matrix).argmax(), shape) # pivot row, pivot column\n",
    "        reduced_matrix[pivot_row] /= reduced_matrix[pivot_row][pivot_col]\n",
    "        multi = np.array(reduced_matrix[:, pivot_col])\n",
    "        multi[pivot_row] = 0.\n",
    "        reduced_matrix -= np.dot(multi.reshape((-1, 1)), reduced_matrix[pivot_row].reshape((1, -1)))\n",
    "        mask[pivot_row] = 0.\n",
    "        mask[:, pivot_col] = 0.\n",
    "        pivots[pivot_row] = pivot_col # the column-index of pivot_row-th row is pivot_col\n",
    "    \n",
    "    reduced_matrix = reduced_matrix[pivots != -1,:]\n",
    "    pivots = pivots[pivots != -1]\n",
    "    if large_element_alert:\n",
    "        if np.abs(reduced_matrix).max() > 1.01:\n",
    "            print(\"WARNING : reduced matrix has large element %f\" %(np.abs(reduced_matrix).max()))\n",
    "        \n",
    "    \n",
    "    return reduced_matrix, pivots\n",
    "\n",
    "def LinearRefine(regressor, response, num_elimination=1, regularizer=0.):\n",
    "    if (regressor.ndim != 2) or (regressor.size == 0):\n",
    "        raise ValueError(\"regressor should be a non-empty numpy matrix\")\n",
    "    elif (response.ndim != 2) or (response.size == 0):\n",
    "        raise ValueError(\"response should be a non-empty numpy matrix\")\n",
    "    elif len(regressor) != len(response):\n",
    "        raise ValueError(\"len(regressor) != len(response)\")\n",
    "    \n",
    "    if regularizer < 0.:\n",
    "        regularizer = 0.\n",
    "        print(\"SetRegularizer error, regularizer must be non-negative, has been set to zero.\")\n",
    "    \n",
    "    num_var = regressor.shape[1]\n",
    "    regressor = np.append(regressor, np.ones((len(regressor), 1)), axis=1) # add bias\n",
    "    gram = np.dot(regressor.T, regressor)\n",
    "    gram += regularizer * regressor.shape[0] * np.identity(regressor.shape[1])\n",
    "    projected = np.dot(regressor.T, response)\n",
    "    is_leave = np.ones((gram.shape[0]), dtype=bool)\n",
    "    response_square = np.square(response).sum(axis=0)\n",
    "    for d in range(num_elimination):\n",
    "        square_err = np.inf * np.ones((gram.shape[0]), dtype=bool)\n",
    "        for i in range(num_var):\n",
    "            if is_leave[i]:\n",
    "                work_index = (is_leave * (np.arange(len(is_leave)) != i)).astype(bool)\n",
    "                coe = np.linalg.solve(gram[work_index][:, work_index], projected[work_index])\n",
    "                square_err[i] = (response_square\n",
    "                                 - (coe * projected[work_index]).sum(axis=0) \n",
    "                                 - regularizer * np.square(coe).sum(axis=0)\n",
    "                                ).sum()\n",
    "        \n",
    "        is_leave[np.argmin(square_err)] = False\n",
    "    \n",
    "    rms = square_err.min()/(regressor.shape[0] - is_leave.sum())\n",
    "    coe = np.linalg.solve(gram[is_leave][:, is_leave], projected[is_leave])\n",
    "    bias = coe[-1].reshape(1, -1)\n",
    "    return is_leave[:-1], coe[:-1], bias, rms\n",
    "\n",
    "def McmcNormal(points, drop_times = 10, mean=0., std=1.):\n",
    "    # Useing Markov chain Monte Carlo method to get a new point from normal distribution with given points\n",
    "    # each element is get from mean and std\n",
    "    output = np.random.normal(mean, std, points.shape[1:])\n",
    "    if drop_times > 1:\n",
    "        for t in range(1, drop_times):\n",
    "            candicate = np.random.normal(mean, std, points.shape[1:])\n",
    "            candicate_distance = np.sqrt(np.square(np.subtract(points, candicate)).sum(axis=tuple(np.arange(1, len(points.shape))))).min()\n",
    "            # distance of candicate to target\n",
    "            output_distance = np.sqrt(np.square(np.subtract(points, output)).sum(axis=tuple(np.arange(1, len(points.shape))))).min()\n",
    "            # distance of currently output to target\n",
    "            if np.random.rand()*output_distance < candicate_distance:\n",
    "                output = np.array(candicate)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def McmcColExtend(input_matrix, drop_times, extend_min=0.):\n",
    "    # return a matrix with a new col with same level by McmcNormal\n",
    "    input_matrix = input_matrix.T # transpose cols to rows\n",
    "    output = np.insert(input_matrix,\n",
    "                       len(input_matrix),\n",
    "                       McmcNormal(input_matrix, drop_times, 0, max(extend_min, np.sqrt(np.square(input_matrix).mean()))),\n",
    "                       axis=0\n",
    "                      ).T # transpose rows back to cols\n",
    "    return output\n",
    "\n",
    "# Data\n",
    "\n",
    "class Data():\n",
    "    def __init__(self, inputs=np.zeros((0,0)), labels=np.zeros((0,0)), weight = None):\n",
    "        self.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetData(self, inputs=np.zeros((0,0)), labels=np.zeros((0,0)), weight = None):\n",
    "        if len(inputs) != len(labels):\n",
    "            raise ValueError(\"num_datums error, #inputs != #labels.\")\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        if IsNone(weight):\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "        elif weight.shape != (len(inputs)):\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "            print(\"WARNING : weight shape error, set uniform weight.\")\n",
    "        elif weight.sum() <= 0:\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "            print(\"WARNING : get non-positive weight sum, set uniform weight.\")\n",
    "        else:\n",
    "            self.weight = weight\n",
    "        \n",
    "        self.weight /= self.weight.sum()\n",
    "    \n",
    "    def GetNumDatums(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def GetDatumSize(self):\n",
    "        # return size of input and label\n",
    "        return (self.inputs.shape[1], self.labels.shape[1])\n",
    "    \n",
    "    def Shuffle(self):\n",
    "        # shuffling datumds\n",
    "        new_index = np.arange(self.GetNumDatums())\n",
    "        np.random.shuffle(new_index)\n",
    "        self.inputs = self.inputs[new_index]\n",
    "        self.labels = self.labels[new_index]\n",
    "    \n",
    "    def IsClassification(self):\n",
    "        # cheaking if labels of this data is classification\n",
    "        # By cheaking:\n",
    "        # 1. labels have only two value : 0, 1\n",
    "        # 2. two or more classes\n",
    "        # 3. each datum has unique 1\n",
    "        output = ((self.labels == 0) + (self.labels == 1)).all()\n",
    "        output *= (self.labels.shape[1]>1)\n",
    "        output *= (self.labels.sum(axis=1)==1).all()\n",
    "        return output\n",
    "\n",
    "# Data end\n",
    "\n",
    "# VariableArray\n",
    "\n",
    "class VariableArray():\n",
    "    def __init__(self, size, cwise_step_initial=0.1):\n",
    "        self.value = np.random.normal(0., 1., size) # array value\n",
    "        self.total_deri = np.zeros(self.value.shape) # total derivative, used to descent\n",
    "        self.last_total_deri = np.zeros(self.value.shape) # last total derivative\n",
    "        self.moving = np.zeros(self.value.shape) # moving array\n",
    "        self.cwise_step = cwise_step_initial*np.ones(self.value.shape) # component-wise step\n",
    "        \n",
    "        self.regulariz_rate = 0.\n",
    "        self.regulariz_margin = 0.\n",
    "    \n",
    "    def SetValue(self, input_value, cwise_step_initial=0.1):\n",
    "        self.value = np.array(input_value) # array value\n",
    "        self.total_deri = np.zeros(self.value.shape) # total derivative, used to descent\n",
    "        self.last_total_deri = np.zeros(self.value.shape) # last total derivative\n",
    "        self.moving = np.zeros(self.value.shape) # moving array\n",
    "        self.cwise_step = cwise_step_initial*np.ones(self.value.shape) # component-wise step\n",
    "    \n",
    "    def SetDeri(self, input_value):\n",
    "        if input_value.shape != self.total_deri.shape:\n",
    "            raise ValueError(\"input_value shape error\")\n",
    "        \n",
    "        self.total_deri = np.array(input_value)\n",
    "    \n",
    "    def DeriModify(self, input_value):\n",
    "        if input_value.shape != self.total_deri.shape:\n",
    "            raise ValueError(\"input_value shape error\")\n",
    "        \n",
    "        self.total_deri += input_value\n",
    "    \n",
    "    def ZeroDeri(self):\n",
    "        self.total_deri *= 0\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        self.regulariz_rate = max(rate, 0.)\n",
    "        self.regulariz_margin = max(margin, 0.)\n",
    "    \n",
    "    def ResetCwiseStep(self, new_cwise_step):\n",
    "        self.cwise_step = new_cwise_step * np.ones(self.cwise_step.shape)\n",
    "    \n",
    "    def Regularize(self):\n",
    "        if self.regulariz_rate != 0:\n",
    "            self.total_deri += OverPenalty(self.value, self.regulariz_rate, self.regulariz_margin)\n",
    "    \n",
    "    def Descent(self, step=1., method=\"normal\", move_max=1.):\n",
    "        self.Regularize()\n",
    "        if method == \"normal\":\n",
    "            self.moving = self.total_deri * step\n",
    "            self.moving = -1*CutValue(self.moving, move_max)\n",
    "        elif method == \"Rprop\":\n",
    "            self.moving = ArraySign(self.total_deri)\n",
    "            self.movint_return = ArraySign(self.total_deri*self.last_total_deri)\n",
    "            self.cwise_step *= 1.2*(self.movint_return>0) + 1.*(self.movint_return==0) + 0.5*(self.movint_return<0)\n",
    "            self.cwise_step = CutValue(self.cwise_step, move_max)\n",
    "            self.moving *= -1*self.cwise_step\n",
    "        else:\n",
    "            raise ValueError(\"descent method error\")\n",
    "        \n",
    "        self.value += self.moving\n",
    "        \n",
    "        self.last_total_deri = np.array(self.total_deri)\n",
    "        self.ZeroDeri()\n",
    "\n",
    "# VariableArray end\n",
    "\n",
    "# Activation functions defined start\n",
    "\n",
    "class Identity():\n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return np.ones(flow_in.shape, dtype = np.float64)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Sigmoid():\n",
    "    def Forward(self, flow_in):\n",
    "        return expit(flow_in)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return expit(flow_in)*expit(-flow_in)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Hypertan():\n",
    "    def Forward(self, flow_in):\n",
    "        flow_in = CutValue(flow_in, 100)\n",
    "        return np.tanh(flow_in)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        flow_in = CutValue(flow_in, 100) # cut value out of [-100, 100] to 100, cosh(-100) = cosh(100)\n",
    "        return 1. / np.square(np.cosh(flow_in))\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class SoftSign():\n",
    "    def Forward(self, flow_in):\n",
    "        return ArraySign(flow_in)*(1. - 1./(np.abs(flow_in) + 1.))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1. / np.square(np.abs(flow_in) + 1.)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Relu():\n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in*(flow_in>0)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1.*(flow_in>0)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class LeakyRelu():\n",
    "    def __init__(self, alpha = 0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in*(flow_in>0) + self.alpha*flow_in*(flow_in<0)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1.*(flow_in>0) + self.alpha*(flow_in<0)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class SoftPlus():\n",
    "    def Forward(self, flow_in):\n",
    "        return np.log(1. + np.exp(flow_in))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return expit(flow_in)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Selu():\n",
    "    def __init__(self):\n",
    "        self.ahpha = 1.05071\n",
    "        self.beta = 1.67326\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        return self.ahpha*(flow_in*(flow_in>=0) + self.beta*(np.exp(flow_in) - 1)*(flow_in<0))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return self.ahpha*(1.*(flow_in>=0) + self.beta*np.exp(flow_in)*(flow_in<0))\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Softmax():\n",
    "    def Forward(self, flow_in):\n",
    "        output = flow_in - flow_in.max(axis=1).reshape(-1,1)\n",
    "        output = np.exp(output)\n",
    "        output /= output.sum(axis=1).reshape(-1,1)\n",
    "        return output\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        flow_out = self.Forward(flow_in) # result of self.trans\n",
    "        return flow_out*back_flow - flow_out*((flow_out*back_flow).sum(axis=1).reshape(-1,1))\n",
    "\n",
    "# Activation functions defined end\n",
    "\n",
    "# Layer\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, num_unit, activationFunction):\n",
    "        if type(activationFunction) == type:\n",
    "            raise TypeError(\"activationFunction should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        self.num_unit = num_unit\n",
    "        self.activationFunction = activationFunction\n",
    "        self.linear = VariableArray((0, self.num_unit)) # linear weights working before active function\n",
    "        self.bias = VariableArray((1, self.num_unit)) # bias working before active function\n",
    "        self.flow_in = np.zeros((0, self.num_unit))\n",
    "        self.flow_out = np.zeros((0, self.num_unit))\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        self.flow_in = np.dot(flow_in, self.linear.value) + self.bias.value\n",
    "        self.flow_out = self.activationFunction.Forward(self.flow_in)\n",
    "    \n",
    "    def Backward(self, back_flow, layer_source):\n",
    "        deri = self.activationFunction.Backward(self.flow_in, back_flow)\n",
    "        self.linear.DeriModify(np.dot(layer_source.T, deri))\n",
    "        self.bias.DeriModify(np.sum(deri, axis=0).reshape(1, -1))\n",
    "        deri = np.dot(deri, self.linear.value.T)\n",
    "        return deri\n",
    "    \n",
    "    def ZeroDeri(self):\n",
    "        self.linear.ZeroDeri()\n",
    "        self.bias.ZeroDeri()\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        self.linear.SetRegularizer(rate, margin)\n",
    "        self.bias.SetRegularizer(rate, margin)\n",
    "    \n",
    "    def Descent(self, step, method):\n",
    "        self.linear.Descent(step, method)\n",
    "        self.bias.Descent(step, method)\n",
    "    \n",
    "    def ResetCwiseStep(self, new_cwise_step):\n",
    "        self.linear.ResetCwiseStep(new_cwise_step)\n",
    "        self.bias.ResetCwiseStep(new_cwise_step)\n",
    "        \n",
    "    def GetPCVar(self, weight):\n",
    "        if IsNone(weight):\n",
    "            weight = np.ones((self.flow_out.shape[0])) / self.flow_out.shape[0]\n",
    "        \n",
    "        mean_flow_out = self.flow_out.mean(axis=0)\n",
    "        centered_flow_out = self.flow_out - mean_flow_out\n",
    "        cov = np.dot(centered_flow_out.T * weight, centered_flow_out) # covariance matrix\n",
    "        information= np.linalg.eigvalsh(cov) #information is sorted from small to large\n",
    "        return information\n",
    "    \n",
    "    def GetDimension(self):\n",
    "        return self.linear.value.size + self.bias.value.size\n",
    "\n",
    "# Layer end\n",
    "\n",
    "# Loss function\n",
    "\n",
    "class LossFunction():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "    \n",
    "    def SetMethod(self, method):\n",
    "        self.method = method\n",
    "    \n",
    "    def GetLoss(self, inference, target, weight):\n",
    "        if self.method == \"r2\":\n",
    "            output = WeightedSum(np.square(inference - target).sum(axis=1), weight)\n",
    "            output /= WeightedSum(np.square(target - target.mean(axis=0)).sum(axis=1), weight)\n",
    "        elif self.method == \"cross entropy\":\n",
    "            output = WeightedSum((-target*np.log(inference)).sum(axis=1), weight)\n",
    "        else:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def Backward(self, inference, target, weight):\n",
    "        if self.method == \"r2\":\n",
    "            output = WeightedRow(2.*(inference - target), weight)\n",
    "            output /= WeightedSum(np.square(target - target.mean(axis=0)).sum(axis=1), weight)\n",
    "        elif self.method == \"cross entropy\":\n",
    "            output = WeightedRow(-(target/inference), weight)\n",
    "        else:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "class Terminator():\n",
    "    def __init__(self, short_period = 5, long_period = 10, threshold = 0.):\n",
    "        try:\n",
    "            short_period = int(short_period)\n",
    "            long_period = int(long_period)\n",
    "        except:\n",
    "            raise ValueError(\"period should be a value, and will be transfer to int\")\n",
    "        \n",
    "        if short_period <= 0:\n",
    "            short_period = 1\n",
    "            print(\"WARNING : short_period <= 0, set 1\")\n",
    "        \n",
    "        if long_period <= short_period:\n",
    "            long_period = short_period + 1\n",
    "            print(\"WARNING : long_period <= short_period, set short_period + 1\")\n",
    "        \n",
    "        self.short_period = short_period\n",
    "        self.long_period = long_period\n",
    "        self.threshold = threshold\n",
    "        self.record = []\n",
    "    \n",
    "    def Reset(self, short_period, long_period, threshold = 0.):\n",
    "        try:\n",
    "            short_period = int(short_period)\n",
    "            long_period = int(long_period)\n",
    "        except:\n",
    "            raise ValueError(\"period should be a value, and will be transfer to int\")\n",
    "        \n",
    "        if short_period <= 0:\n",
    "            short_period = 1\n",
    "            print(\"WARNING : short_period <= 0, set 1\")\n",
    "        \n",
    "        if long_period <= short_period:\n",
    "            long_period = short_period + 1\n",
    "            print(\"WARNING : long_period <= short_period, set %d\" %(short_period + 1))\n",
    "        \n",
    "        self.short_period = short_period\n",
    "        self.long_period = long_period\n",
    "        self.threshold = threshold\n",
    "        self.record = []\n",
    "    \n",
    "    def Hit(self, input_value):\n",
    "        try:\n",
    "            input_value = float(input_value)\n",
    "        except:\n",
    "            raise ValueError(\"input_value should be a real value\")\n",
    "        \n",
    "        self.record = [input_value] + self.record[:self.long_period-1]\n",
    "        if len(self.record) == self.long_period:\n",
    "            return (np.mean(self.record[:self.short_period]) - self.threshold > np.mean(self.record))\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def Clear(self):\n",
    "        self.record = []\n",
    "\n",
    "class DogikoNeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.hiddenLayerList = []\n",
    "        self.outputFunction = None\n",
    "        self.outputLayer = None\n",
    "        self.lossFunction = None\n",
    "        self.trainData = Data()\n",
    "        self.validData = Data()\n",
    "        self.testData = Data()\n",
    "        self.regulariz_rate = 0.\n",
    "        self.regulariz_margin = 0.\n",
    "        self.has_build = False\n",
    "        self.hit_tolerance = 0.1\n",
    "        self.terminator = Terminator()\n",
    "    \n",
    "    def SetLossFunction(self, method):\n",
    "        if method not in [\"r2\", \"cross entropy\"]:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        self.lossFunction = LossFunction(method)\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        if rate < 0.:\n",
    "            print(\"WARNING : regulariz_rate error, get negative value, setting to 0.\")\n",
    "        \n",
    "        if margin < 0.:\n",
    "            print(\"WARNING : regulariz_margin error, get negative value, setting to 0.\")\n",
    "        \n",
    "        self.regulariz_rate = max(rate, 0.)\n",
    "        self.regulariz_margin = max(margin, 0.)\n",
    "        \n",
    "        if self.has_build:\n",
    "            for l in range(self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "            \n",
    "            self.outputLayer.SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "    \n",
    "    def SetTrainData(self, inputs, labels, weight = None):\n",
    "        if self.has_build:\n",
    "            if inputs.shape[1] != self.inputs_size:\n",
    "                raise ValueError(\"inputs size doesn't match with this model\")\n",
    "            \n",
    "            if labels.shape[1] != self.labels_size:\n",
    "                raise ValueError(\"labels size doesn't match with this model\")\n",
    "        \n",
    "        self.trainData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetValidData(self, inputs, labels, weight = None):\n",
    "        if self.has_build:\n",
    "            if inputs.shape[1] != self.inputs_size:\n",
    "                raise ValueError(\"inputs size doesn't match with this model\")\n",
    "            \n",
    "            if labels.shape[1] != self.labels_size:\n",
    "                raise ValueError(\"labels size doesn't match with this model\")\n",
    "        \n",
    "        self.validData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetTestData(self, inputs, labels, weight = None):\n",
    "        if self.has_build:\n",
    "            if inputs.shape[1] != self.inputs_size:\n",
    "                raise ValueError(\"inputs size doesn't match with this model\")\n",
    "            \n",
    "            if labels.shape[1] != self.labels_size:\n",
    "                raise ValueError(\"labels size doesn't match with this model\")\n",
    "        \n",
    "        self.testData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def AddHiddenLayer(self, num_unit, activationFunction):\n",
    "        self.hiddenLayerList.append(Layer(num_unit, activationFunction))\n",
    "        if self.has_build:\n",
    "            print(\"WARNING : a hidden layer added after build, please re-build model or set related value manually.\")\n",
    "    \n",
    "    def SetOutputFunction(self, activationFunction):\n",
    "        # units of output layer is fixed as same as label size\n",
    "        self.outputFunction = activationFunction\n",
    "        if self.has_build:\n",
    "            self.outputLayer.activationFunction = self.outputFunction\n",
    "    \n",
    "    def SetHitTolerance(self, tolerance):\n",
    "        try:\n",
    "            if float(tolerance) <= 0.:\n",
    "                print(\"setting tolerance failed, tolerance should be positive real value\")\n",
    "            \n",
    "            self.hit_tolerance = float(tolerance)\n",
    "        except:\n",
    "            print(\"setting tolerance failed, tolerance should be positive real value\")\n",
    "    \n",
    "    def SetTerminator(self, short_period, long_period, threshold = 0.):\n",
    "        self.terminator.Reset(short_period, long_period, threshold)\n",
    "        \n",
    "    def ClearTerminator(self):\n",
    "        self.terminator.Clear()\n",
    "    \n",
    "    def GetNumHiddenLayers(self):\n",
    "        return len(self.hiddenLayerList)\n",
    "    \n",
    "    def Build(self):\n",
    "        if IsNone(self.lossFunction):\n",
    "            raise ValueError(\"Set loss function before build.\")\n",
    "        \n",
    "        if self.lossFunction.method == \"cross entropy\":\n",
    "            if type(self.outputFunction) not in [Sigmoid, Softmax, SoftSign]:\n",
    "                print (\"WARNING : chosen loss function is cross entropy but the output of output layer function may out of (0, 1)\")\n",
    "            \n",
    "        \n",
    "        if IsNone(self.outputFunction):\n",
    "            self.outputFunction = Identity()\n",
    "            print (\"WARNING : doesn't set outputFunction before build, set Identity().\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[0],\n",
    "                    self.validData.GetDatumSize()[0],\n",
    "                    self.testData.GetDatumSize()[0]\n",
    "                   ])) == 1:\n",
    "            self.inputs_size = self.trainData.GetDatumSize()[0]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same inputs size\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[1],\n",
    "                    self.validData.GetDatumSize()[1],\n",
    "                    self.testData.GetDatumSize()[1]\n",
    "                   ])) == 1:\n",
    "            self.labels_size = self.trainData.GetDatumSize()[1]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same labels size\")\n",
    "        \n",
    "        # when hidden layer exist, set first layer value\n",
    "        if self.GetNumHiddenLayers() >0:\n",
    "            self.hiddenLayerList[0].linear.SetValue(np.random.normal(0.,\n",
    "                                                                     1.,\n",
    "                                                                     (self.inputs_size, self.hiddenLayerList[0].num_unit)\n",
    "                                                                    )\n",
    "                                                   )\n",
    "            self.hiddenLayerList[0].bias.SetValue(np.random.normal(0.,\n",
    "                                                                   1.,\n",
    "                                                                   (1, self.hiddenLayerList[0].num_unit)\n",
    "                                                                  )\n",
    "                                                 )\n",
    "            # normalize flow_in by modify layer variables\n",
    "            self.hiddenLayerList[0].Forward(self.trainData.inputs)\n",
    "            flow_in_mean = self.hiddenLayerList[0].flow_in.mean(axis=0)\n",
    "            flow_in_std = self.hiddenLayerList[0].flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "            self.hiddenLayerList[0].linear.value /= flow_in_std\n",
    "            self.hiddenLayerList[0].bias.value -= flow_in_mean\n",
    "            self.hiddenLayerList[0].bias.value /= flow_in_std\n",
    "            self.hiddenLayerList[0].Forward(self.trainData.inputs)\n",
    "            # when hidden layer num >= 2, set internal layer value\n",
    "            for l in range(1, self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].linear.SetValue(np.random.normal(0.,\n",
    "                                                                         1.,\n",
    "                                                                         (self.hiddenLayerList[l-1].num_unit,\n",
    "                                                                          self.hiddenLayerList[l].num_unit\n",
    "                                                                         )\n",
    "                                                                        )\n",
    "                                                       )\n",
    "                self.hiddenLayerList[l].bias.SetValue(np.random.normal(0.,\n",
    "                                                                       1.,\n",
    "                                                                       (1, self.hiddenLayerList[l].num_unit)\n",
    "                                                                      )\n",
    "                                                     )\n",
    "                # normalize flow_in by modify layer variables\n",
    "                self.hiddenLayerList[l].Forward(self.hiddenLayerList[l-1].flow_out)\n",
    "                flow_in_mean = self.hiddenLayerList[l].flow_in.mean(axis=0)\n",
    "                flow_in_std = self.hiddenLayerList[l].flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "                self.hiddenLayerList[l].linear.value /= flow_in_std\n",
    "                self.hiddenLayerList[l].bias.value -= flow_in_mean\n",
    "                self.hiddenLayerList[l].bias.value /= flow_in_std\n",
    "                self.hiddenLayerList[l].Forward(self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            # set output layer\n",
    "            self.outputLayer = Layer(self.labels_size, self.outputFunction)\n",
    "            self.outputLayer.linear.SetValue(np.random.normal(0.,\n",
    "                                                              1.,\n",
    "                                                              (self.hiddenLayerList[-1].num_unit,\n",
    "                                                               self.outputLayer.num_unit\n",
    "                                                              )\n",
    "                                                             )\n",
    "                                            )\n",
    "            # normalize flow_in by modify layer variables\n",
    "            self.outputLayer.Forward(self.hiddenLayerList[-1].flow_out)\n",
    "            flow_in_mean = self.outputLayer.flow_in.mean(axis=0)\n",
    "            flow_in_std = self.outputLayer.flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "            self.outputLayer.linear.value /= flow_in_std\n",
    "            self.outputLayer.bias.value -= flow_in_mean\n",
    "            self.outputLayer.bias.value /= flow_in_std\n",
    "            self.outputLayer.Forward(self.hiddenLayerList[-1].flow_out)\n",
    "            \n",
    "        else: # case no hiddenlayer\n",
    "            self.outputLayer.linear.SetValue(np.random.normal(0.,\n",
    "                                                              1.,\n",
    "                                                              (self.inputs_size,\n",
    "                                                               self.outputLayer.num_unit\n",
    "                                                              )\n",
    "                                                             )\n",
    "                                            )\n",
    "        \n",
    "            self.outputLayer.bias.SetValue(np.random.normal(0.,\n",
    "                                                            1.,\n",
    "                                                            (1, self.outputLayer.num_unit)\n",
    "                                                           )\n",
    "                                          )\n",
    "            # normalize flow out to fit labels by modify layer variables\n",
    "            self.outputLayer.Forward(self.trainData.inputs)\n",
    "            flow_in_mean = self.outputLayer.flow_in.mean(axis=0)\n",
    "            flow_in_std = self.outputLayer.flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "            self.outputLayer.linear.value /= flow_in_std\n",
    "            self.outputLayer.bias.value -= flow_in_mean\n",
    "            self.outputLayer.bias.value /= flow_in_std\n",
    "            self.outputLayer.Forward(self.trainData.inputs)\n",
    "        \n",
    "        # Set regularizer\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            self.hiddenLayerList[l].SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "        \n",
    "        self.outputLayer.SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "        \n",
    "        self.has_build = True\n",
    "    \n",
    "    def save_model(self, target_folder):\n",
    "        if type(target_folder) != str:\n",
    "            raise TypeError(\"target_folder should be a str\")\n",
    "        \n",
    "        while target_folder[-1] ==\"/\":\n",
    "            target_folder = target_folder[:-1]\n",
    "        \n",
    "        if not os.path.exists(target_folder):\n",
    "            os.makedirs(target_folder)\n",
    "        \n",
    "        target_folder += \"/\"\n",
    "        \n",
    "        info_dict = {}\n",
    "        info_dict[\"inputs size\"] = self.inputs_size\n",
    "        info_dict[\"labels size\"] = self.labels_size\n",
    "        info_dict[\"loss\"] = self.lossFunction.method\n",
    "        info_dict[\"regularizer\"] = {\"rate\" : self.regulariz_rate,\n",
    "                                    \"margin\" : self.regulariz_margin\n",
    "                                   }\n",
    "        info_dict[\"tolerance\"] = self.hit_tolerance\n",
    "        info_dict[\"num units\"] = []\n",
    "        info_dict[\"activation function\"] = []\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            info_dict[\"num units\"].append(self.hiddenLayerList[l].num_unit)\n",
    "            info_dict[\"activation function\"].append(self.hiddenLayerList[l].activationFunction)\n",
    "        \n",
    "        info_dict[\"output function\"] = self.outputFunction\n",
    "        info_dict[\"terminator\"] = self.terminator\n",
    "        np.save(target_folder + \"model_info.npy\", info_dict)\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            np.save(target_folder + \"L%d.npy\" %(l), self.hiddenLayerList[l].linear.value)\n",
    "            np.save(target_folder + \"B%d.npy\" %(l), self.hiddenLayerList[l].bias.value)\n",
    "        \n",
    "        np.save(target_folder + \"Lo.npy\" , self.outputLayer.linear.value)\n",
    "        np.save(target_folder + \"Bo.npy\" , self.outputLayer.bias.value)\n",
    "    \n",
    "    def load_model(self, target_folder):\n",
    "        if len(set([self.trainData.GetDatumSize()[0],\n",
    "                    self.validData.GetDatumSize()[0],\n",
    "                    self.testData.GetDatumSize()[0]\n",
    "                   ])) == 1:\n",
    "            self.inputs_size = self.trainData.GetDatumSize()[0]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same inputs size\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[1],\n",
    "                    self.validData.GetDatumSize()[1],\n",
    "                    self.testData.GetDatumSize()[1]\n",
    "                   ])) == 1:\n",
    "            self.labels_size = self.trainData.GetDatumSize()[1]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same labels size\")\n",
    "        \n",
    "        if type(target_folder) != str:\n",
    "            raise TypeError(\"target_folder should be a str\")\n",
    "        \n",
    "        while target_folder[-1] ==\"/\":\n",
    "            target_folder = target_folder[:-1]\n",
    "        \n",
    "        target_folder += \"/\"\n",
    "        \n",
    "        self.has_build = False\n",
    "        \n",
    "        info_dict = np.load(target_folder + \"model_info.npy\").item()\n",
    "        if self.trainData.GetDatumSize() != (info_dict[\"inputs size\"], info_dict[\"labels size\"]):\n",
    "            raise ValueError(\"Datum size error, (inputs size, output size) for this model should be (%d, %d)\" %self.trainData.GetDatumSize())\n",
    "        \n",
    "        self.SetLossFunction(info_dict[\"loss\"])\n",
    "        self.SetRegularizer(info_dict[\"regularizer\"][\"rate\"], info_dict[\"regularizer\"][\"margin\"])\n",
    "        self.hit_tolerance = info_dict[\"tolerance\"]\n",
    "        \n",
    "        self.hiddenLayerList = []\n",
    "        for l in range(len(info_dict[\"num units\"])):\n",
    "            self.AddHiddenLayer(info_dict[\"num units\"][l], info_dict[\"activation function\"][l])\n",
    "        \n",
    "        self.SetOutputFunction(info_dict[\"output function\"])\n",
    "        self.terminator = info_dict[\"terminator\"]\n",
    "        self.Build()\n",
    "        \n",
    "        for l in range(len(info_dict[\"num units\"])):\n",
    "            self.hiddenLayerList[l].linear.value = np.load(target_folder + \"L%d.npy\" %(l))\n",
    "            self.hiddenLayerList[l].bias.value = np.load(target_folder + \"B%d.npy\" %(l))\n",
    "        \n",
    "        self.outputLayer.linear.value = np.load(target_folder + \"Lo.npy\")\n",
    "        self.outputLayer.bias.value = np.load(target_folder + \"Bo.npy\")\n",
    "    \n",
    "    def GetDimension(self):\n",
    "        output = self.outputLayer.GetDimension()\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            output += self.hiddenLayerList[l].GetDimension()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def GetInference(self, inputs):\n",
    "        if inputs.shape[1] != self.inputs_size:\n",
    "            raise ValueError(\"inputs size shoud be %d, get %d\" %(self.inputs_size, inputs.shape[1]))\n",
    "        \n",
    "        if self.GetNumHiddenLayers() > 0:\n",
    "            self.hiddenLayerList[0].Forward(inputs)\n",
    "            for l in range(1, self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].Forward(self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            self.outputLayer.Forward(self.hiddenLayerList[-1].flow_out)\n",
    "        else:\n",
    "            self.outputLayer.Forward(inputs)\n",
    "        \n",
    "        return self.outputLayer.flow_out\n",
    "    \n",
    "    def GetLoss(self, inference, target, weight = None):\n",
    "        if inference.shape != target.shape:\n",
    "            raise ValueError(\"shape between inference and target non-equal\")\n",
    "        \n",
    "        return self.lossFunction.GetLoss(inference, target, weight)\n",
    "    \n",
    "    def GetTrainLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.trainData.inputs), self.trainData.labels, self.trainData.weight)\n",
    "    \n",
    "    def GetValidLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.validData.inputs), self.validData.labels, self.validData.weight)\n",
    "    \n",
    "    def GetTestLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.testData.inputs), self.testData.labels, self.testData.weight)\n",
    "    \n",
    "    def GetAccuracy(self, inference, target, decimals=4, tolerance = 0.1):\n",
    "        # tolerance : for regression(r2) model, given a tolerance to verify hit or miss,\n",
    "        #             this variable is meaningless for classifycation\n",
    "        if self.lossFunction.method == \"cross entropy\":\n",
    "            output = (inference.argmax(axis=1) == target.argmax(axis=1)).mean()\n",
    "        elif self.lossFunction.method == \"r2\":\n",
    "            output = np.square(inference - target).sum(axis=1)\n",
    "            output = (output < tolerance).mean()\n",
    "        \n",
    "        return np.round(output, decimals)\n",
    "    \n",
    "    def GetTrainAccuracy(self, tolerance = None, decimals=4):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.trainData.inputs), self.trainData.labels, decimals, tolerance)\n",
    "    \n",
    "    def GetValidAccuracy(self, tolerance = None, decimals=4):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.validData.inputs), self.validData.labels, decimals, tolerance)\n",
    "    \n",
    "    def GetTestAccuracy(self, tolerance = None, decimals=4):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.testData.inputs), self.testData.labels, decimals, tolerance)\n",
    "    \n",
    "    def Backward(self, inputs, labels, weight):\n",
    "        deri = self.lossFunction.Backward(self.GetInference(inputs), labels, weight)\n",
    "        if self.GetNumHiddenLayers() > 0:\n",
    "            deri = self.outputLayer.Backward(deri, self.hiddenLayerList[-1].flow_out)\n",
    "            for l in range(self.GetNumHiddenLayers()-1, 0, -1):\n",
    "                deri = self.hiddenLayerList[l].Backward(deri, self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            deri = self.hiddenLayerList[0].Backward(deri, inputs)\n",
    "        else:\n",
    "            deri = self.outputLayer.Backward(deri, inputs)\n",
    "        \n",
    "    def ZeroDeri(self):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            deri = self.hiddenLayerList[l].ZeroDeri()\n",
    "        \n",
    "        deri = self.outputLayer.ZeroDeri()\n",
    "    \n",
    "    def ResetCwiseStep(self, new_cwise_step=0.1):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            deri = self.hiddenLayerList[l].ResetCwiseStep(new_cwise_step)\n",
    "        \n",
    "        deri = self.outputLayer.ResetCwiseStep(new_cwise_step)\n",
    "    \n",
    "    def Descent(self, step = 1., method = \"normal\"):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            self.hiddenLayerList[l].Descent(step, method)\n",
    "        \n",
    "        self.outputLayer.Descent(step, method)\n",
    "    \n",
    "    def BatchFit(self, batch_inputs, batch_labels, batch_weight, step = 1., method = \"normal\"):\n",
    "        batch_weight /= batch_weight.sum()\n",
    "        self.Backward(batch_inputs, batch_labels, batch_weight)\n",
    "        self.Descent(step, method)\n",
    "    \n",
    "    def EpochFit(self, batch_size = None, step = 1., method = \"normal\"):\n",
    "        if type(batch_size) == type(None):\n",
    "            self.BatchFit(self.trainData.inputs, self.trainData.labels, self.trainData.weight, step, method)\n",
    "        elif type(batch_size) == int:\n",
    "            if batch_size > 0:\n",
    "                for b in range(np.ceil(self.trainData.GetNumDatums()/ batch_size).astype(np.int)):\n",
    "                    self.BatchFit(self.trainData.inputs[b*batch_size: (b+1)*batch_size],\n",
    "                                  self.trainData.labels[b*batch_size: (b+1)*batch_size],\n",
    "                                  self.trainData.weight[b*batch_size: (b+1)*batch_size],\n",
    "                                  step,\n",
    "                                  method\n",
    "                                 )\n",
    "            else:\n",
    "                raise ValueError(\"batch_size should be positive int\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"batch_size should be positive int\")\n",
    "    \n",
    "    def Train(self, times, batch_size = None, step = 1., method = \"normal\", is_termination = False, shuffling = False):\n",
    "        self.terminator.Clear()\n",
    "        for t in range(times):\n",
    "            if shuffling:\n",
    "                self.trainData.Shuffle()\n",
    "            \n",
    "            self.EpochFit(batch_size, step, method)\n",
    "            if is_termination:\n",
    "                if self.terminator.Hit(10*np.log10(self.GetValidLoss() + 0.000000001)):\n",
    "                # 0.000000001, bias for prevent error when log(0)\n",
    "                    return t+1\n",
    "        \n",
    "        return times\n",
    "    \n",
    "    def AddUnit(self, layer_index, num_added = 1, output_linear_bound = 1., cwise_step_initial = 0.1, drop_times=5):\n",
    "        if layer_index not in range(self.GetNumHiddenLayers()):\n",
    "            raise ValueError(\"layer_index should be an int from 0 to (#layer-1) for hiddenlayer\")\n",
    "        \n",
    "        if type(num_added) != int:\n",
    "            raise ValueError(\"num_added should be positive int\")\n",
    "        elif num_added <= 0:\n",
    "            raise ValueError(\"num_added should be positive int\")\n",
    "        \n",
    "        try:\n",
    "            if output_linear_bound < 0.:\n",
    "                raise ValueError(\"output_weight_bound should be non-negative\")\n",
    "        except:\n",
    "            raise ValueError(\"output_weight_bound should a non-negative real value\")\n",
    "        \n",
    "        new_linear = self.hiddenLayerList[layer_index].linear.value\n",
    "        new_bias = self.hiddenLayerList[layer_index].bias.value\n",
    "        for u in range(num_added):\n",
    "            new_linear = McmcColExtend(new_linear, drop_times)\n",
    "            new_bias = McmcColExtend(new_bias, drop_times)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].linear.SetValue(new_linear, cwise_step_initial)\n",
    "        self.hiddenLayerList[layer_index].bias.SetValue(new_bias, cwise_step_initial)\n",
    "        \n",
    "        if layer_index < self.GetNumHiddenLayers() - 1: # if not final hiddenlayer\n",
    "            new_output_linear = self.hiddenLayerList[layer_index+1].linear.value\n",
    "            new_output_linear = np.append(new_output_linear,\n",
    "                                          output_linear_bound * (2 * np.random.rand(num_added, self.hiddenLayerList[layer_index+1].num_unit) - 1),\n",
    "                                          axis=0\n",
    "                                         )\n",
    "            self.hiddenLayerList[layer_index+1].linear.SetValue(new_output_linear, cwise_step_initial)\n",
    "        \n",
    "        else: # if final hiddenlayer\n",
    "            new_output_linear = self.outputLayer.linear.value\n",
    "            new_output_linear = np.append(new_output_linear,\n",
    "                                          output_linear_bound * (2 * np.random.rand(num_added, self.outputLayer.num_unit) - 1),\n",
    "                                          axis=0\n",
    "                                         )\n",
    "            self.outputLayer.linear.SetValue(new_output_linear, cwise_step_initial)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].num_unit += num_added\n",
    "    \n",
    "    def UnitsRefined(self, layer_index, reference_data = None, weight = None, method=\"remove\", threshold = 1):\n",
    "        if self.hiddenLayerList[layer_index].num_unit == 1:\n",
    "            print(\"WARNING : Units Refining failed, layer %d has only 1 unit, pass this process\"%(layer_index))\n",
    "            return None\n",
    "        \n",
    "        if type(method) != str:\n",
    "            raise ValueError(\"method must be a str, and equal to 'remain', 'remove', 'info' or 'info ratio'.\")\n",
    "        \n",
    "        if type(layer_index) != int:\n",
    "            raise TypeError(\"layer_index should be an int between 0 to (# hidden layers - 1)\")\n",
    "        elif (layer_index > self.GetNumHiddenLayers() - 1) or (layer_index < 0):\n",
    "            raise ValueError(\"layer_index should be an int between 0 to (# hidden layers - 1)\")\n",
    "        \n",
    "        if (type(threshold) == int) and (method == \"remove\"):\n",
    "            if (threshold > self.hiddenLayerList[layer_index].num_unit-1):\n",
    "                raise ValueError(\"int threshold error : removed #neuron should less than currently #neuron\")\n",
    "            elif (threshold < -self.hiddenLayerList[layer_index].num_unit) or (threshold==0):\n",
    "                return None\n",
    "                # do nothing if remove no #neuron (threshold=0) or want to remain #neuron more than currently\n",
    "            \n",
    "        elif ((threshold > 1) and (threshold < 0)):\n",
    "            raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "        \n",
    "        if IsNone(reference_data):\n",
    "            reference_data = self.trainData.inputs\n",
    "            weight = self.trainData.weight\n",
    "        \n",
    "        self.GetInference(reference_data)\n",
    "        \n",
    "        if IsNone(weight):\n",
    "            weight = np.ones(reference_data.shape[0])\n",
    "            weight /= weight.size\n",
    "        \n",
    "        info = self.hiddenLayerList[layer_index].GetPCVar(weight)\n",
    "        if method in [\"remove\", \"remain\"]:\n",
    "            if type(threshold) != int:\n",
    "                raise TypeError(\"when using method 'remove' or 'remain', threshold represent the num of units want to remove, should be an int\")\n",
    "            \n",
    "            if method == \"remove\":\n",
    "                if (threshold < 0) or (threshold >= len(info)):\n",
    "                    raise ValueError(\"when using method 'remove', threshold should be in [0, #units - 1]\")\n",
    "                \n",
    "                cut_index = threshold\n",
    "            else: # case \"remain\"\n",
    "                if (threshold <= 0) or (threshold > len(info)):\n",
    "                    raise ValueError(\"when using method 'remain', threshold should be in [1, #units]\")\n",
    "                \n",
    "                cut_index = self.hiddenLayerList[layer_index].num_unit - threshold\n",
    "        \n",
    "        elif method in [\"info\", \"info ratio\"]:\n",
    "            if method == \"info ratio\":\n",
    "                threshold *= info.sum()\n",
    "            \n",
    "            cut_index = self.hiddenLayerList[layer_index].num_unit - (info >= threshold).sum()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"method must be 'remain', 'remove', 'info' or 'info ratio'.\")\n",
    "        \n",
    "        if layer_index < self.GetNumHiddenLayers() - 1: # case not final hidden layer\n",
    "            units_remain, new_linear, new_bias, mse = LinearRefine(self.hiddenLayerList[layer_index].flow_out,\n",
    "                                                                   self.hiddenLayerList[layer_index+1].flow_in,\n",
    "                                                                   num_elimination=cut_index,\n",
    "                                                                   regularizer = 10**-8\n",
    "                                                                  )\n",
    "            self.hiddenLayerList[layer_index+1].linear.SetValue(new_linear)\n",
    "            self.hiddenLayerList[layer_index+1].bias.SetValue(new_bias)\n",
    "        else:\n",
    "            units_remain, new_linear, new_bias, mse = LinearRefine(self.hiddenLayerList[layer_index].flow_out,\n",
    "                                                                   self.outputLayer.flow_in,\n",
    "                                                                   num_elimination=cut_index,\n",
    "                                                                   regularizer = 10**-8\n",
    "                                                                  )\n",
    "            self.outputLayer.linear.SetValue(new_linear)\n",
    "            self.outputLayer.bias.SetValue(new_bias)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].linear.SetValue(self.hiddenLayerList[layer_index].linear.value[:, units_remain])\n",
    "        self.hiddenLayerList[layer_index].bias.SetValue(self.hiddenLayerList[layer_index].bias.value[:, units_remain])\n",
    "        self.hiddenLayerList[layer_index].num_unit -= cut_index\n",
    "\n",
    "\"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient cheak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(0, 1, (1000,2))\n",
    "Y = np.dot(X, np.random.normal(0, 2, (2,1)))\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X, Y)\n",
    "NN.SetValidData(X, Y)\n",
    "NN.SetTestData(X, Y)\n",
    "NN.SetLossFunction(\"r2\")\n",
    "NN.SetRegularizer(0., 0.)\n",
    "NN.AddHiddenLayer(2, Identity())\n",
    "NN.AddHiddenLayer(2, Sigmoid())\n",
    "NN.AddHiddenLayer(2, Hypertan())\n",
    "NN.AddHiddenLayer(2, Relu())\n",
    "NN.AddHiddenLayer(2, LeakyRelu())\n",
    "NN.AddHiddenLayer(2, SoftPlus())\n",
    "NN.SetOutputFunction(Selu())\n",
    "NN.SetTerminator(10,20,-0.)\n",
    "NN.Build()\n",
    "\n",
    "step = 0.000001\n",
    "\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    for j in range(NN.hiddenLayerList[l].linear.value.shape[1]):\n",
    "        for i in range(NN.hiddenLayerList[l].linear.value.shape[0]):\n",
    "            b = NN.GetTrainLoss()\n",
    "            NN.ZeroDeri()\n",
    "            NN.Backward(X, Y, NN.trainData.weight)\n",
    "            partial_deri = NN.hiddenLayerList[l].linear.total_deri[i, j]\n",
    "            NN.hiddenLayerList[l].linear.value[i, j] += step\n",
    "            a = NN.GetTrainLoss()\n",
    "            if np.abs(((a-b)/step)/partial_deri - 1) > 0.001:\n",
    "                print(((a-b)/step)/partial_deri - 1)\n",
    "        \n",
    "        b = NN.GetTrainLoss()\n",
    "        NN.ZeroDeri()\n",
    "        NN.Backward(X, Y, NN.trainData.weight)\n",
    "        partial_deri = NN.hiddenLayerList[l].bias.total_deri[0, j]\n",
    "        NN.hiddenLayerList[l].bias.value[0, j] += step\n",
    "        a = NN.GetTrainLoss()\n",
    "        if np.abs(((a-b)/step)/partial_deri - 1) > 0.001:\n",
    "            print(((a-b)/step)/partial_deri - 1)\n",
    "\n",
    "for j in range(NN.outputLayer.linear.value.shape[1]):\n",
    "    for i in range(NN.outputLayer.linear.value.shape[0]):\n",
    "        b = NN.GetTrainLoss()\n",
    "        NN.ZeroDeri()\n",
    "        NN.Backward(X, Y, NN.trainData.weight)\n",
    "        partial_deri = NN.outputLayer.linear.total_deri[i, j]\n",
    "        NN.outputLayer.linear.value[i, j] += step\n",
    "        a = NN.GetTrainLoss()\n",
    "        if np.abs(((a-b)/step)/partial_deri - 1) > 0.001:\n",
    "            print(((a-b)/step)/partial_deri - 1)\n",
    "\n",
    "    b = NN.GetTrainLoss()\n",
    "    NN.ZeroDeri()\n",
    "    NN.Backward(X, Y, NN.trainData.weight)\n",
    "    partial_deri = NN.outputLayer.bias.total_deri[0, j]\n",
    "    NN.outputLayer.bias.value[0, j] += step\n",
    "    a = NN.GetTrainLoss()\n",
    "    if np.abs(((a-b)/step)/partial_deri - 1) > 0.001:\n",
    "        print(((a-b)/step)/partial_deri - 1)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ((np.arange(400)-200)/100).reshape(-1,1)\n",
    "Y = np.zeros((X.shape[0], 2))\n",
    "Y[:, :1] = 1*(X>-1.)\n",
    "Y[:, :1] *= 1*(X<1.)\n",
    "Y[:, 1:] = 1 - Y[:, :1]\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X, Y)\n",
    "NN.SetValidData(X, Y)\n",
    "NN.SetTestData(X, Y)\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.01, 3.)\n",
    "NN.AddHiddenLayer(10, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,20,-0.01)\n",
    "NN.Build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(NN.Train(times=500, method=\"Rprop\", is_termination=False), end = \", \")\n",
    "print(NN.hiddenLayerList[0].num_unit, NN.GetTrainLoss())\n",
    "\n",
    "for t in range(5):\n",
    "    NN.UnitsRefined(layer_index=0, method=\"info\", threshold=0.01)\n",
    "    print(NN.Train(times=500, method=\"Rprop\", is_termination=False), end = \", \")\n",
    "    print(NN.hiddenLayerList[0].num_unit, NN.GetTrainLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Odd or even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ((np.arange(800)-400)/100).reshape(-1,1)\n",
    "Y = np.zeros(X.shape)\n",
    "Y[:, :] = 1*((X%2)>=1.)\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X, Y)\n",
    "NN.SetValidData(X, Y)\n",
    "NN.SetTestData(X, Y)\n",
    "NN.SetLossFunction(\"r2\")\n",
    "NN.SetRegularizer(0.001, 0.)\n",
    "NN.AddHiddenLayer(10, Hypertan())\n",
    "NN.SetOutputFunction(Sigmoid())\n",
    "NN.SetTerminator(10,30,-0.01)\n",
    "NN.Build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(NN.Train(times=500, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "print(NN.hiddenLayerList[0].num_unit, \", \", NN.GetTrainLoss())\n",
    "\n",
    "plt.plot(X.reshape(-1), NN.GetInference(NN.trainData.inputs)[:,0], \"bp\")\n",
    "plt.show()\n",
    "\n",
    "for t in range(10):\n",
    "    NN.AddUnit(layer_index=0, num_added=3, output_linear_bound=0.1)\n",
    "    print(NN.Train(times=500, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    NN.UnitsRefined(layer_index=0, method=\"remain\", threshold=7)\n",
    "    print(NN.Train(times=500, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    print(NN.hiddenLayerList[0].num_unit, \", \", NN.GetTrainLoss())\n",
    "    if NN.GetTrainLoss()>1.:\n",
    "        break\n",
    "    else:\n",
    "        pre_input = NN.hiddenLayerList[0].flow_out\n",
    "        pre_output = NN.outputLayer.flow_in\n",
    "    \n",
    "    #plt.plot(X.reshape(-1), NN.GetInference(NN.trainData.inputs)[:,0], \"bp\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skew chessboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = 4*np.random.rand(20000,2) - 2\n",
    "Y = np.zeros((X.shape[0],2))\n",
    "Y[:, 0] = (((np.floor((X[:,0] + X[:,1])%2) + np.floor((X[:,0] - X[:,1])%2)) %2) == 1)\n",
    "Y[:, 1] = 1- Y[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:10000], Y[:10000])\n",
    "NN.SetValidData(X[10000:15000], Y[10000:15000])\n",
    "NN.SetTestData(X[15000:], Y[15000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 1.)\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.)\n",
    "\n",
    "times = 100\n",
    "record = np.zeros((times))\n",
    "for t in range(times):\n",
    "    NN.Build()\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "    record[t] = NN.GetTestAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record.sort()\n",
    "record[-25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat add-fit-kill-fit for fix number of units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69, 0.5768 0.5606 0.5768\n",
      "200, 0.7169 0.714 0.7232\n",
      "166, 0.7629 0.7576 0.7698\n",
      "316, 0.7827 0.7782 0.7924\n",
      "56, 0.796 0.7994 0.8032\n",
      "177, 0.8078 0.8024 0.8054\n",
      "158, 0.8211 0.8122 0.822\n",
      "195, 0.8629 0.8536 0.8688\n",
      "173, 0.8712 0.8578 0.8722\n",
      "88, 0.8731 0.8604 0.874\n",
      "45, 0.8751 0.8626 0.877\n",
      "173, 0.8754 0.8648 0.878\n",
      "22, 0.8778 0.8654 0.8788\n",
      "43, 0.8777 0.8644 0.879\n",
      "22, 0.879 0.8652 0.8794\n",
      "228, 0.8801 0.8698 0.882\n",
      "77, 0.883 0.87 0.8852\n",
      "16, 0.884 0.8718 0.8846\n",
      "62, 0.8856 0.8752 0.8878\n",
      "138, 0.8838 0.8736 0.8878\n",
      "114, 0.8894 0.879 0.8916\n",
      "81, 0.8943 0.891 0.8956\n",
      "354, 0.9037 0.9016 0.902\n",
      "103, 0.9093 0.909 0.906\n",
      "112, 0.911 0.912 0.91\n",
      "50, 0.9152 0.912 0.9132\n",
      "43, 0.9194 0.914 0.914\n",
      "186, 0.9213 0.9148 0.9198\n",
      "87, 0.922 0.9172 0.918\n",
      "312, 0.9248 0.922 0.9228\n",
      "169, 0.9259 0.9262 0.919\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQkJCqpTQAISEhIRUKaEBCAkJCalS/n8QoK6TUQO54QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11498efd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:10000], Y[:10000])\n",
    "NN.SetValidData(X[10000:15000], Y[10000:15000])\n",
    "NN.SetTestData(X[15000:], Y[15000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.001, 2.)\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(5,10, -0.)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=500, batch_size=5000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(30):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"remove\", threshold=2)\n",
    "        NN.AddUnit(layer_index=l, num_added=2, output_linear_bound=0.001)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.01)\n",
    "    print(NN.Train(times=500, batch_size=5000, method=\"Rprop\", is_termination=True, shuffling=True), end = \", \")    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "I = NN.GetInference(X)[:, 0]\n",
    "plt.plot(X[:, 0][I>0.5], X[:, 1][I>0.5], \"bp\")\n",
    "plt.plot(X[:, 0][I<0.5], X[:, 1][I<0.5], \"rp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find suitable units by info ratio of layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:10000], Y[:10000])\n",
    "NN.SetValidData(X[10000:15000], Y[10000:15000])\n",
    "NN.SetTestData(X[15000:], Y[15000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 1.)\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=1000, batch_size=5000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(20):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.AddUnit(layer_index=l, num_added=1, output_linear_bound=0.01)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"info ratio\", threshold=0.001)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "I = NN.GetInference(X)[:, 0]\n",
    "plt.plot(X[:, 0][I>0.5], X[:, 1][I>0.5], \"bp\")\n",
    "plt.plot(X[:, 0][I<0.5], X[:, 1][I<0.5], \"rp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Is Skin\" on UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.load(\"../UCI/Is-skin/data-npy/data.npy\").astype(np.float64)\n",
    "Y = np.load(\"../UCI/Is-skin/data-npy/label.npy\").astype(np.float64)\n",
    "X *= 2/255\n",
    "X -= 1\n",
    "shuffle = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffle)\n",
    "X = X[shuffle]\n",
    "Y = Y[shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:150000], Y[:150000])\n",
    "NN.SetValidData(X[150000:200000], Y[150000:200000])\n",
    "NN.SetTestData(X[200000:], Y[200000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 0.)\n",
    "NN.AddHiddenLayer(2, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.01)\n",
    "for t in range(10):\n",
    "    NN.Build()\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat add-fit-kill-fit for fix number of units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:150000], Y[:150000])\n",
    "NN.SetValidData(X[150000:200000], Y[150000:200000])\n",
    "NN.SetTestData(X[200000:], Y[200000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 1.)\n",
    "NN.AddHiddenLayer(2, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(10):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.AddUnit(layer_index=l, num_added=1, output_linear_bound=0.01)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"remain\", threshold=2, enhance_alert=False)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find suitable units by info ratio of layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:150000], Y[:150000])\n",
    "NN.SetValidData(X[150000:200000], Y[150000:200000])\n",
    "NN.SetTestData(X[200000:], Y[200000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 0.)\n",
    "NN.AddHiddenLayer(10, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.01)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(20):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.AddUnit(layer_index=l, num_added=1, output_linear_bound=0.01)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"info ratio\", threshold=0.0001)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
