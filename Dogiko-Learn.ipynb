{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def IsNone(target):\n",
    "    return (type(target) == type(None))\n",
    "\n",
    "def ArraySign(input_array):\n",
    "    # return +1, 0, -1 respect to positive, zero, negtive\n",
    "    return 1.*(input_array>0) - 1.*(input_array<0)\n",
    "\n",
    "def CutValue(input_array, cut_value):\n",
    "    output = np.abs(input_array)\n",
    "    output = ArraySign(input_array) * (output * (output < cut_value) + cut_value * (output >= cut_value))\n",
    "    return output\n",
    "\n",
    "def WeightedSum(input_array, weight):\n",
    "    try:\n",
    "        return (input_array.reshape(-1)*weight).sum()\n",
    "    except:\n",
    "        raise ValueError(\"weight should be an 1-d array with the same size with input_array\")\n",
    "\n",
    "def WeightedRow(input_array, weight):\n",
    "    try:\n",
    "        return input_array*weight.reshape(-1,1)\n",
    "    except:\n",
    "        raise ValueError(\"weight should be an 1-d array with the same length with first shape of input_array\")\n",
    "\n",
    "def OverPenalty(input_value, rate = 0.1, threshold=0.):\n",
    "    output = np.abs(input_value) - threshold\n",
    "    output *= (output > 0)\n",
    "    output *= rate * ArraySign(input_value)\n",
    "    return output\n",
    "\n",
    "def RowOperate(matrix, threshold = 0.1**15):\n",
    "    reduced_matrix = np.array(matrix)\n",
    "    filtered_matrix = np.array(matrix)\n",
    "    shape = matrix.shape # matrix size\n",
    "    mask = np.ones(shape)\n",
    "    pivots = -1*np.ones((min(shape)), dtype = np.int) # store pivots, # of pivots <= min(rows, columns)\n",
    "    for t in range(len(pivots)):\n",
    "        filtered_matrix = reduced_matrix * mask # filter\n",
    "        if np.abs(filtered_matrix).max() < threshold:\n",
    "            break\n",
    "        \n",
    "        pivot_row, pivot_col = np.unravel_index(np.abs(filtered_matrix).argmax(), shape) # pivot row, pivot column\n",
    "        reduced_matrix[pivot_row] /= reduced_matrix[pivot_row][pivot_col]\n",
    "        multi = np.array(reduced_matrix[:, pivot_col])\n",
    "        multi[pivot_row] = 0.\n",
    "        reduced_matrix -= np.dot(multi.reshape((shape[0], 1)), reduced_matrix[pivot_row].reshape((1, shape[1])))\n",
    "        mask[pivot_row] = 0.\n",
    "        mask[:, pivot_col] = 0.\n",
    "        pivots[pivot_row] = pivot_col # the column-index of pivot_row-th row is pivot_col\n",
    "    \n",
    "    reduced_matrix = reduced_matrix[pivots != -1,:]\n",
    "    pivots = pivots[pivots != -1]\n",
    "    return reduced_matrix, pivots\n",
    "\n",
    "def McmcNormal(points, drop_times = 10, mean=0., std=1.):\n",
    "    # Useing Markov chain Monte Carlo method to get a new point from normal distribution with given points\n",
    "    # each element is get from mean and std\n",
    "    output = np.random.normal(mean, std, points.shape[1:])\n",
    "    if drop_times>1:\n",
    "        for t in range(1, drop_times):\n",
    "            candicate = np.random.normal(mean, std, points.shape[1:])\n",
    "            candicate_distance = np.sqrt(np.square(np.subtract(points, candicate)).sum(axis=tuple(np.arange(1, len(points.shape))))).min()\n",
    "            # distance of candicate to target\n",
    "            output_distance = np.sqrt(np.square(np.subtract(points, output)).sum(axis=tuple(np.arange(1, len(points.shape))))).min()\n",
    "            # distance of currently output to target\n",
    "            if np.random.rand()*output_distance < candicate_distance:\n",
    "                output = np.array(candicate)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def McmcMatrixExtend(input_matrix, drop_times):\n",
    "    # return a matrix with a new row with same level by McmcNormal\n",
    "    return np.insert(input_matrix,\n",
    "                     len(input_matrix),\n",
    "                     McmcNormal(input_matrix, drop_times, input_matrix.mean(), input_matrix.std()),\n",
    "                     axis=0\n",
    "                    )\n",
    "\n",
    "# Data\n",
    "\n",
    "class Data():\n",
    "    def __init__(self, inputs=np.zeros((0,0)), labels=np.zeros((0,0)), weight = None):\n",
    "        self.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetData(self, inputs=np.zeros((0,0)), labels=np.zeros((0,0)), weight = None):\n",
    "        if len(inputs) != len(labels):\n",
    "            raise ValueError(\"num_datums error, #inputs != #labels.\")\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        if IsNone(weight):\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "        elif weight.shape != (len(inputs)):\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "            print(\"WARNING : weight shape error, set uniform weight.\")\n",
    "        elif weight.sum() <= 0:\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "            print(\"WARNING : get non-positive weight sum, set uniform weight.\")\n",
    "        else:\n",
    "            self.weight = weight\n",
    "        \n",
    "        self.weight /= self.weight.sum()\n",
    "    \n",
    "    def GetNumDatums(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def GetDatumSize(self):\n",
    "        # return size of input and label\n",
    "        return (self.inputs.shape[1], self.labels.shape[1])\n",
    "    \n",
    "    def IsClassification(self):\n",
    "        output = ((self.labels == 0) + (self.labels == 1)).all()\n",
    "        output *= (self.labels.shape[1]>1)\n",
    "        output *= (self.labels.sum(axis=1)==1).all()\n",
    "        return output\n",
    "\n",
    "# Data end\n",
    "\n",
    "# VariableArray\n",
    "\n",
    "class VariableArray():\n",
    "    def __init__(self, size, cwise_step_initial=0.1):\n",
    "        self.value = np.random.normal(0., 1., size) # array value\n",
    "        self.total_deri = np.zeros(self.value.shape) # total derivative, used to descent\n",
    "        self.last_total_deri = np.zeros(self.value.shape) # last total derivative\n",
    "        self.moving = np.zeros(self.value.shape) # moving array\n",
    "        self.cwise_step = cwise_step_initial*np.ones(self.value.shape) # component-wise step\n",
    "        \n",
    "        self.regulariz_rate = 0.\n",
    "        self.regulariz_margin = 0.\n",
    "    \n",
    "    def SetValue(self, input_value, cwise_step_initial=0.1):\n",
    "        self.value = np.array(input_value) # array value\n",
    "        self.total_deri = np.zeros(self.value.shape) # total derivative, used to descent\n",
    "        self.last_total_deri = np.zeros(self.value.shape) # last total derivative\n",
    "        self.moving = np.zeros(self.value.shape) # moving array\n",
    "        self.cwise_step = cwise_step_initial*np.ones(self.value.shape) # component-wise step\n",
    "    \n",
    "    def SetDeri(self, input_value):\n",
    "        if input_value.shape != self.total_deri.shape:\n",
    "            raise ValueError(\"input_value shape error\")\n",
    "        \n",
    "        self.total_deri = np.array(input_value)\n",
    "    \n",
    "    def DeriModify(self, input_value):\n",
    "        if input_value.shape != self.total_deri.shape:\n",
    "            raise ValueError(\"input_value shape error\")\n",
    "        \n",
    "        self.total_deri += input_value\n",
    "    \n",
    "    def ZeroDeri(self):\n",
    "        self.total_deri *= 0\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        self.regulariz_rate = max(rate, 0.)\n",
    "        self.regulariz_margin = max(margin, 0.)\n",
    "    \n",
    "    def AddRow(self, input_row, cwise_step_initial=0.1):\n",
    "        self.value = np.append(self.value, np.array([input_row]), axis = 0)\n",
    "        self.total_deri = np.append(self.total_deri, np.zeros((1,) + input_row.shape), axis = 0)\n",
    "        self.last_total_deri = None\n",
    "        self.moving = np.zeros(self.value.shape)\n",
    "        self.cwise_step = np.append(self.cwise_step, cwise_step_initial * np.ones((1,) + input_row.shape), axis = 0)\n",
    "    \n",
    "    def AddCol(self, input_column, cwise_step_initial=0.1):\n",
    "        self.value = np.append(self.value, np.array([input_column]).T, axis = 1)\n",
    "        self.total_deri = np.append(self.total_deri, np.zeros(input_column.shape + (1,)), axis = 1)\n",
    "        self.last_total_deri = None\n",
    "        self.moving = np.zeros(self.value.shape)\n",
    "        self.cwise_step = np.append(self.cwise_step, cwise_step_initial * np.ones(input_column.shape + (1,)), axis = 1)\n",
    "    \n",
    "    def ResetCwiseStep(self, input_cwise_step):\n",
    "        self.cwise_step = input_cwise_step * np.ones(self.cwise_step.shape)\n",
    "    \n",
    "    def Regularize(self):\n",
    "        if self.regulariz_rate != 0:\n",
    "            self.total_deri += OverPenalty(self.value, self.regulariz_rate, self.regulariz_margin)\n",
    "    \n",
    "    def Descent(self, step=1., method=\"normal\", move_max=1.):\n",
    "        self.Regularize()\n",
    "        if method == \"normal\":\n",
    "            self.moving = self.total_deri * step\n",
    "            self.moving = -1*CutValue(self.moving, move_max)\n",
    "        elif method == \"Rprop\":\n",
    "            self.moving = ArraySign(self.total_deri)\n",
    "            self.movint_return = ArraySign(self.total_deri*self.last_total_deri)\n",
    "            self.cwise_step *= 1.2*(self.movint_return>0) + 1.*(self.movint_return==0) + 0.5*(self.movint_return<0)\n",
    "            self.cwise_step = CutValue(self.cwise_step, move_max)\n",
    "            self.moving *= -1*self.cwise_step\n",
    "        else:\n",
    "            raise ValueError(\"descent method error\")\n",
    "        \n",
    "        self.value += self.moving\n",
    "        \n",
    "        self.last_total_deri = np.array(self.total_deri)\n",
    "        self.ZeroDeri()\n",
    "\n",
    "# VariableArray end\n",
    "\n",
    "# Activation functions defined start\n",
    "\n",
    "class Identity():\n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return np.ones(flow_in.shape, dtype = np.float64)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Sigmoid():\n",
    "    def Forward(self, flow_in):\n",
    "        return expit(flow_in)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return expit(flow_in)*expit(-flow_in)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Hypertan():\n",
    "    def Forward(self, flow_in):\n",
    "        flow_in = CutValue(flow_in, 100)\n",
    "        return np.tanh(flow_in)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        flow_in = CutValue(flow_in, 100) # cut value out of [-100, 100] to 100, cosh(-100) = cosh(100)\n",
    "        return 1. / np.square(np.cosh(flow_in))\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class SoftSign():\n",
    "    def Forward(self, flow_in):\n",
    "        return ArraySign(flow_in)*(1. - 1./(np.abs(flow_in) + 1.))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1. / np.square(np.abs(flow_in) + 1.)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Relu():\n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in*(flow_in>0)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1.*(flow_in>0)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class LeakyRelu():\n",
    "    def __init__(self, alpha = 0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in*(flow_in>0) + self.alpha*flow_in*(flow_in<0)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1.*(flow_in>0) + self.alpha*(flow_in<0)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class SoftPlus():\n",
    "    def Forward(self, flow_in):\n",
    "        return np.log(1. + np.exp(flow_in))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return expit(flow_in)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Selu():\n",
    "    def __init__(self):\n",
    "        self.ahpha = 1.05071\n",
    "        self.beta = 1.67326\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        return self.ahpha*(flow_in*(flow_in>=0) + self.beta*(np.exp(flow_in) - 1)*(flow_in<0))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return self.ahpha*(1.*(flow_in>=0) + self.beta*np.exp(flow_in)*(flow_in<0))\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Softmax():\n",
    "    def Forward(self, flow_in):\n",
    "        output = flow_in - flow_in.max(axis=1).reshape(-1,1)\n",
    "        output = np.exp(output)\n",
    "        output /= output.sum(axis=1).reshape(-1,1)\n",
    "        return output\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        flow_out = self.Forward(flow_in) # result of self.trans\n",
    "        return flow_out*back_flow - flow_out*((flow_out*back_flow).sum(axis=1).reshape(-1,1))\n",
    "\n",
    "# Activation functions defined end\n",
    "\n",
    "# Layer\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, num_unit, activationFunction):\n",
    "        if type(activationFunction) == type:\n",
    "            raise TypeError(\"activationFunction should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        self.num_unit = num_unit\n",
    "        self.activationFunction = activationFunction\n",
    "        self.linear = VariableArray((0, self.num_unit)) # linear weights working before active function\n",
    "        self.bias = VariableArray((1, self.num_unit)) # bias working before active function\n",
    "        self.flow_in = np.zeros((0, self.num_unit))\n",
    "        self.flow_out = np.zeros((0, self.num_unit))\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        self.flow_in = np.dot(flow_in, self.linear.value) + self.bias.value\n",
    "        self.flow_out = self.activationFunction.Forward(self.flow_in)\n",
    "    \n",
    "    def Backward(self, back_flow, layer_source):\n",
    "        deri = self.activationFunction.Backward(self.flow_in, back_flow)\n",
    "        self.linear.DeriModify(np.dot(layer_source.T, deri))\n",
    "        self.bias.DeriModify(np.sum(deri, axis=0).reshape(1, -1))\n",
    "        deri = np.dot(deri, self.linear.value.T)\n",
    "        return deri\n",
    "    \n",
    "    def ZeroDeri(self):\n",
    "        self.linear.ZeroDeri()\n",
    "        self.bias.ZeroDeri()\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        self.linear.SetRegularizer(rate, margin)\n",
    "        self.bias.SetRegularizer(rate, margin)\n",
    "    \n",
    "    def Descent(self, step, method):\n",
    "        self.linear.Descent(step, method)\n",
    "        self.bias.Descent(step, method)\n",
    "    \n",
    "    def ResetCwiseStep(self, new_cwise_step):\n",
    "        self.linear.ResetCwiseStep(new_cwise_step)\n",
    "        self.bias.ResetCwiseStep(new_cwise_step)\n",
    "    \n",
    "    def GetPCA(self, weight):\n",
    "        if IsNone(weight):\n",
    "            weight = np.ones((self.flow_out.shape[0])) / self.flow_out.shape[0]\n",
    "        \n",
    "        centered_flow_out = self.flow_out - self.flow_out.mean(axis=0)\n",
    "        cov = np.dot(centered_flow_out.T * weight, centered_flow_out) # covariance matrix\n",
    "        output = np.linalg.eigh(cov)\n",
    "        output[1] = output[1].T # transpose eigen vector from column to row\n",
    "        return output\n",
    "    \n",
    "    def GetDimension(self):\n",
    "        return self.linear.value.size + self.bias.value.size\n",
    "\n",
    "# Layer end\n",
    "\n",
    "# Loss function\n",
    "\n",
    "class LossFunction():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "    \n",
    "    def SetMethod(self, method):\n",
    "        self.method = method\n",
    "    \n",
    "    def GetLoss(self, inference, target, weight):\n",
    "        if self.method == \"r2\":\n",
    "            output = WeightedSum(np.square(inference - target).sum(axis=1), weight)\n",
    "            output /= WeightedSum(np.square(target - target.mean(axis=0)).sum(axis=1), weight)\n",
    "        elif self.method == \"cross entropy\":\n",
    "            output = WeightedSum((-target*np.log(inference)).sum(axis=1), weight)\n",
    "        else:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def Backward(self, inference, target, weight):\n",
    "        if self.method == \"r2\":\n",
    "            output = WeightedRow(2.*(inference - target), weight)\n",
    "            output /= WeightedSum(np.square(target - target.mean(axis=0)).sum(axis=1), weight)\n",
    "        elif self.method == \"cross entropy\":\n",
    "            output = WeightedRow(-(target/inference), weight)\n",
    "        else:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "class Terminator():\n",
    "    def __init__(self, short_period = 5, long_period = 10, threshold = 0.):\n",
    "        try:\n",
    "            short_period = int(short_period)\n",
    "            long_period = int(long_period)\n",
    "        except:\n",
    "            raise ValueError(\"period should be a value, and will be transfer to int\")\n",
    "        \n",
    "        if short_period <= 0:\n",
    "            short_period = 1\n",
    "            print(\"WARNING : short_period <= 0, set 1\")\n",
    "        \n",
    "        if long_period <= short_period:\n",
    "            long_period = short_period + 1\n",
    "            print(\"WARNING : long_period <= short_period, set short_period + 1\")\n",
    "        \n",
    "        self.short_period = short_period\n",
    "        self.long_period = long_period\n",
    "        self.threshold = threshold\n",
    "        self.record = []\n",
    "    \n",
    "    def Reset(self, short_period, long_period, threshold = 0.):\n",
    "        try:\n",
    "            short_period = int(short_period)\n",
    "            long_period = int(long_period)\n",
    "        except:\n",
    "            raise ValueError(\"period should be a value, and will be transfer to int\")\n",
    "        \n",
    "        if short_period <= 0:\n",
    "            short_period = 1\n",
    "            print(\"WARNING : short_period <= 0, set 1\")\n",
    "        \n",
    "        if long_period <= short_period:\n",
    "            long_period = short_period + 1\n",
    "            print(\"WARNING : long_period <= short_period, set %d\" %(short_period + 1))\n",
    "        \n",
    "        self.short_period = short_period\n",
    "        self.long_period = long_period\n",
    "        self.threshold = threshold\n",
    "        self.record = []\n",
    "    \n",
    "    def Hit(self, input_value):\n",
    "        try:\n",
    "            input_value = float(input_value)\n",
    "        except:\n",
    "            raise ValueError(\"input_value should be a real value\")\n",
    "        \n",
    "        self.record = [input_value] + self.record[:self.long_period-1]\n",
    "        if len(self.record) == self.long_period:\n",
    "            return (np.mean(self.record[:self.short_period]) - self.threshold > np.mean(self.record))\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def Clear(self):\n",
    "        self.record = []\n",
    "\n",
    "class DogikoNeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.hiddenLayerList = []\n",
    "        self.outputFunction = None\n",
    "        self.outputLayer = None\n",
    "        self.lossFunction = None\n",
    "        self.trainData = Data()\n",
    "        self.validData = Data()\n",
    "        self.testData = Data()\n",
    "        self.regulariz_rate = 0.\n",
    "        self.regulariz_margin = 0.\n",
    "        self.has_build = False\n",
    "        self.hit_tolerance = 0.1\n",
    "        self.terminator = Terminator()\n",
    "    \n",
    "    def SetLossFunction(self, method):\n",
    "        if method not in [\"r2\", \"cross entropy\"]:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        self.lossFunction = LossFunction(method)\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        if rate < 0.:\n",
    "            print(\"WARNING : regulariz_rate error, get negative value, setting to 0.\")\n",
    "        \n",
    "        if margin < 0.:\n",
    "            print(\"WARNING : regulariz_margin error, get negative value, setting to 0.\")\n",
    "        \n",
    "        self.regulariz_rate = max(rate, 0.)\n",
    "        self.regulariz_margin = max(margin, 0.)\n",
    "        \n",
    "        if self.has_build:\n",
    "            for l in range(self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "            \n",
    "            self.outputLayer.SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "    \n",
    "    def SetTrainData(self, inputs, labels, weight = None):\n",
    "        self.trainData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetValidData(self, inputs, labels, weight = None):\n",
    "        self.validData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetTestData(self, inputs, labels, weight = None):\n",
    "        self.testData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def AddHiddenLayer(self, num_unit, activationFunction):\n",
    "        self.hiddenLayerList.append(Layer(num_unit, activationFunction))\n",
    "        if self.has_build:\n",
    "            print(\"WARNING : a hidden layer added after build, please re-build model or set related value manually.\")\n",
    "    \n",
    "    def SetOutputFunction(self, activationFunction):\n",
    "        # units of output layer is fixed as same as label size\n",
    "        self.outputFunction = activationFunction\n",
    "        if self.has_build:\n",
    "            self.outputLayer.activationFunction = self.outputFunction\n",
    "    \n",
    "    def SetHitTolerance(self, tolerance):\n",
    "        try:\n",
    "            if float(tolerance) <= 0.:\n",
    "                print(\"setting tolerance failed, tolerance should be positive real value\")\n",
    "            \n",
    "            self.hit_tolerance = float(tolerance)\n",
    "        except:\n",
    "            print(\"setting tolerance failed, tolerance should be positive real value\")\n",
    "    \n",
    "    def SetTerminator(self, short_period, long_period, threshold = 0.):\n",
    "        self.terminator.Reset(short_period, long_period, threshold)\n",
    "        \n",
    "    def ClearTerminator(self):\n",
    "        self.terminator.Clear()\n",
    "    \n",
    "    def GetNumHiddenLayers(self):\n",
    "        return len(self.hiddenLayerList)\n",
    "    \n",
    "    def Build(self):\n",
    "        if IsNone(self.lossFunction):\n",
    "            raise ValueError(\"Set loss function before build.\")\n",
    "        \n",
    "        if self.lossFunction.method == \"cross entropy\":\n",
    "            if type(self.outputFunction) not in [Sigmoid, Softmax, SoftSign]:\n",
    "                print (\"WARNING : chosen loss function is cross entropy but the output of output layer function may out of (0, 1)\")\n",
    "            \n",
    "        \n",
    "        if IsNone(self.outputFunction):\n",
    "            self.outputFunction = Identity()\n",
    "            print (\"WARNING : doesn't set outputFunction before build, set Identity().\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[0],\n",
    "                    self.validData.GetDatumSize()[0],\n",
    "                    self.testData.GetDatumSize()[0]\n",
    "                   ])) == 1:\n",
    "            self.inputs_size = self.trainData.GetDatumSize()[0]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same inputs size\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[1],\n",
    "                    self.validData.GetDatumSize()[1],\n",
    "                    self.testData.GetDatumSize()[1]\n",
    "                   ])) == 1:\n",
    "            self.labels_size = self.trainData.GetDatumSize()[1]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same labels size\")\n",
    "        \n",
    "        # when hidden layer exist, set first layer value\n",
    "        if self.GetNumHiddenLayers() >0:\n",
    "            self.hiddenLayerList[0].linear.SetValue(np.random.normal(0.,\n",
    "                                                                     1.,\n",
    "                                                                     (self.inputs_size, self.hiddenLayerList[0].num_unit)\n",
    "                                                                    )\n",
    "                                                   )\n",
    "            self.hiddenLayerList[0].bias.SetValue(np.random.normal(0.,\n",
    "                                                                   1.,\n",
    "                                                                   (1, self.hiddenLayerList[0].num_unit)\n",
    "                                                                  )\n",
    "                                                 )\n",
    "        \n",
    "        # when hidden layer num >= 2, set internal layer value\n",
    "        for l in range(1, self.GetNumHiddenLayers()):\n",
    "            self.hiddenLayerList[l].linear.SetValue(np.random.normal(0.,\n",
    "                                                                     1.,\n",
    "                                                                     (self.hiddenLayerList[l-1].num_unit,\n",
    "                                                                      self.hiddenLayerList[l].num_unit\n",
    "                                                                     )\n",
    "                                                                    )\n",
    "                                                   )\n",
    "            self.hiddenLayerList[l].bias.SetValue(np.random.normal(0.,\n",
    "                                                                   1.,\n",
    "                                                                   (1, self.hiddenLayerList[l].num_unit)\n",
    "                                                                  )\n",
    "                                                 )\n",
    "        \n",
    "        # set output layer\n",
    "        self.outputLayer = Layer(self.labels_size, self.outputFunction)\n",
    "        if self.GetNumHiddenLayers() >0:\n",
    "            self.outputLayer.linear.SetValue(np.random.normal(0.,\n",
    "                                                              1.,\n",
    "                                                              (self.hiddenLayerList[-1].num_unit,\n",
    "                                                               self.outputLayer.num_unit\n",
    "                                                              )\n",
    "                                                             )\n",
    "                                            )\n",
    "        else:\n",
    "            self.outputLayer.linear.SetValue(np.random.normal(0.,\n",
    "                                                              1.,\n",
    "                                                              (self.inputs_size,\n",
    "                                                               self.outputLayer.num_unit\n",
    "                                                              )\n",
    "                                                             )\n",
    "                                            )\n",
    "        \n",
    "        self.outputLayer.bias.SetValue(np.random.normal(0.,\n",
    "                                                        1.,\n",
    "                                                        (1, self.outputLayer.num_unit)\n",
    "                                                       )\n",
    "                                      )\n",
    "        \n",
    "        # Set regularizer\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            self.hiddenLayerList[l].SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "        \n",
    "        self.outputLayer.SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "        \n",
    "        self.has_build = True\n",
    "    \n",
    "    def GetInference(self, inputs):\n",
    "        if inputs.shape[1] != self.inputs_size:\n",
    "            raise ValueError(\"inputs size shoud be %d, get %d\" %(self.inputs_size, inputs.shape[1]))\n",
    "        \n",
    "        if self.GetNumHiddenLayers() > 0:\n",
    "            self.hiddenLayerList[0].Forward(inputs)\n",
    "            for l in range(1, self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].Forward(self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            self.outputLayer.Forward(self.hiddenLayerList[-1].flow_out)\n",
    "        else:\n",
    "            self.outputLayer.Forward(inputs)\n",
    "        \n",
    "        return self.outputLayer.flow_out\n",
    "    \n",
    "    def GetLoss(self, inference, target, weight = None):\n",
    "        if inference.shape != target.shape:\n",
    "            raise ValueError(\"inference and target non-equal\")\n",
    "        \n",
    "        return self.lossFunction.GetLoss(inference, target, weight)\n",
    "    \n",
    "    def GetTrainLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.trainData.inputs), self.trainData.labels, self.trainData.weight)\n",
    "    \n",
    "    def GetValidLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.validData.inputs), self.validData.labels, self.validData.weight)\n",
    "    \n",
    "    def GetTestLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.testData.inputs), self.testData.labels, self.testData.weight)\n",
    "    \n",
    "    def GetAccuracy(self, inference, target, tolerance = 0.1):\n",
    "        # tolerance : for regression(r2) model, given a tolerance to verify hit or miss,\n",
    "        #             this variable is meaningless for classifycation\n",
    "        if self.lossFunction.method == \"cross entropy\":\n",
    "            output = (inference.argmax(axis=1) == target.argmax(axis=1)).mean()\n",
    "        elif self.lossFunction.method == \"r2\":\n",
    "            output = np.square(inference - target).sum(axis=1)\n",
    "            output = (output < tolerance).mean()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def GetTrainAccuracy(self, tolerance = None):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.trainData.inputs), self.trainData.labels, tolerance)\n",
    "    \n",
    "    def GetValidAccuracy(self, tolerance = None):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.validData.inputs), self.validData.labels, tolerance)\n",
    "    \n",
    "    def GetTestAccuracy(self, tolerance = None):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.testData.inputs), self.testData.labels, tolerance)\n",
    "    \n",
    "    def Backward(self, inputs, labels, weight):\n",
    "        deri = self.lossFunction.Backward(self.GetInference(inputs), labels, weight)\n",
    "        if self.GetNumHiddenLayers() >0:\n",
    "            deri = self.outputLayer.Backward(deri, self.hiddenLayerList[-1].flow_out)\n",
    "            for l in range(self.GetNumHiddenLayers()-1, 0, -1):\n",
    "                deri = self.hiddenLayerList[l].Backward(deri, self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            deri = self.hiddenLayerList[0].Backward(deri, inputs)\n",
    "        else:\n",
    "            deri = self.outputLayer.Backward(deri, inputs)\n",
    "        \n",
    "    def ZeroDeri(self):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            deri = self.hiddenLayerList[l].ZeroDeri()\n",
    "        \n",
    "        deri = self.outputLayer.ZeroDeri()\n",
    "    \n",
    "    def ResetCwiseStep(self):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            deri = self.hiddenLayerList[l].ResetCwiseStep()\n",
    "        \n",
    "        deri = self.outputLayer.ResetCwiseStep()\n",
    "    \n",
    "    def Descent(self, step = 1., method = \"normal\"):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            self.hiddenLayerList[l].Descent(step, method)\n",
    "        \n",
    "        self.outputLayer.Descent(step, method)\n",
    "    \n",
    "    def BatchFit(self, batch_inputs, batch_labels, batch_weight, step = 1., method = \"normal\"):\n",
    "        batch_weight /= batch_weight.sum()\n",
    "        self.Backward(batch_inputs, batch_labels, batch_weight)\n",
    "        self.Descent(step, method)\n",
    "    \n",
    "    def EpochFit(self, batch_size = None, step = 1., method = \"normal\"):\n",
    "        if type(batch_size) == type(None):\n",
    "            self.BatchFit(self.testData.inputs, self.testData.labels, self.testData.weight, step, method)\n",
    "        elif type(batch_size) == int:\n",
    "            if batch_size > 0:\n",
    "                for b in range(np.ceil(self.testData.GetNumDatums()/ batch_size).astype(np.int)):\n",
    "                    self.BatchFit(self.testData.inputs[b*batch_size: (b+1)*batch_size],\n",
    "                                  self.testData.labels[b*batch_size: (b+1)*batch_size],\n",
    "                                  step,\n",
    "                                  method\n",
    "                                 )\n",
    "            else:\n",
    "                raise ValueError(\"batch_size should be positive int\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"batch_size should be positive int\")\n",
    "    \n",
    "    def Train(self, times, batch_size = None, step = 1., method = \"normal\", is_termination = False):\n",
    "        self.ResetCwiseStep()\n",
    "        self.terminator.Clear()\n",
    "        for t in range(times):\n",
    "            self.EpochFit(batch_size, step, method)\n",
    "            if is_termination:\n",
    "                if self.terminator.Hit(10*np.log10(self.GetValidLoss() + 0.000000001)):\n",
    "                # 0.000000001, bias for prevent error when log(0)\n",
    "                    return t+1\n",
    "        \n",
    "        return times\n",
    "    \n",
    "    def AddUnit(self, layer_index, num_added = 1, output_linear_bound = 1.):\n",
    "        if layer_index not in range(self.GetNumHiddenLayers() + ):\n",
    "            raise ValueError(\"layer_index should be an int from 0 to (#layer-1) for hiddenlayer\")\n",
    "        \n",
    "        if type(num_added) != int:\n",
    "            raise ValueError(\"num_added should be positive int\")\n",
    "        elif num_added <= 0:\n",
    "            raise ValueError(\"num_added should be positive int\")\n",
    "        \n",
    "        try:\n",
    "            if output_linear_bound < 0.:\n",
    "                raise ValueError(\"output_weight_bound should be non-negative\")\n",
    "        except:\n",
    "            raise ValueError(\"output_weight_bound should a non-negative real value\")\n",
    "        \n",
    "        for u in range(num_added):\n",
    "            self.hiddenLayerList[layer_index].linear.\n",
    "            \n",
    "            #self.ly[].w.add_row(mcmc_normal(self.ly[l].w.v, mean=self.ly[l].w.v.mean(), std=self.ly[l].w.v.std()))\n",
    "            #self.ly[l].b.add_row(mcmc_normal(self.ly[l].b.v, mean=self.ly[l].b.v.mean(), std=self.ly[l].b.v.std()))\n",
    "            #self.ly[l+1].w.add_column(output_weight_bound*(2*np.random.rand((self.ly[l+1].nn))-1.))\n",
    "            #self.ly[l].nn += 1\n",
    "        \n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    def neuron_refined(self, l, reference_data = None, threshold = 0.01):\n",
    "        # l : the # of layer\n",
    "        # threshold : threshold for information contained of dimension be remaind\n",
    "        if type(l) != int:\n",
    "            raise TypeError(\"l should be the layer no. of hidden layer, an int between 0 to (neural_number - 2)\")\n",
    "        elif (l >= self.ln - 1) or (l < 0):\n",
    "            raise ValueError(\"l should be the layer no. of hidden layer, an int between 0 to (neural_number - 2)\")\n",
    "        \n",
    "        try:\n",
    "            if ((threshold< 1) and (threshold>0)) or (type(threshold) == int):\n",
    "                if (threshold > self.ly[l].nn-1):\n",
    "                    raise ValueError(\"int threshold error : removed #neuron should less than currently #neuron\")\n",
    "                elif (threshold < -self.ly[l].nn) or (threshold==0):\n",
    "                    return None\n",
    "                    # do nothing if remove no #neuron (threshold=0) or want to remain #neuron more than currently\n",
    "            else:\n",
    "                raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "        except:\n",
    "            raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "        \n",
    "        if type(reference_data) == type(None):\n",
    "            self.prediction(self.tx)\n",
    "        else:\n",
    "            self.prediction(reference_data)\n",
    "        \n",
    "        ym = self.ly[l].y.mean(axis=1).reshape((self.ly[l].nn,1)) # y (output of Layer) mean of each neurons\n",
    "        yn = self.ly[l].y - ym # centralized y\n",
    "        ab = np.dot(self.ly[l+1].w.v, ym) # Adjusted bias\n",
    "        ev, em = np.linalg.eigh(np.dot(yn, yn.T)) # eigenvalues and eigenmatrix(with eigenvectors as columns)\n",
    "        ir = ev/ev.sum() # info ratio for each eigenvector\n",
    "        # op, pv :column operator result and pivots\n",
    "        if (threshold< 1) and (threshold>0):\n",
    "            op, pv = column_operate(em[:,ir > threshold])\n",
    "        else:\n",
    "            op, pv = column_operate(em[:,ir >= ir[ir.argsort()[threshold]]])\n",
    "            \n",
    "        nw = np.dot(self.ly[l+1].w.v, op) # new weight\n",
    "        self.ly[l+1].b.assign_values(self.ly[l+1].b.v + (np.dot(self.ly[l+1].w.v, ym) -np.dot(nw, ym[pv])))\n",
    "        self.ly[l+1].w.assign_values(nw) # l+1 weight should be rewrite after l+1 bias have been rewrite\n",
    "        self.ly[l].w.assign_values(self.ly[l].w.v[pv])\n",
    "        self.ly[l].b.assign_values(self.ly[l].b.v[pv])\n",
    "        self.ly[l].nn = len(pv)\n",
    "    \n",
    "    def neuron_proliferate(self, proliferating_layer, proliferating_n = 1, output_weight_bound = 1.):\n",
    "        if proliferating_layer not in range(self.ln):\n",
    "            raise ValueError(\"proliferating_layer should be an int from 0 to (#layer-1)\")\n",
    "            \n",
    "        if type(proliferating_n) != int:\n",
    "            raise ValueError(\"proliferating_n should be int\")\n",
    "        \n",
    "        if proliferating_n <= 0:\n",
    "            raise ValueError(\"proliferating_n should be postive\")\n",
    "            \n",
    "        if output_weight_bound < 0.:\n",
    "            raise ValueError(\"output_weight_bound should be non-negative\")\n",
    "            \n",
    "        l = proliferating_layer\n",
    "        for t in range(proliferating_n):\n",
    "            self.ly[l].w.add_row(mcmc_normal(self.ly[l].w.v, mean=self.ly[l].w.v.mean(), std=self.ly[l].w.v.std()))\n",
    "            self.ly[l].b.add_row(mcmc_normal(self.ly[l].b.v, mean=self.ly[l].b.v.mean(), std=self.ly[l].b.v.std()))\n",
    "            self.ly[l+1].w.add_column(output_weight_bound*(2*np.random.rand((self.ly[l+1].nn))-1.))\n",
    "            self.ly[l].nn += 1\n",
    "    \n",
    "    def reset_cs(self, new_cs):\n",
    "        for l in range(self.ln):\n",
    "            self.ly[l].reset_cs(new_cs)\n",
    "    \n",
    "    def inter_layer_linear_regression(self, layer_interval):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                if ls == 0:\n",
    "                    ri = np.array(self.px.T) # regression input\n",
    "                else:\n",
    "                    ri = np.array(self.ly[ls-1].y)\n",
    "                \n",
    "                ri = np.append(ri, np.ones((1, ri.shape[1])), axis=0) # append 1. for each datum as bias\n",
    "                ro = np.array(self.ly[le].x)\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        rr = np.linalg.lstsq(ri.T, ro.T) # regression result (matrix, residuals, rank of ri, singuler values of ri)\n",
    "        if len(rr[1]) == 0:\n",
    "            raise ValueError(\"output data of layer\" + str(ls-1) + \"(= -1, for input data) should be full rank, try self.nruron_refine first\")\n",
    "        \n",
    "        return rr[0], rr[1]/ri.shape[1]\n",
    "    \n",
    "    def find_linearist_layers(self, reference_data = None):\n",
    "        output = (0, 0, np.inf, np.array([[]]), np.zeros((0,0)))\n",
    "        if type(reference_data) == type(None):\n",
    "            self.prediction(self.tx)\n",
    "        else:\n",
    "            self.prediction(reference_data)\n",
    "        \n",
    "        for l1 in range(self.ln-1):\n",
    "            for l2 in range(i+1, self.ln):\n",
    "                rr = self.inter_layer_linear_regression((l1,l2))\n",
    "                if np.sqrt(rr[1].sum()) < output[2]:\n",
    "                    output = (l1, l2, np.sqrt(rr[1].sum()), rr[0])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def layer_filled(self, layer_interval, weights, bias):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        if weights.shape[0] != bias.shape[0]:\n",
    "            raise ValueError(\"weights.shape[0] doesn't match bias.shape[0]\")\n",
    "        \n",
    "        if weights.shape[0] != self.ly[le].nn:\n",
    "            raise ValueError(\"weights.shape[0] doesn't match #neuron of layer at end of layer_interval\")\n",
    "        \n",
    "        self.ly[le].w.assign_values(weights)\n",
    "        self.ly[le].b.assign_values(bias)\n",
    "        self.ly = self.ly[:ls] + self.ly[le:]\n",
    "        self.ln = len(self.ly)\n",
    "    \n",
    "    def linear_filled(self, layer_interval):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "            \n",
    "        rr = self.inter_layer_linear_regression(layer_interval)\n",
    "        self.layer_filled(layer_interval, rr[0].T[:,:-1], rr[0].T[:,-1:])\n",
    "    \n",
    "    def insert_layer(self, position, weights, bias, activation_function, next_layer_weights, next_layer_bias):\n",
    "        if type(position) == int:\n",
    "            if position in range(self.ln):\n",
    "                pass\n",
    "        else:\n",
    "            raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        \n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        ilo, ili = weights.shape # input and output size of inserted layer\n",
    "        nlo, nli = next_layer_weights.shape # input and output size of next layer\n",
    "        \n",
    "        if position == 0:\n",
    "            if ili != self.xs:\n",
    "                raise ValueError(\"weights.shape error, cheak input and output size for this new layer\")\n",
    "        else:\n",
    "            if ili != self.ly[position-1].nn:\n",
    "                raise ValueError(\"weights.shape error, cheak input and output size for this new layer\")\n",
    "        \n",
    "        if (ilo != bias.shape[0]) or (ilo != nli):\n",
    "            raise ValueError(\"to define #neuron of new layer, all related weighs and bias size should be consistent\")\n",
    "        \n",
    "        if nlo != self.ly[position].nn:\n",
    "            raise ValueError(\"next_layer_weights.shape error, cheak #neuron of next layer\")\n",
    "        \n",
    "        if next_layer_bias.shape[0] != self.ly[position].nn:\n",
    "            raise ValueError(\"next_layer_bias.shape error, cheak #neuron of next layer\")\n",
    "        \n",
    "        if (bias.shape[1] != 1) or (next_layer_bias.shape[1] != 1):\n",
    "            raise ValueError(\"bias shape should be (#neuron, 1)\")\n",
    "        \n",
    "        l = position\n",
    "        \n",
    "        self.ly.insert(l, Layer(ilo, activation_function))\n",
    "        self.ly[l].w.assign_values(weights)\n",
    "        self.ly[l].b.assign_values(bias)\n",
    "        self.ly[l+1].w.assign_values(next_layer_weights)\n",
    "        self.ly[l+1].b.assign_values(next_layer_bias)\n",
    "        \n",
    "        self.ln = len(self.ly)\n",
    "    \n",
    "    def identity_dig(self, position, activation_function):\n",
    "        if type(position) == int:\n",
    "            if position in range(self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        else:\n",
    "            raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        \n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        l = position\n",
    "        # ids : size of identity transform, input size of new layer\n",
    "        if l == 0:\n",
    "            ids = self.xs\n",
    "        else:\n",
    "            ids = self.ly[l-1].nn\n",
    "        \n",
    "        if type(activation_function) in [Relu, SoftPlus]:\n",
    "            liw = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 0)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 1)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) == LeakyRelu:\n",
    "            liw = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 0)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 1) / (1.+activation_function.alpha)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) == Identity:\n",
    "            liw = np.identity(ids)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.identity(ids)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) in [Sigmoid, Hypertan, Selu]:\n",
    "            # li : input of new layer\n",
    "            if l == 0:\n",
    "                li = np.array(self.tx.T)\n",
    "            else:\n",
    "                li = np.array(self.ly[l-1].y)\n",
    "            \n",
    "            lim = li.mean(axis=1)\n",
    "            lis = li.std(axis=1) + 1.\n",
    "            \n",
    "            liw = np.diag(1./lis)\n",
    "            if type(activation_function) == Selu:\n",
    "                lib = 1.-(lim/lis).reshape(-1,1) # let mean become one before transform by activation function\n",
    "            else:\n",
    "                lib = -(lim/lis).reshape(-1,1) # let mean become zero before transform by activation function\n",
    "            \n",
    "            lo = activation_function.trans(np.dot(liw, li)+lib)\n",
    "            lo = np.append(lo, np.ones((1, lo.shape[1])), axis=0) # append 1. for each datum as bias\n",
    "            rr = np.linalg.lstsq(lo.T, li.T) # regression result (matrix, residuals, rank of ri, singuler values of ri)\n",
    "            # since the goal is construct identity, try to find linear transform form layer output to layer input\n",
    "            low = rr[0].T[:,:-1]\n",
    "            lob = rr[0].T[:,-1:]\n",
    "        else:\n",
    "            raise TypeError(\"activation_function type error\")\n",
    "        \n",
    "        nlw = np.dot(self.ly[l].w.v, low)\n",
    "        nlb = np.dot(self.ly[l].w.v, lob) + self.ly[l].b.v\n",
    "        \n",
    "        self.insert_layer(l,\n",
    "                          liw,\n",
    "                          lib,\n",
    "                          activation_function,\n",
    "                          nlw,\n",
    "                          nlb\n",
    "                         )\n",
    "    \n",
    "    def dimension(self):\n",
    "        output = 0\n",
    "        for l in range(self.ln):\n",
    "            output += self.ly[l].dimension()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def save_weight(self, dir_name):\n",
    "        for l in range(self.ln):\n",
    "            np.save(dir_name + \"/w%i.npy\" % l, self.ly[l].w.v)\n",
    "            np.save(dir_name + \"/b%i.npy\" % l, self.ly[l].b.v)\n",
    "    \n",
    "    def load_weight(self, dir_name):\n",
    "        for l in range(self.ln):\n",
    "            try:\n",
    "                if l == 0:\n",
    "                    if np.load(dir_name + \"/w%i.npy\" % l).shape[1] != self.xs:\n",
    "                        raise ValueError(\"layer %i input size error, cheak weight size.\" % l)\n",
    "                else:\n",
    "                    if np.load(dir_name + \"/w%i.npy\" % l).shape[1] != self.ly[l-1].nn:\n",
    "                        raise ValueError(\"layer %i input size error, cheak weight size.\" % l)\n",
    "\n",
    "                if np.load(dir_name + \"/w%i.npy\" % l).shape[0] != self.ly[l].nn:\n",
    "                    raise ValueError(\"layer %i neuron size error, cheak weight size.\" % l)\n",
    "\n",
    "                if np.load(dir_name + \"/b%i.npy\" % l).shape[0] != self.ly[l].nn:\n",
    "                    raise ValueError(\"layer %i neuron size error, cheak bias size.\" % l)\n",
    "\n",
    "                if np.load(dir_name + \"/b%i.npy\" % l).shape[1] != 1:\n",
    "                    raise ValueError(\"layer %i bias size error, should be 1.\" % l)\n",
    "            \n",
    "            except:\n",
    "                raise ValueError(\"load .npy error, cheak dir.\")\n",
    "            \n",
    "            self.ly[l].w.assign_values(np.load(dir_name + \"/w%i.npy\" % l))\n",
    "            self.ly[l].b.assign_values(np.load(dir_name + \"/b%i.npy\" % l))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ((np.arange(300)-150)/100).reshape(-1,1)\n",
    "Y = np.zeros((X.shape[0], 2))\n",
    "Y[:, :1] = 1*(X<0.)\n",
    "Y[:, 1:] = 1 - Y[:, :1]\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X, Y)\n",
    "NN.SetValidData(X, Y)\n",
    "NN.SetTestData(X, Y)\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 3.)\n",
    "NN.AddHiddenLayer(4, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,20,-0.1)\n",
    "NN.Build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0032287947607516788"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(NN.Train(times=500, method=\"Rprop\", is_termination=True))\n",
    "NN.GetTrainLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.99830859, 143.83598636]), array([[-0.59992686, -0.80005485],\n",
       "        [-0.80005485,  0.59992686]]))"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = np.dot(np.random.normal(0., 1., (1000000,1)), np.array([[0.6, 0.8]]))\n",
    "foo += np.dot(np.random.normal(0., 12., (1000000,1)), np.array([[0.8, -0.6]]))\n",
    "np.linalg.eigh(np.dot(foo.T, foo)/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a6f5737dafd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   5053\u001b[0m             raise IndexError(\n\u001b[1;32m   5054\u001b[0m                 \u001b[0;34m\"index %i is out of bounds for axis %i with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5055\u001b[0;31m                 \"size %i\" % (obj, axis, N))\n\u001b[0m\u001b[1;32m   5056\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5057\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "np.insert(np.arange(6).reshape(3, 2), 3, np.arange(2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
