{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ArraySign(input_array):\n",
    "    # return +1, 0, -1 respect to positive, zero, negtive\n",
    "    return 1.*(input_array>0) - 1.*(input_array<0)\n",
    "\n",
    "def CutValue(input_array, cut_value):\n",
    "    output = np.abs(input_array)\n",
    "    output = ArraySign(input_array) * (output * (output < cut_value) + cut_value * (output >= cut_value))\n",
    "    return output\n",
    "\n",
    "def OverPenalty(input_value, rate = 0.1, threshold=0.):\n",
    "    output = np.abs(input_value) - threshold\n",
    "    output *= (output > 0)\n",
    "    output *= rate * ArraySign(input_value)\n",
    "    return output\n",
    "\n",
    "def RowOperate(matrix, threshold = 0.1**15):\n",
    "    reduced_matrix = np.array(matrix)\n",
    "    filtered_matrix = np.array(matrix)\n",
    "    shape = matrix.shape # matrix size\n",
    "    mask = np.ones(shape)\n",
    "    pivots = -1*np.ones((min(shape)), dtype = np.int) # store pivots, # of pivots <= min(rows, columns)\n",
    "    for t in range(len(pivots)):\n",
    "        filtered_matrix = reduced_matrix * mask # filter\n",
    "        if np.abs(filtered_matrix).max() < threshold:\n",
    "            break\n",
    "        \n",
    "        pivot_row, pivot_col = np.unravel_index(np.abs(filtered_matrix).argmax(), shape) # pivot row, pivot column\n",
    "        reduced_matrix[pivot_row] /= reduced_matrix[pivot_row][pivot_col]\n",
    "        multi = np.array(reduced_matrix[:, pivot_col])\n",
    "        multi[pivot_row] = 0.\n",
    "        reduced_matrix -= np.dot(multi.reshape((shape[0], 1)), reduced_matrix[pivot_row].reshape((1, shape[1])))\n",
    "        mask[pivot_row] = 0.\n",
    "        mask[:, pivot_col] = 0.\n",
    "        pivots[pivot_row] = pivot_col # the column-index of pivot_row-th row is pivot_col\n",
    "    \n",
    "    reduced_matrix = reduced_matrix[pivots != -1,:]\n",
    "    pivots = pivots[pivots != -1]\n",
    "    \n",
    "    return reduced_matrix, pivots\n",
    "\n",
    "def McmcNormal(points, drop_times = 10, mean=0., std=1.):\n",
    "    output = np.random.normal(mean, std/np.sqrt(points[0].size), points.shape[1:])\n",
    "    if drop_times>1:\n",
    "        for t in range(1, drop_times):\n",
    "            candicate = np.random.normal(mean, std/np.sqrt(points[0].size), points.shape[1:])\n",
    "            candicate_distance = np.sqrt(np.square(np.subtract(points, candicate)).sum(axis=tuple(np.arange(1, len(points.shape))))).min()\n",
    "            # distance of candicate to target\n",
    "            output_distance = np.sqrt(np.square(np.subtract(points, output)).sum(axis=tuple(np.arange(1, len(points.shape))))).min()\n",
    "            # distance of currently output to target\n",
    "            if np.random.rand()*output_distance < candicate_distance:\n",
    "                output = np.array(candicate)\n",
    "    \n",
    "    return output\n",
    "\n",
    "class VariableArray():\n",
    "    def __init__(self, size, cwise_step_initial=0.1):\n",
    "        self.value = np.random.normal(0., 1., size) # array value\n",
    "        self.total_deri = np.zeros(self.value.shape) # total derivative, used to descent\n",
    "        self.last_total_deri = None # last total derivative\n",
    "        self.moving = np.zeros(self.value.shape) # moving array\n",
    "        self.cwise_step = cwise_step_initial*np.ones(self.value.shape) # component-wise step\n",
    "        \n",
    "        self.regulariz_rate = 0.\n",
    "        self.regulariz_margin = 0.\n",
    "    \n",
    "    def SetValue(self, input_value, cwise_step_initial=0.1):\n",
    "        self.value = np.array(input_value) # array value\n",
    "        self.total_deri = np.zeros(self.value.shape) # total derivative, used to descent\n",
    "        self.last_total_deri = None # last total derivative\n",
    "        self.moving = np.zeros(self.value.shape) # moving array\n",
    "        self.cwise_step = cwise_step_initial*np.ones(self.value.shape) # component-wise step\n",
    "    \n",
    "    def SetDeri(self, input_value):\n",
    "        if input_value.shape != self.total_deri.shape:\n",
    "            raise ValueError(\"input_value shape error\")\n",
    "        \n",
    "        self.last_total_deri = np.array(self.input_value)\n",
    "        self.total_deri = np.array(input_value)\n",
    "    \n",
    "    def DeriModify(self, input_value):\n",
    "        if input_value.shape != self.total_deri.shape:\n",
    "            raise ValueError(\"input_value shape error\")\n",
    "        \n",
    "        self.total_deri += input_value\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        self.regulariz_rate = max(rate, 0.)\n",
    "        self.regulariz_margin = max(margin, 0.)\n",
    "    \n",
    "    def AddRow(self, input_row, cwise_step_initial=0.1):\n",
    "        self.value = np.append(self.value, np.array([input_row]), axis = 0)\n",
    "        self.total_deri = np.append(self.total_deri, np.zeros((1,) + input_row.shape), axis = 0)\n",
    "        self.last_total_deri = None\n",
    "        self.moving = np.zeros(self.value.shape)\n",
    "        self.cwise_step = np.append(self.cwise_step, cwise_step_initial * np.ones((1,) + input_row.shape), axis = 0)\n",
    "    \n",
    "    def AddColumn(self, input_column, cwise_step_initial=0.1):\n",
    "        self.value = np.append(self.value, np.array([input_column]).T, axis = 1)\n",
    "        self.total_deri = np.append(self.total_deri, np.zeros(input_column.shape + (1,)), axis = 1)\n",
    "        self.last_total_deri = None\n",
    "        self.moving = np.zeros(self.value.shape)\n",
    "        self.cwise_step = np.append(self.cwise_step, cwise_step_initial * np.ones(input_column.shape + (1,)), axis = 1)\n",
    "    \n",
    "    def ResetCwiseStep(self, input_cwise_step):\n",
    "        self.cwise_step = input_cwise_step * np.ones(self.cwise_step.shape)\n",
    "    \n",
    "    def Regularize(self):\n",
    "        self.total_deri -= OverPenalty(self.value, self.regulariz_rate, self.regulariz_margin)\n",
    "    \n",
    "    def Descent(self, step=1., method=\"normal\", move_max=1.):\n",
    "        self.Regularize()\n",
    "        if method == \"normal\":\n",
    "            self.moving = self.total_deri * step\n",
    "        elif method == \"Rprop\":\n",
    "            self.moving = ArraySign(self.total_deri)\n",
    "            self.movint_return = ArraySign(self.total_deri*self.last_total_deri)\n",
    "            self.cwise_step *= 1.2*(self.movint_return>0) + 1.*(self.movint_return==0) + 0.5*(self.movint_return<0)\n",
    "            self.moving *= self.cwise_step\n",
    "        else:\n",
    "            raise ValueError(\"descent method error\")\n",
    "        \n",
    "        self.moving = -1*CutValue(self.moving, move_max)\n",
    "        self.value += self.moving\n",
    "\n",
    "    def \n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Activation functions defined start\n",
    "\n",
    "class Identity():\n",
    "    def trans(self, x):\n",
    "        return x\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return np.ones(x.shape, dtype = np.float64)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Sigmoid():\n",
    "    def trans(self, x):\n",
    "        return expit(x)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return expit(x)*expit(-x)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Hypertan():\n",
    "    def trans(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        x[(x<-100)&(x>100)] = 100 # cut value out of [-100, 100] to 100, cosh(-100) = cosh(100)\n",
    "        return 1. / np.square(np.cosh(x))\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class SoftSign():\n",
    "    def trans(self, x):\n",
    "        return array_sign(x)*(1. - 1./(np.abs(x) + 1.))\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1. / np.square(np.abs(x) + 1.)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Relu():\n",
    "    def trans(self, x):\n",
    "        return x*(x>0)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1.*(x>0)\n",
    "    \n",
    "    def backward(self, x, _input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class LeakyRelu():\n",
    "    def __init__(self, alpha = 0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def trans(self, x):\n",
    "        return x*(x>0) + self.alpha*x*(x<0)\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return 1.*(x>0) + self.alpha*(x<0)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class SoftPlus():\n",
    "    def trans(self, x):\n",
    "        return np.log(1. + np.exp(x))\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return expit(x)\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Selu():\n",
    "    def __init__(self):\n",
    "        self.ahpha = 1.05071\n",
    "        self.beta = 1.67326\n",
    "    \n",
    "    def trans(self, x):\n",
    "        return self.ahpha*(x*(x>=0) + self.beta*(np.exp(x) - 1)*(x<0))\n",
    "    \n",
    "    def diff(self, x):\n",
    "        return self.ahpha*(1.*(x>=0) + self.beta*np.exp(x)*(x<0))\n",
    "    \n",
    "    def backward(self, x,_input):\n",
    "        return self.diff(x)*_input\n",
    "\n",
    "class Softmax():\n",
    "    def trans(self, x):\n",
    "        output = x - x.max(axis=0)\n",
    "        output = np.exp(output)\n",
    "        output /= output.sum(axis=0)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, x, _input):\n",
    "        tr = self.trans(x) # result of self.trans\n",
    "        return tr*_input - tr*((tr*_input).sum(axis=0))\n",
    "\n",
    "# Activation functions defined end\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, neuron_n, activation_function):\n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        self.nn = neuron_n\n",
    "        self.af = activation_function\n",
    "        self.w = VariableArray((self.nn, 0)) # linear weights working before active function\n",
    "        self.b = VariableArray((self.nn, 1)) # bias working before active function\n",
    "        self.x = np.zeros((0, self.nn))\n",
    "        self.y = np.zeros((0, self.nn))\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        self.x = np.dot(self.w.v, _input) + self.b.v\n",
    "        self.y = self.af.trans(self.x)\n",
    "    \n",
    "    def backward(self, _input, source):\n",
    "        derivative = self.af.backward(self.x, _input)\n",
    "        self.w.derivative_assign(np.dot(derivative, source.T))\n",
    "        self.b.derivative_assign(np.sum(derivative, axis=1).reshape(derivative.shape[0], 1))\n",
    "        derivative = np.dot(derivative.T, self.w.v)\n",
    "        return derivative.T\n",
    "    \n",
    "    def descent(self, step, descent_method, regularizer):\n",
    "        self.w.descent(step, descent_method, regularizer)\n",
    "        self.b.descent(step, descent_method, regularizer)\n",
    "    \n",
    "    def reset_cs(self, new_cs):\n",
    "        self.w.reset_cs(new_cs)\n",
    "        self.b.reset_cs(new_cs)\n",
    "    \n",
    "    def dimension(self):\n",
    "        return self.w.v.size + self.b.v.size\n",
    "\n",
    "class DogikoLearn():\n",
    "    def __init__(self, loss_function = \"r2\"):\n",
    "        self.lf = loss_function # loss function type\n",
    "        self.ly = [] # layers list\n",
    "        self.rg = (\"None\",) # Regularizetion method\n",
    "        self.csi = 0.1 # initial component-wise step when claim new weights and bias\n",
    "    \n",
    "    def r_square_regularizer(self, alpha):\n",
    "        # Assign regularization method as radius square\n",
    "        # i.e Error += alpha*0.5*sum(weight**2) when descent\n",
    "        if alpha <= 0:\n",
    "            raise ValueError(\"Input should be positive\")\n",
    "        \n",
    "        self.rg = (\"r_square\", alpha)\n",
    "    \n",
    "    def rs_extend_regularizer(self, alpha, beta):\n",
    "        # Assign regularization method as radius square\n",
    "        # i.e Error += alpha*0.5sum(weight**2) when descent\n",
    "        if (alpha <= 0) or (alpha <= 0):\n",
    "            raise ValueError(\"All input should be positive\")\n",
    "        \n",
    "        self.rg = (\"rs_extend\", alpha, beta)\n",
    "    \n",
    "    def set_training_data(self, training_input, training_labels):\n",
    "        self.tx = np.array(training_input) # training data input\n",
    "        self.ty = np.array(training_labels) # training data lables(answers)\n",
    "        if self.tx.shape[0] != self.ty.shape[0]:\n",
    "            temp_min = min(self.tx.shape[0], self.ty.shape[0])\n",
    "            self.tx = self.tx[:temp_min]\n",
    "            self.ty = self.ty[:temp_min]\n",
    "            print(\"training data #input != #output, took the minimun size automatically\")\n",
    "        \n",
    "        self.xs = self.tx.shape[1] # size of each datum input\n",
    "        self.ys = self.ty.shape[1] # size of each datum output\n",
    "    \n",
    "    def set_validation_data(self, validation_input, validation_labels):\n",
    "        self.vx = np.array(validation_input) # validation data input\n",
    "        self.vy = np.array(validation_labels) # validation data lables(answers)\n",
    "        if self.vx.shape[1] != self.xs:\n",
    "            raise ValueError(\"validation data input size should be equal to training data\")\n",
    "        \n",
    "        if self.vy.shape[1] != self.ys:\n",
    "            raise ValueError(\"validation data lables size should be equal to training data\")\n",
    "    \n",
    "    def add_layer(self, new_layer):\n",
    "        if type(new_layer) != Layer:\n",
    "            raise TypeError(\"new_layer should be a Layer (class). eg: 'Layer(30, Sigmoid())'\")\n",
    "        \n",
    "        self.ly.append(new_layer)\n",
    "    \n",
    "    def build(self):\n",
    "        self.ln = len(self.ly) # amount of layers\n",
    "        self.ly[0].w.assign_values(np.random.normal(0., 1., (self.ly[0].nn, self.xs)), self.csi)\n",
    "        self.ly[0].b.assign_values(np.random.normal(0., 1., (self.ly[0].nn, 1)), self.csi)\n",
    "        for l in range(1,self.ln):\n",
    "            self.ly[l].w.assign_values(np.random.normal(0., 1., (self.ly[l].nn, self.ly[l-1].nn)), self.csi)\n",
    "            self.ly[l].b.assign_values(np.random.normal(0., 1., (self.ly[l].nn, 1)), self.csi)\n",
    "        \n",
    "        if self.ly[-1].nn != self.ys: # cheak output size\n",
    "            raise ValueError(\"output layer must has the same size with datum lables(answer)\")\n",
    "    \n",
    "    def prediction(self, data_input):\n",
    "        self.px = np.array(data_input) # prediction data input of last time predic\n",
    "        if self.px.shape[1] != self.xs:\n",
    "            raise ValueError(\"datum size error\")\n",
    "        \n",
    "        self.ly[0].forward(self.px.T)\n",
    "        for l in range(1,self.ln):\n",
    "            self.ly[l].forward(self.ly[l-1].y)\n",
    "        \n",
    "        self.py = np.array(self.ly[-1].y.T) # prediction result of last time predict\n",
    "        \n",
    "        return self.py\n",
    "    \n",
    "    def descent(self, step = 1., descent_method = \"normal\"):\n",
    "        for l in range(self.ln):\n",
    "            self.ly[l].descent(step, descent_method, self.rg)\n",
    "        \n",
    "        if descent_method in [\"Rprop\", \"Dogiko Rprop\"]:\n",
    "            self.max_cs = 0.\n",
    "            for l in range(self.ln):\n",
    "                self.max_cs = max(self.max_cs, self.ly[l].w.max_cs(), self.ly[l].b.max_cs())\n",
    "    \n",
    "    def evaluate(self, _input, labels):\n",
    "        self.prediction(_input)\n",
    "        if self.lf == \"r2\":\n",
    "            return np.square(self.py - labels).mean()/labels.var(axis=0).mean()\n",
    "        elif self.lf == \"ce\":\n",
    "            return (-1*labels*np.log(self.py+0.0001)).sum(axis=1).mean()\n",
    "        else:\n",
    "            raise ValueError(\"loss function should be 'r2' or 'ce'\")\n",
    "    \n",
    "    def gradient_get(self, _input, labels):\n",
    "        self.prediction(_input)\n",
    "        if self.lf == \"r2\":\n",
    "            temp_derivative = 2*(self.py - labels).T/(labels.shape[0]*labels.var(axis=0).sum())\n",
    "        elif self.lf == \"ce\":\n",
    "            temp_derivative = -1*(labels/(self.py + 0.0001)).T/labels.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"loss function should be 'r2' or 'ce'\")\n",
    "        \n",
    "        for l in range(self.ln-1, 0, -1):\n",
    "            temp_derivative = self.ly[l].backward(temp_derivative, self.ly[l-1].y)\n",
    "        \n",
    "        self.ly[0].backward(temp_derivative, _input.T)\n",
    "    \n",
    "    def batch_fit(self, batch_input, batch_labels, step = 1., descent_method = \"normal\"):\n",
    "        self.gradient_get(batch_input, batch_labels)\n",
    "        self.descent(step, descent_method)\n",
    "    \n",
    "    def epoch_fit(self, batch_size = None, step = 1., descent_method = \"normal\"):\n",
    "        if type(batch_size) == type(None):\n",
    "            self.batch_fit(self.tx, self.ty, step, descent_method)\n",
    "        elif type(batch_size) == int:\n",
    "            if batch_size > 0:\n",
    "                for b in range(np.ceil(self.tx.shape[0]/ batch_size).astype(np.int)):\n",
    "                    self.batch_fit(self.tx[b*batch_size: (b+1)*batch_size],\n",
    "                                   self.ty[b*batch_size: (b+1)*batch_size],\n",
    "                                   step,\n",
    "                                   descent_method\n",
    "                                  )\n",
    "            else:\n",
    "                raise ValueError(\"batch_size should be positive int\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"batch_size should be positive int\")\n",
    "    \n",
    "    def train(self, times, batch_size = None, step = 1., descent_method = \"normal\", termination = [0,0,0.]):\n",
    "        is_termination = False\n",
    "        try:\n",
    "            termination[2] > 12345 # test whether threshold is an real number (no mater int, float,.etc)\n",
    "            termination[0] = int(termination[0])\n",
    "            termination[1] = int(termination[1])\n",
    "            if (termination[0] < termination[1]) and (termination[0] > 0):\n",
    "                is_termination = True\n",
    "                error_record = []\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for t in range(times):\n",
    "            self.epoch_fit(batch_size, step, descent_method)\n",
    "            if is_termination:\n",
    "                if self.lf == \"ce\":\n",
    "                    error_record.append(10*np.log10(1.000000001 - self.validation_accuracy()))\n",
    "                else:\n",
    "                    error_record.append(10*np.log10(self.validation_error()+0.000000001))\n",
    "                # 0.000000001, bias for prevent error when log(0)\n",
    "                \n",
    "                if t >= (termination[1] - 1):\n",
    "                    short_mean = sum(error_record[(-termination[0]):])/termination[0]\n",
    "                    long_mean = sum(error_record[(-termination[1]):])/termination[1]\n",
    "                    if (long_mean - short_mean) < termination[2]:\n",
    "                        return t+1\n",
    "        \n",
    "        return times\n",
    "            \n",
    "    def accuracy(self, inference, target):\n",
    "        if inference.shape != target.shape:\n",
    "            raise ValueError(\"shape of inference and target non-equal\")\n",
    "            \n",
    "        return (inference.argmax(axis=1) == target.argmax(axis=1)).sum()/inference.shape[0]\n",
    "    \n",
    "    def training_error(self):\n",
    "        return self.evaluate(self.tx, self.ty)\n",
    "    \n",
    "    def training_accuracy(self):\n",
    "        return self.accuracy(self.prediction(self.tx), self.ty)\n",
    "    \n",
    "    def validation_error(self):\n",
    "        return self.evaluate(self.vx, self.vy)\n",
    "    \n",
    "    def validation_accuracy(self):\n",
    "        return self.accuracy(self.prediction(self.vx), self.vy)\n",
    "    \n",
    "    def neuron_refined(self, l, reference_data = None, threshold = 0.01):\n",
    "        # l : the # of layer\n",
    "        # threshold : threshold for information contained of dimension be remaind\n",
    "        if type(l) != int:\n",
    "            raise TypeError(\"l should be the layer no. of hidden layer, an int between 0 to (neural_number - 2)\")\n",
    "        elif (l >= self.ln - 1) or (l < 0):\n",
    "            raise ValueError(\"l should be the layer no. of hidden layer, an int between 0 to (neural_number - 2)\")\n",
    "        \n",
    "        try:\n",
    "            if ((threshold< 1) and (threshold>0)) or (type(threshold) == int):\n",
    "                if (threshold > self.ly[l].nn-1):\n",
    "                    raise ValueError(\"int threshold error : removed #neuron should less than currently #neuron\")\n",
    "                elif (threshold < -self.ly[l].nn) or (threshold==0):\n",
    "                    return None\n",
    "                    # do nothing if remove no #neuron (threshold=0) or want to remain #neuron more than currently\n",
    "            else:\n",
    "                raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "        except:\n",
    "            raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "        \n",
    "        if type(reference_data) == type(None):\n",
    "            self.prediction(self.tx)\n",
    "        else:\n",
    "            self.prediction(reference_data)\n",
    "        \n",
    "        ym = self.ly[l].y.mean(axis=1).reshape((self.ly[l].nn,1)) # y (output of Layer) mean of each neurons\n",
    "        yn = self.ly[l].y - ym # centralized y\n",
    "        ab = np.dot(self.ly[l+1].w.v, ym) # Adjusted bias\n",
    "        ev, em = np.linalg.eigh(np.dot(yn, yn.T)) # eigenvalues and eigenmatrix(with eigenvectors as columns)\n",
    "        ir = ev/ev.sum() # info ratio for each eigenvector\n",
    "        # op, pv :column operator result and pivots\n",
    "        if (threshold< 1) and (threshold>0):\n",
    "            op, pv = column_operate(em[:,ir > threshold])\n",
    "        else:\n",
    "            op, pv = column_operate(em[:,ir >= ir[ir.argsort()[threshold]]])\n",
    "            \n",
    "        nw = np.dot(self.ly[l+1].w.v, op) # new weight\n",
    "        self.ly[l+1].b.assign_values(self.ly[l+1].b.v + (np.dot(self.ly[l+1].w.v, ym) -np.dot(nw, ym[pv])))\n",
    "        self.ly[l+1].w.assign_values(nw) # l+1 weight should be rewrite after l+1 bias have been rewrite\n",
    "        self.ly[l].w.assign_values(self.ly[l].w.v[pv])\n",
    "        self.ly[l].b.assign_values(self.ly[l].b.v[pv])\n",
    "        self.ly[l].nn = len(pv)\n",
    "    \n",
    "    def neuron_proliferate(self, proliferating_layer, proliferating_n = 1, output_weight_bound = 1.):\n",
    "        if proliferating_layer not in range(self.ln):\n",
    "            raise ValueError(\"proliferating_layer should be an int from 0 to (#layer-1)\")\n",
    "            \n",
    "        if type(proliferating_n) != int:\n",
    "            raise ValueError(\"proliferating_n should be int\")\n",
    "        \n",
    "        if proliferating_n <= 0:\n",
    "            raise ValueError(\"proliferating_n should be postive\")\n",
    "            \n",
    "        if output_weight_bound < 0.:\n",
    "            raise ValueError(\"output_weight_bound should be non-negative\")\n",
    "            \n",
    "        l = proliferating_layer\n",
    "        for t in range(proliferating_n):\n",
    "            self.ly[l].w.add_row(mcmc_normal(self.ly[l].w.v, mean=self.ly[l].w.v.mean(), std=self.ly[l].w.v.std()))\n",
    "            self.ly[l].b.add_row(mcmc_normal(self.ly[l].b.v, mean=self.ly[l].b.v.mean(), std=self.ly[l].b.v.std()))\n",
    "            self.ly[l+1].w.add_column(output_weight_bound*(2*np.random.rand((self.ly[l+1].nn))-1.))\n",
    "            self.ly[l].nn += 1\n",
    "    \n",
    "    def reset_cs(self, new_cs):\n",
    "        for l in range(self.ln):\n",
    "            self.ly[l].reset_cs(new_cs)\n",
    "    \n",
    "    def inter_layer_linear_regression(self, layer_interval):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                if ls == 0:\n",
    "                    ri = np.array(self.px.T) # regression input\n",
    "                else:\n",
    "                    ri = np.array(self.ly[ls-1].y)\n",
    "                \n",
    "                ri = np.append(ri, np.ones((1, ri.shape[1])), axis=0) # append 1. for each datum as bias\n",
    "                ro = np.array(self.ly[le].x)\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        rr = np.linalg.lstsq(ri.T, ro.T) # regression result (matrix, residuals, rank of ri, singuler values of ri)\n",
    "        if len(rr[1]) == 0:\n",
    "            raise ValueError(\"output data of layer\" + str(ls-1) + \"(= -1, for input data) should be full rank, try self.nruron_refine first\")\n",
    "        \n",
    "        return rr[0], rr[1]/ri.shape[1]\n",
    "    \n",
    "    def find_linearist_layers(self, reference_data = None):\n",
    "        output = (0, 0, np.inf, np.array([[]]), np.zeros((0,0)))\n",
    "        if type(reference_data) == type(None):\n",
    "            self.prediction(self.tx)\n",
    "        else:\n",
    "            self.prediction(reference_data)\n",
    "        \n",
    "        for l1 in range(self.ln-1):\n",
    "            for l2 in range(i+1, self.ln):\n",
    "                rr = self.inter_layer_linear_regression((l1,l2))\n",
    "                if np.sqrt(rr[1].sum()) < output[2]:\n",
    "                    output = (l1, l2, np.sqrt(rr[1].sum()), rr[0])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def layer_filled(self, layer_interval, weights, bias):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        if weights.shape[0] != bias.shape[0]:\n",
    "            raise ValueError(\"weights.shape[0] doesn't match bias.shape[0]\")\n",
    "        \n",
    "        if weights.shape[0] != self.ly[le].nn:\n",
    "            raise ValueError(\"weights.shape[0] doesn't match #neuron of layer at end of layer_interval\")\n",
    "        \n",
    "        self.ly[le].w.assign_values(weights)\n",
    "        self.ly[le].b.assign_values(bias)\n",
    "        self.ly = self.ly[:ls] + self.ly[le:]\n",
    "        self.ln = len(self.ly)\n",
    "    \n",
    "    def linear_filled(self, layer_interval):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "            \n",
    "        rr = self.inter_layer_linear_regression(layer_interval)\n",
    "        self.layer_filled(layer_interval, rr[0].T[:,:-1], rr[0].T[:,-1:])\n",
    "    \n",
    "    def insert_layer(self, position, weights, bias, activation_function, next_layer_weights, next_layer_bias):\n",
    "        if type(position) == int:\n",
    "            if position in range(self.ln):\n",
    "                pass\n",
    "        else:\n",
    "            raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        \n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        ilo, ili = weights.shape # input and output size of inserted layer\n",
    "        nlo, nli = next_layer_weights.shape # input and output size of next layer\n",
    "        \n",
    "        if position == 0:\n",
    "            if ili != self.xs:\n",
    "                raise ValueError(\"weights.shape error, cheak input and output size for this new layer\")\n",
    "        else:\n",
    "            if ili != self.ly[position-1].nn:\n",
    "                raise ValueError(\"weights.shape error, cheak input and output size for this new layer\")\n",
    "        \n",
    "        if (ilo != bias.shape[0]) or (ilo != nli):\n",
    "            raise ValueError(\"to define #neuron of new layer, all related weighs and bias size should be consistent\")\n",
    "        \n",
    "        if nlo != self.ly[position].nn:\n",
    "            raise ValueError(\"next_layer_weights.shape error, cheak #neuron of next layer\")\n",
    "        \n",
    "        if next_layer_bias.shape[0] != self.ly[position].nn:\n",
    "            raise ValueError(\"next_layer_bias.shape error, cheak #neuron of next layer\")\n",
    "        \n",
    "        if (bias.shape[1] != 1) or (next_layer_bias.shape[1] != 1):\n",
    "            raise ValueError(\"bias shape should be (#neuron, 1)\")\n",
    "        \n",
    "        l = position\n",
    "        \n",
    "        self.ly.insert(l, Layer(ilo, activation_function))\n",
    "        self.ly[l].w.assign_values(weights)\n",
    "        self.ly[l].b.assign_values(bias)\n",
    "        self.ly[l+1].w.assign_values(next_layer_weights)\n",
    "        self.ly[l+1].b.assign_values(next_layer_bias)\n",
    "        \n",
    "        self.ln = len(self.ly)\n",
    "    \n",
    "    def identity_dig(self, position, activation_function):\n",
    "        if type(position) == int:\n",
    "            if position in range(self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        else:\n",
    "            raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        \n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        l = position\n",
    "        # ids : size of identity transform, input size of new layer\n",
    "        if l == 0:\n",
    "            ids = self.xs\n",
    "        else:\n",
    "            ids = self.ly[l-1].nn\n",
    "        \n",
    "        if type(activation_function) in [Relu, SoftPlus]:\n",
    "            liw = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 0)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 1)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) == LeakyRelu:\n",
    "            liw = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 0)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 1) / (1.+activation_function.alpha)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) == Identity:\n",
    "            liw = np.identity(ids)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.identity(ids)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) in [Sigmoid, Hypertan, Selu]:\n",
    "            # li : input of new layer\n",
    "            if l == 0:\n",
    "                li = np.array(self.tx.T)\n",
    "            else:\n",
    "                li = np.array(self.ly[l-1].y)\n",
    "            \n",
    "            lim = li.mean(axis=1)\n",
    "            lis = li.std(axis=1) + 1.\n",
    "            \n",
    "            liw = np.diag(1./lis)\n",
    "            if type(activation_function) == Selu:\n",
    "                lib = 1.-(lim/lis).reshape(-1,1) # let mean become one before transform by activation function\n",
    "            else:\n",
    "                lib = -(lim/lis).reshape(-1,1) # let mean become zero before transform by activation function\n",
    "            \n",
    "            lo = activation_function.trans(np.dot(liw, li)+lib)\n",
    "            lo = np.append(lo, np.ones((1, lo.shape[1])), axis=0) # append 1. for each datum as bias\n",
    "            rr = np.linalg.lstsq(lo.T, li.T) # regression result (matrix, residuals, rank of ri, singuler values of ri)\n",
    "            # since the goal is construct identity, try to find linear transform form layer output to layer input\n",
    "            low = rr[0].T[:,:-1]\n",
    "            lob = rr[0].T[:,-1:]\n",
    "        else:\n",
    "            raise TypeError(\"activation_function type error\")\n",
    "        \n",
    "        nlw = np.dot(self.ly[l].w.v, low)\n",
    "        nlb = np.dot(self.ly[l].w.v, lob) + self.ly[l].b.v\n",
    "        \n",
    "        self.insert_layer(l,\n",
    "                          liw,\n",
    "                          lib,\n",
    "                          activation_function,\n",
    "                          nlw,\n",
    "                          nlb\n",
    "                         )\n",
    "    \n",
    "    def dimension(self):\n",
    "        output = 0\n",
    "        for l in range(self.ln):\n",
    "            output += self.ly[l].dimension()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def save_weight(self, dir_name):\n",
    "        for l in range(self.ln):\n",
    "            np.save(dir_name + \"/w%i.npy\" % l, self.ly[l].w.v)\n",
    "            np.save(dir_name + \"/b%i.npy\" % l, self.ly[l].b.v)\n",
    "    \n",
    "    def load_weight(self, dir_name):\n",
    "        for l in range(self.ln):\n",
    "            try:\n",
    "                if l == 0:\n",
    "                    if np.load(dir_name + \"/w%i.npy\" % l).shape[1] != self.xs:\n",
    "                        raise ValueError(\"layer %i input size error, cheak weight size.\" % l)\n",
    "                else:\n",
    "                    if np.load(dir_name + \"/w%i.npy\" % l).shape[1] != self.ly[l-1].nn:\n",
    "                        raise ValueError(\"layer %i input size error, cheak weight size.\" % l)\n",
    "\n",
    "                if np.load(dir_name + \"/w%i.npy\" % l).shape[0] != self.ly[l].nn:\n",
    "                    raise ValueError(\"layer %i neuron size error, cheak weight size.\" % l)\n",
    "\n",
    "                if np.load(dir_name + \"/b%i.npy\" % l).shape[0] != self.ly[l].nn:\n",
    "                    raise ValueError(\"layer %i neuron size error, cheak bias size.\" % l)\n",
    "\n",
    "                if np.load(dir_name + \"/b%i.npy\" % l).shape[1] != 1:\n",
    "                    raise ValueError(\"layer %i bias size error, should be 1.\" % l)\n",
    "            \n",
    "            except:\n",
    "                raise ValueError(\"load .npy error, cheak dir.\")\n",
    "            \n",
    "            self.ly[l].w.assign_values(np.load(dir_name + \"/w%i.npy\" % l))\n",
    "            self.ly[l].b.assign_values(np.load(dir_name + \"/b%i.npy\" % l))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45, -0.35, -0.25, -0.15, -0.05, -0.  ,  0.05,  0.15,  0.25,\n",
       "        0.35,  0.45])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
