{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsNone(target):\n",
    "    return (type(target) == type(None))\n",
    "\n",
    "def ArraySign(input_array):\n",
    "    # return +1, 0, -1 respect to positive, zero, negtive\n",
    "    return 1.*(input_array>0) - 1.*(input_array<0)\n",
    "\n",
    "def CutValue(input_array, cut_value):\n",
    "    output = np.abs(input_array)\n",
    "    output = ArraySign(input_array) * (output * (output < cut_value) + cut_value * (output >= cut_value))\n",
    "    return output\n",
    "\n",
    "def WeightedSum(input_array, weight):\n",
    "    try:\n",
    "        return (input_array.reshape(-1)*weight).sum()\n",
    "    except:\n",
    "        raise ValueError(\"weight should be an 1-d array with the same size with input_array\")\n",
    "\n",
    "def WeightedRow(input_array, weight):\n",
    "    try:\n",
    "        return input_array*weight.reshape(-1,1)\n",
    "    except:\n",
    "        raise ValueError(\"weight should be an 1-d array with the same length with first shape of input_array\")\n",
    "\n",
    "def OverPenalty(input_value, rate = 0.1, threshold=0.):\n",
    "    output = np.abs(input_value) - threshold\n",
    "    output *= (output > 0)\n",
    "    output *= rate * ArraySign(input_value)\n",
    "    return output\n",
    "\n",
    "def RowOperate(matrix, threshold = 0.000000000000001, large_element_alert=False):\n",
    "    # matrix : matrix with rows > columns\n",
    "    reduced_matrix = np.array(matrix)\n",
    "    filtered_matrix = np.array(matrix)\n",
    "    shape = matrix.shape # matrix size\n",
    "    mask = np.ones(shape)\n",
    "    pivots = -1*np.ones((min(shape)), dtype = np.int) # store pivots, (# of pivots) <= min(rows, columns)\n",
    "    for p in range(len(pivots)):\n",
    "        filtered_matrix = reduced_matrix * mask # filter\n",
    "        if np.abs(filtered_matrix).max() < threshold:\n",
    "            print(\"WARNING : input rows not independ for threshold %d when apply row operation\" %(threshold))\n",
    "            break\n",
    "        \n",
    "        pivot_row, pivot_col = np.unravel_index(np.abs(filtered_matrix).argmax(), shape) # pivot row, pivot column\n",
    "        reduced_matrix[pivot_row] /= reduced_matrix[pivot_row][pivot_col]\n",
    "        multi = np.array(reduced_matrix[:, pivot_col])\n",
    "        multi[pivot_row] = 0.\n",
    "        reduced_matrix -= np.dot(multi.reshape((-1, 1)), reduced_matrix[pivot_row].reshape((1, -1)))\n",
    "        mask[pivot_row] = 0.\n",
    "        mask[:, pivot_col] = 0.\n",
    "        pivots[pivot_row] = pivot_col # the column-index of pivot_row-th row is pivot_col\n",
    "    \n",
    "    reduced_matrix = reduced_matrix[pivots != -1,:]\n",
    "    pivots = pivots[pivots != -1]\n",
    "    if large_element_alert:\n",
    "        if np.abs(reduced_matrix).max() > 1.01:\n",
    "            print(\"WARNING : reduced matrix has large element %f\" %(np.abs(reduced_matrix).max()))\n",
    "        \n",
    "    \n",
    "    return reduced_matrix, pivots\n",
    "\n",
    "def LinearRefine(regressor, response, num_elimination=1, regularizer=0.):\n",
    "    if (regressor.ndim != 2) or (regressor.size == 0):\n",
    "        raise ValueError(\"regressor should be a non-empty numpy matrix\")\n",
    "    elif (response.ndim != 2) or (response.size == 0):\n",
    "        raise ValueError(\"response should be a non-empty numpy matrix\")\n",
    "    elif len(regressor) != len(response):\n",
    "        raise ValueError(\"len(regressor) != len(response)\")\n",
    "    \n",
    "    if regularizer < 0.:\n",
    "        regularizer = 0.\n",
    "        print(\"SetRegularizer error, regularizer must be non-negative, has been set to zero.\")\n",
    "    \n",
    "    num_var = regressor.shape[1]\n",
    "    regressor = np.append(regressor, np.ones((len(regressor), 1)), axis=1) # add bias\n",
    "    gram = np.dot(regressor.T, regressor)\n",
    "    gram += regularizer * regressor.shape[0] * np.identity(regressor.shape[1])\n",
    "    projected = np.dot(regressor.T, response)\n",
    "    is_leave = np.ones((gram.shape[0]), dtype=bool)\n",
    "    response_square = np.square(response).sum(axis=0)\n",
    "    for d in range(num_elimination):\n",
    "        square_err = np.inf * np.ones((gram.shape[0]), dtype=bool)\n",
    "        for i in range(num_var):\n",
    "            if is_leave[i]:\n",
    "                work_index = (is_leave * (np.arange(len(is_leave)) != i)).astype(bool)\n",
    "                coe = np.linalg.solve(gram[work_index][:, work_index], projected[work_index])\n",
    "                square_err[i] = (response_square\n",
    "                                 - (coe * projected[work_index]).sum(axis=0) \n",
    "                                 - regularizer * np.square(coe).sum(axis=0)\n",
    "                                ).sum()\n",
    "        \n",
    "        is_leave[np.argmin(square_err)] = False\n",
    "    \n",
    "    rms = square_err.min()/(regressor.shape[0] - is_leave.sum())\n",
    "    coe = np.linalg.solve(gram[is_leave][:, is_leave], projected[is_leave])\n",
    "    bias = coe[-1].reshape(1, -1)\n",
    "    return is_leave[:-1], coe[:-1], bias, rms\n",
    "    \n",
    "    \"\"\"\n",
    "    if inputs.shape[0] != outputs.shape[0]:\n",
    "        raise ValueError(\"inputs size doesn't match with this outputs\")\n",
    "    \n",
    "    if inputs.shape[1] <= num_elimination:\n",
    "        raise ValueError(\"inputs variable equal or less than num_elimination\")\n",
    "    \n",
    "    remain_cols = np.ones(inputs.shape[1], dtype=bool)\n",
    "    for t in range(num_elimination):\n",
    "        loss = np.ones(inputs.shape[1])*np.inf\n",
    "        loss[remain_cols] = 0.\n",
    "        for c in range(inputs.shape[1]):\n",
    "            if remain_cols[c]:\n",
    "                regression_result = np.linalg.lstsq(np.c_[inputs[:, remain_cols*(np.arange(inputs.shape[1])!=c)],\n",
    "                                                          np.ones(len(inputs))\n",
    "                                                         ],\n",
    "                                                    outputs,\n",
    "                                                    rcond=rcond\n",
    "                                                   )\n",
    "                if len(regression_result[1]) > 0:\n",
    "                    loss[c] = regression_result[1].sum()/len(inputs)\n",
    "                else:\n",
    "                    inference = np.dot(np.c_[inputs[:, remain_cols*(np.arange(inputs.shape[1])!=c)],\n",
    "                                             np.ones(len(inputs))\n",
    "                                            ],\n",
    "                                       regression_result[0]\n",
    "                                      )\n",
    "                    loss[c] = np.square(inference - outputs).sum()/len(inputs)\n",
    "                    \n",
    "                loss[c] += regularizer * (regression_result[0]**2).max()\n",
    "            \n",
    "        \n",
    "        remain_cols[loss.argmin()] = False\n",
    "    \n",
    "    regression_result = np.linalg.lstsq(np.c_[inputs[:, remain_cols], np.ones(len(inputs))],\n",
    "                                        outputs,\n",
    "                                        rcond=rcond\n",
    "                                       )\n",
    "    \n",
    "    linear_coe = regression_result[0][:-1]\n",
    "    bias = regression_result[0][-1:]\n",
    "    mses = regression_result[1]/len(inputs)\n",
    "    \n",
    "    return remain_cols, linear_coe, bias, mses\n",
    "    \"\"\"\n",
    "\n",
    "def McmcNormal(points, drop_times = 10, mean=0., std=1.):\n",
    "    # Useing Markov chain Monte Carlo method to get a new point from normal distribution with given points\n",
    "    # each element is get from mean and std\n",
    "    output = np.random.normal(mean, std, points.shape[1:])\n",
    "    if drop_times>1:\n",
    "        for t in range(1, drop_times):\n",
    "            candicate = np.random.normal(mean, std, points.shape[1:])\n",
    "            candicate_distance = np.sqrt(np.square(np.subtract(points, candicate)).sum(axis=tuple(np.arange(1, len(points.shape))))).min()\n",
    "            # distance of candicate to target\n",
    "            output_distance = np.sqrt(np.square(np.subtract(points, output)).sum(axis=tuple(np.arange(1, len(points.shape))))).min()\n",
    "            # distance of currently output to target\n",
    "            if np.random.rand()*output_distance < candicate_distance:\n",
    "                output = np.array(candicate)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def McmcColExtend(input_matrix, drop_times, extend_min=0.):\n",
    "    # return a matrix with a new col with same level by McmcNormal\n",
    "    input_matrix = input_matrix.T # transpose cols to rows\n",
    "    output = np.insert(input_matrix,\n",
    "                       len(input_matrix),\n",
    "                       McmcNormal(input_matrix, drop_times, 0, max(extend_min, np.sqrt(np.square(input_matrix).mean()))),\n",
    "                       axis=0\n",
    "                      ).T # transpose rows back to cols\n",
    "\n",
    "    return output\n",
    "\n",
    "# Data\n",
    "\n",
    "class Data():\n",
    "    def __init__(self, inputs=np.zeros((0,0)), labels=np.zeros((0,0)), weight = None):\n",
    "        self.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetData(self, inputs=np.zeros((0,0)), labels=np.zeros((0,0)), weight = None):\n",
    "        if len(inputs) != len(labels):\n",
    "            raise ValueError(\"num_datums error, #inputs != #labels.\")\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        if IsNone(weight):\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "        elif weight.shape != (len(inputs)):\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "            print(\"WARNING : weight shape error, set uniform weight.\")\n",
    "        elif weight.sum() <= 0:\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "            print(\"WARNING : get non-positive weight sum, set uniform weight.\")\n",
    "        else:\n",
    "            self.weight = weight\n",
    "        \n",
    "        self.weight /= self.weight.sum()\n",
    "    \n",
    "    def GetNumDatums(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def GetDatumSize(self):\n",
    "        # return size of input and label\n",
    "        return (self.inputs.shape[1], self.labels.shape[1])\n",
    "    \n",
    "    def Shuffle(self):\n",
    "        # shuffling datumds\n",
    "        new_index = np.arange(self.GetNumDatums())\n",
    "        np.random.shuffle(new_index)\n",
    "        self.inputs = self.inputs[new_index]\n",
    "        self.labels = self.labels[new_index]\n",
    "    \n",
    "    def IsClassification(self):\n",
    "        # cheaking if labels of this data is classification\n",
    "        # By cheaking:\n",
    "        # 1. labels have only two value : 0, 1\n",
    "        # 2. two or more classes\n",
    "        # 3. each datum has unique 1\n",
    "        output = ((self.labels == 0) + (self.labels == 1)).all()\n",
    "        output *= (self.labels.shape[1]>1)\n",
    "        output *= (self.labels.sum(axis=1)==1).all()\n",
    "        return output\n",
    "\n",
    "# Data end\n",
    "\n",
    "# VariableArray\n",
    "\n",
    "class VariableArray():\n",
    "    def __init__(self, size, cwise_step_initial=0.1):\n",
    "        self.value = np.random.normal(0., 1., size) # array value\n",
    "        self.total_deri = np.zeros(self.value.shape) # total derivative, used to descent\n",
    "        self.last_total_deri = np.zeros(self.value.shape) # last total derivative\n",
    "        self.moving = np.zeros(self.value.shape) # moving array\n",
    "        self.cwise_step = cwise_step_initial*np.ones(self.value.shape) # component-wise step\n",
    "        \n",
    "        self.regulariz_rate = 0.\n",
    "        self.regulariz_margin = 0.\n",
    "    \n",
    "    def SetValue(self, input_value, cwise_step_initial=0.1):\n",
    "        self.value = np.array(input_value) # array value\n",
    "        self.total_deri = np.zeros(self.value.shape) # total derivative, used to descent\n",
    "        self.last_total_deri = np.zeros(self.value.shape) # last total derivative\n",
    "        self.moving = np.zeros(self.value.shape) # moving array\n",
    "        self.cwise_step = cwise_step_initial*np.ones(self.value.shape) # component-wise step\n",
    "    \n",
    "    def SetDeri(self, input_value):\n",
    "        if input_value.shape != self.total_deri.shape:\n",
    "            raise ValueError(\"input_value shape error\")\n",
    "        \n",
    "        self.total_deri = np.array(input_value)\n",
    "    \n",
    "    def DeriModify(self, input_value):\n",
    "        if input_value.shape != self.total_deri.shape:\n",
    "            raise ValueError(\"input_value shape error\")\n",
    "        \n",
    "        self.total_deri += input_value\n",
    "    \n",
    "    def ZeroDeri(self):\n",
    "        self.total_deri *= 0\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        self.regulariz_rate = max(rate, 0.)\n",
    "        self.regulariz_margin = max(margin, 0.)\n",
    "    \n",
    "    def ResetCwiseStep(self, new_cwise_step):\n",
    "        self.cwise_step = new_cwise_step * np.ones(self.cwise_step.shape)\n",
    "    \n",
    "    def Regularize(self):\n",
    "        if self.regulariz_rate != 0:\n",
    "            self.total_deri += OverPenalty(self.value, self.regulariz_rate, self.regulariz_margin)\n",
    "    \n",
    "    def Descent(self, step=1., method=\"normal\", move_max=1.):\n",
    "        self.Regularize()\n",
    "        if method == \"normal\":\n",
    "            self.moving = self.total_deri * step\n",
    "            self.moving = -1*CutValue(self.moving, move_max)\n",
    "        elif method == \"Rprop\":\n",
    "            self.moving = ArraySign(self.total_deri)\n",
    "            self.movint_return = ArraySign(self.total_deri*self.last_total_deri)\n",
    "            self.cwise_step *= 1.2*(self.movint_return>0) + 1.*(self.movint_return==0) + 0.5*(self.movint_return<0)\n",
    "            self.cwise_step = CutValue(self.cwise_step, move_max)\n",
    "            self.moving *= -1*self.cwise_step\n",
    "        else:\n",
    "            raise ValueError(\"descent method error\")\n",
    "        \n",
    "        self.value += self.moving\n",
    "        \n",
    "        self.last_total_deri = np.array(self.total_deri)\n",
    "        self.ZeroDeri()\n",
    "\n",
    "# VariableArray end\n",
    "\n",
    "# Activation functions defined start\n",
    "\n",
    "class Identity():\n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return np.ones(flow_in.shape, dtype = np.float64)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Sigmoid():\n",
    "    def Forward(self, flow_in):\n",
    "        return expit(flow_in)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return expit(flow_in)*expit(-flow_in)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Hypertan():\n",
    "    def Forward(self, flow_in):\n",
    "        flow_in = CutValue(flow_in, 100)\n",
    "        return np.tanh(flow_in)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        flow_in = CutValue(flow_in, 100) # cut value out of [-100, 100] to 100, cosh(-100) = cosh(100)\n",
    "        return 1. / np.square(np.cosh(flow_in))\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class SoftSign():\n",
    "    def Forward(self, flow_in):\n",
    "        return ArraySign(flow_in)*(1. - 1./(np.abs(flow_in) + 1.))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1. / np.square(np.abs(flow_in) + 1.)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Relu():\n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in*(flow_in>0)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1.*(flow_in>0)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class LeakyRelu():\n",
    "    def __init__(self, alpha = 0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in*(flow_in>0) + self.alpha*flow_in*(flow_in<0)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1.*(flow_in>0) + self.alpha*(flow_in<0)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class SoftPlus():\n",
    "    def Forward(self, flow_in):\n",
    "        return np.log(1. + np.exp(flow_in))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return expit(flow_in)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Selu():\n",
    "    def __init__(self):\n",
    "        self.ahpha = 1.05071\n",
    "        self.beta = 1.67326\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        return self.ahpha*(flow_in*(flow_in>=0) + self.beta*(np.exp(flow_in) - 1)*(flow_in<0))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return self.ahpha*(1.*(flow_in>=0) + self.beta*np.exp(flow_in)*(flow_in<0))\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Softmax():\n",
    "    def Forward(self, flow_in):\n",
    "        output = flow_in - flow_in.max(axis=1).reshape(-1,1)\n",
    "        output = np.exp(output)\n",
    "        output /= output.sum(axis=1).reshape(-1,1)\n",
    "        return output\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        flow_out = self.Forward(flow_in) # result of self.trans\n",
    "        return flow_out*back_flow - flow_out*((flow_out*back_flow).sum(axis=1).reshape(-1,1))\n",
    "\n",
    "# Activation functions defined end\n",
    "\n",
    "# Layer\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, num_unit, activationFunction):\n",
    "        if type(activationFunction) == type:\n",
    "            raise TypeError(\"activationFunction should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        self.num_unit = num_unit\n",
    "        self.activationFunction = activationFunction\n",
    "        self.linear = VariableArray((0, self.num_unit)) # linear weights working before active function\n",
    "        self.bias = VariableArray((1, self.num_unit)) # bias working before active function\n",
    "        self.flow_in = np.zeros((0, self.num_unit))\n",
    "        self.flow_out = np.zeros((0, self.num_unit))\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        self.flow_in = np.dot(flow_in, self.linear.value) + self.bias.value\n",
    "        self.flow_out = self.activationFunction.Forward(self.flow_in)\n",
    "    \n",
    "    def Backward(self, back_flow, layer_source):\n",
    "        deri = self.activationFunction.Backward(self.flow_in, back_flow)\n",
    "        self.linear.DeriModify(np.dot(layer_source.T, deri))\n",
    "        self.bias.DeriModify(np.sum(deri, axis=0).reshape(1, -1))\n",
    "        deri = np.dot(deri, self.linear.value.T)\n",
    "        return deri\n",
    "    \n",
    "    def ZeroDeri(self):\n",
    "        self.linear.ZeroDeri()\n",
    "        self.bias.ZeroDeri()\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        self.linear.SetRegularizer(rate, margin)\n",
    "        self.bias.SetRegularizer(rate, margin)\n",
    "    \n",
    "    def Descent(self, step, method):\n",
    "        self.linear.Descent(step, method)\n",
    "        self.bias.Descent(step, method)\n",
    "    \n",
    "    def ResetCwiseStep(self, new_cwise_step):\n",
    "        self.linear.ResetCwiseStep(new_cwise_step)\n",
    "        self.bias.ResetCwiseStep(new_cwise_step)\n",
    "    \n",
    "    def GetPCA(self, weight):\n",
    "        if IsNone(weight):\n",
    "            weight = np.ones((self.flow_out.shape[0])) / self.flow_out.shape[0]\n",
    "        \n",
    "        mean_flow_out = self.flow_out.mean(axis=0)\n",
    "        centered_flow_out = self.flow_out - mean_flow_out\n",
    "        cov = np.dot(centered_flow_out.T * weight, centered_flow_out) # covariance matrix\n",
    "        information, eigen_vectors = np.linalg.eigh(cov) #information is sorted from small to large\n",
    "        principal_components = eigen_vectors.T # transpose eigen vector from column to row\n",
    "        return information, principal_components, mean_flow_out, centered_flow_out\n",
    "    \n",
    "    def GetPCVar(self, weight):\n",
    "        if IsNone(weight):\n",
    "            weight = np.ones((self.flow_out.shape[0])) / self.flow_out.shape[0]\n",
    "        \n",
    "        mean_flow_out = self.flow_out.mean(axis=0)\n",
    "        centered_flow_out = self.flow_out - mean_flow_out\n",
    "        cov = np.dot(centered_flow_out.T * weight, centered_flow_out) # covariance matrix\n",
    "        information= np.linalg.eigvalsh(cov) #information is sorted from small to large\n",
    "        return information\n",
    "    \n",
    "    def BackwardElimination(self, num_elimination, rcond = -1):\n",
    "        remain_cols, linear, bias, mse = LinearRefine(self.flow_out,\n",
    "                                                      self.flow_out,\n",
    "                                                      num_elimination=num_elimination,\n",
    "                                                      rcond=rcond\n",
    "                                                     )\n",
    "        \n",
    "        self.num_unit -= num_elimination\n",
    "        self.linear.SetValue(self.linear.value[:, remain_cols])\n",
    "        self.bias.SetValue(self.linear.bias[:, remain_cols])\n",
    "    \n",
    "    def GetDimension(self):\n",
    "        return self.linear.value.size + self.bias.value.size\n",
    "\n",
    "# Layer end\n",
    "\n",
    "# Loss function\n",
    "\n",
    "class LossFunction():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "    \n",
    "    def SetMethod(self, method):\n",
    "        self.method = method\n",
    "    \n",
    "    def GetLoss(self, inference, target, weight):\n",
    "        if self.method == \"r2\":\n",
    "            output = WeightedSum(np.square(inference - target).sum(axis=1), weight)\n",
    "            output /= WeightedSum(np.square(target - target.mean(axis=0)).sum(axis=1), weight)\n",
    "        elif self.method == \"cross entropy\":\n",
    "            output = WeightedSum((-target*np.log(inference)).sum(axis=1), weight)\n",
    "        else:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def Backward(self, inference, target, weight):\n",
    "        if self.method == \"r2\":\n",
    "            output = WeightedRow(2.*(inference - target), weight)\n",
    "            output /= WeightedSum(np.square(target - target.mean(axis=0)).sum(axis=1), weight)\n",
    "        elif self.method == \"cross entropy\":\n",
    "            output = WeightedRow(-(target/inference), weight)\n",
    "        else:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "class Terminator():\n",
    "    def __init__(self, short_period = 5, long_period = 10, threshold = 0.):\n",
    "        try:\n",
    "            short_period = int(short_period)\n",
    "            long_period = int(long_period)\n",
    "        except:\n",
    "            raise ValueError(\"period should be a value, and will be transfer to int\")\n",
    "        \n",
    "        if short_period <= 0:\n",
    "            short_period = 1\n",
    "            print(\"WARNING : short_period <= 0, set 1\")\n",
    "        \n",
    "        if long_period <= short_period:\n",
    "            long_period = short_period + 1\n",
    "            print(\"WARNING : long_period <= short_period, set short_period + 1\")\n",
    "        \n",
    "        self.short_period = short_period\n",
    "        self.long_period = long_period\n",
    "        self.threshold = threshold\n",
    "        self.record = []\n",
    "    \n",
    "    def Reset(self, short_period, long_period, threshold = 0.):\n",
    "        try:\n",
    "            short_period = int(short_period)\n",
    "            long_period = int(long_period)\n",
    "        except:\n",
    "            raise ValueError(\"period should be a value, and will be transfer to int\")\n",
    "        \n",
    "        if short_period <= 0:\n",
    "            short_period = 1\n",
    "            print(\"WARNING : short_period <= 0, set 1\")\n",
    "        \n",
    "        if long_period <= short_period:\n",
    "            long_period = short_period + 1\n",
    "            print(\"WARNING : long_period <= short_period, set %d\" %(short_period + 1))\n",
    "        \n",
    "        self.short_period = short_period\n",
    "        self.long_period = long_period\n",
    "        self.threshold = threshold\n",
    "        self.record = []\n",
    "    \n",
    "    def Hit(self, input_value):\n",
    "        try:\n",
    "            input_value = float(input_value)\n",
    "        except:\n",
    "            raise ValueError(\"input_value should be a real value\")\n",
    "        \n",
    "        self.record = [input_value] + self.record[:self.long_period-1]\n",
    "        if len(self.record) == self.long_period:\n",
    "            return (np.mean(self.record[:self.short_period]) - self.threshold > np.mean(self.record))\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def Clear(self):\n",
    "        self.record = []\n",
    "\n",
    "class DogikoNeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.hiddenLayerList = []\n",
    "        self.outputFunction = None\n",
    "        self.outputLayer = None\n",
    "        self.lossFunction = None\n",
    "        self.trainData = Data()\n",
    "        self.validData = Data()\n",
    "        self.testData = Data()\n",
    "        self.regulariz_rate = 0.\n",
    "        self.regulariz_margin = 0.\n",
    "        self.has_build = False\n",
    "        self.hit_tolerance = 0.1\n",
    "        self.terminator = Terminator()\n",
    "    \n",
    "    def SetLossFunction(self, method):\n",
    "        if method not in [\"r2\", \"cross entropy\"]:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        self.lossFunction = LossFunction(method)\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        if rate < 0.:\n",
    "            print(\"WARNING : regulariz_rate error, get negative value, setting to 0.\")\n",
    "        \n",
    "        if margin < 0.:\n",
    "            print(\"WARNING : regulariz_margin error, get negative value, setting to 0.\")\n",
    "        \n",
    "        self.regulariz_rate = max(rate, 0.)\n",
    "        self.regulariz_margin = max(margin, 0.)\n",
    "        \n",
    "        if self.has_build:\n",
    "            for l in range(self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "            \n",
    "            self.outputLayer.SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "    \n",
    "    def SetTrainData(self, inputs, labels, weight = None):\n",
    "        if self.has_build:\n",
    "            if inputs.shape[1] != self.inputs_size:\n",
    "                raise ValueError(\"inputs size doesn't match with this model\")\n",
    "            \n",
    "            if labels.shape[1] != self.labels_size:\n",
    "                raise ValueError(\"labels size doesn't match with this model\")\n",
    "        \n",
    "        self.trainData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetValidData(self, inputs, labels, weight = None):\n",
    "        if self.has_build:\n",
    "            if inputs.shape[1] != self.inputs_size:\n",
    "                raise ValueError(\"inputs size doesn't match with this model\")\n",
    "            \n",
    "            if labels.shape[1] != self.labels_size:\n",
    "                raise ValueError(\"labels size doesn't match with this model\")\n",
    "        \n",
    "        self.validData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetTestData(self, inputs, labels, weight = None):\n",
    "        if self.has_build:\n",
    "            if inputs.shape[1] != self.inputs_size:\n",
    "                raise ValueError(\"inputs size doesn't match with this model\")\n",
    "            \n",
    "            if labels.shape[1] != self.labels_size:\n",
    "                raise ValueError(\"labels size doesn't match with this model\")\n",
    "        \n",
    "        self.testData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def AddHiddenLayer(self, num_unit, activationFunction):\n",
    "        self.hiddenLayerList.append(Layer(num_unit, activationFunction))\n",
    "        if self.has_build:\n",
    "            print(\"WARNING : a hidden layer added after build, please re-build model or set related value manually.\")\n",
    "    \n",
    "    def SetOutputFunction(self, activationFunction):\n",
    "        # units of output layer is fixed as same as label size\n",
    "        self.outputFunction = activationFunction\n",
    "        if self.has_build:\n",
    "            self.outputLayer.activationFunction = self.outputFunction\n",
    "    \n",
    "    def SetHitTolerance(self, tolerance):\n",
    "        try:\n",
    "            if float(tolerance) <= 0.:\n",
    "                print(\"setting tolerance failed, tolerance should be positive real value\")\n",
    "            \n",
    "            self.hit_tolerance = float(tolerance)\n",
    "        except:\n",
    "            print(\"setting tolerance failed, tolerance should be positive real value\")\n",
    "    \n",
    "    def SetTerminator(self, short_period, long_period, threshold = 0.):\n",
    "        self.terminator.Reset(short_period, long_period, threshold)\n",
    "        \n",
    "    def ClearTerminator(self):\n",
    "        self.terminator.Clear()\n",
    "    \n",
    "    def GetNumHiddenLayers(self):\n",
    "        return len(self.hiddenLayerList)\n",
    "    \n",
    "    def Build(self):\n",
    "        if IsNone(self.lossFunction):\n",
    "            raise ValueError(\"Set loss function before build.\")\n",
    "        \n",
    "        if self.lossFunction.method == \"cross entropy\":\n",
    "            if type(self.outputFunction) not in [Sigmoid, Softmax, SoftSign]:\n",
    "                print (\"WARNING : chosen loss function is cross entropy but the output of output layer function may out of (0, 1)\")\n",
    "            \n",
    "        \n",
    "        if IsNone(self.outputFunction):\n",
    "            self.outputFunction = Identity()\n",
    "            print (\"WARNING : doesn't set outputFunction before build, set Identity().\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[0],\n",
    "                    self.validData.GetDatumSize()[0],\n",
    "                    self.testData.GetDatumSize()[0]\n",
    "                   ])) == 1:\n",
    "            self.inputs_size = self.trainData.GetDatumSize()[0]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same inputs size\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[1],\n",
    "                    self.validData.GetDatumSize()[1],\n",
    "                    self.testData.GetDatumSize()[1]\n",
    "                   ])) == 1:\n",
    "            self.labels_size = self.trainData.GetDatumSize()[1]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same labels size\")\n",
    "        \n",
    "        # when hidden layer exist, set first layer value\n",
    "        if self.GetNumHiddenLayers() >0:\n",
    "            self.hiddenLayerList[0].linear.SetValue(np.random.normal(0.,\n",
    "                                                                     1.,\n",
    "                                                                     (self.inputs_size, self.hiddenLayerList[0].num_unit)\n",
    "                                                                    )\n",
    "                                                   )\n",
    "            self.hiddenLayerList[0].bias.SetValue(np.random.normal(0.,\n",
    "                                                                   1.,\n",
    "                                                                   (1, self.hiddenLayerList[0].num_unit)\n",
    "                                                                  )\n",
    "                                                 )\n",
    "            # normalize flow_in by modify layer variables\n",
    "            self.hiddenLayerList[0].Forward(self.trainData.inputs)\n",
    "            flow_in_mean = self.hiddenLayerList[0].flow_in.mean(axis=0)\n",
    "            flow_in_std = self.hiddenLayerList[0].flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "            self.hiddenLayerList[0].linear.value /= flow_in_std\n",
    "            self.hiddenLayerList[0].bias.value -= flow_in_mean\n",
    "            self.hiddenLayerList[0].bias.value /= flow_in_std\n",
    "            self.hiddenLayerList[0].Forward(self.trainData.inputs)\n",
    "            # when hidden layer num >= 2, set internal layer value\n",
    "            for l in range(1, self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].linear.SetValue(np.random.normal(0.,\n",
    "                                                                         1.,\n",
    "                                                                         (self.hiddenLayerList[l-1].num_unit,\n",
    "                                                                          self.hiddenLayerList[l].num_unit\n",
    "                                                                         )\n",
    "                                                                        )\n",
    "                                                       )\n",
    "                self.hiddenLayerList[l].bias.SetValue(np.random.normal(0.,\n",
    "                                                                       1.,\n",
    "                                                                       (1, self.hiddenLayerList[l].num_unit)\n",
    "                                                                      )\n",
    "                                                     )\n",
    "                # normalize flow_in by modify layer variables\n",
    "                self.hiddenLayerList[l].Forward(self.hiddenLayerList[l-1].flow_out)\n",
    "                flow_in_mean = self.hiddenLayerList[l].flow_in.mean(axis=0)\n",
    "                flow_in_std = self.hiddenLayerList[l].flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "                self.hiddenLayerList[l].linear.value /= flow_in_std\n",
    "                self.hiddenLayerList[l].bias.value -= flow_in_mean\n",
    "                self.hiddenLayerList[l].bias.value /= flow_in_std\n",
    "                self.hiddenLayerList[l].Forward(self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            # set output layer\n",
    "            self.outputLayer = Layer(self.labels_size, self.outputFunction)\n",
    "            self.outputLayer.linear.SetValue(np.random.normal(0.,\n",
    "                                                              1.,\n",
    "                                                              (self.hiddenLayerList[-1].num_unit,\n",
    "                                                               self.outputLayer.num_unit\n",
    "                                                              )\n",
    "                                                             )\n",
    "                                            )\n",
    "            # normalize flow_in by modify layer variables\n",
    "            self.outputLayer.Forward(self.hiddenLayerList[-1].flow_out)\n",
    "            flow_in_mean = self.outputLayer.flow_in.mean(axis=0)\n",
    "            flow_in_std = self.outputLayer.flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "            self.outputLayer.linear.value /= flow_in_std\n",
    "            self.outputLayer.bias.value -= flow_in_mean\n",
    "            self.outputLayer.bias.value /= flow_in_std\n",
    "            self.outputLayer.Forward(self.hiddenLayerList[-1].flow_out)\n",
    "            \n",
    "        else: # case no hiddenlayer\n",
    "            self.outputLayer.linear.SetValue(np.random.normal(0.,\n",
    "                                                              1.,\n",
    "                                                              (self.inputs_size,\n",
    "                                                               self.outputLayer.num_unit\n",
    "                                                              )\n",
    "                                                             )\n",
    "                                            )\n",
    "        \n",
    "            self.outputLayer.bias.SetValue(np.random.normal(0.,\n",
    "                                                            1.,\n",
    "                                                            (1, self.outputLayer.num_unit)\n",
    "                                                           )\n",
    "                                          )\n",
    "            # normalize flow out to fit labels by modify layer variables\n",
    "            self.outputLayer.Forward(self.trainData.inputs)\n",
    "            flow_in_mean = self.outputLayer.flow_in.mean(axis=0)\n",
    "            flow_in_std = self.outputLayer.flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "            self.outputLayer.linear.value /= flow_in_std\n",
    "            self.outputLayer.bias.value -= flow_in_mean\n",
    "            self.outputLayer.bias.value /= flow_in_std\n",
    "            self.outputLayer.Forward(self.trainData.inputs)\n",
    "        \n",
    "        # Set regularizer\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            self.hiddenLayerList[l].SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "        \n",
    "        self.outputLayer.SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "        \n",
    "        self.has_build = True\n",
    "    \n",
    "    def save_model(self, target_folder):\n",
    "        if type(target_folder) != str:\n",
    "            raise TypeError(\"target_folder should be a str\")\n",
    "        \n",
    "        while target_folder[-1] ==\"/\":\n",
    "            target_folder = target_folder[:-1]\n",
    "        \n",
    "        if not os.path.exists(target_folder):\n",
    "            os.makedirs(target_folder)\n",
    "        \n",
    "        target_folder += \"/\"\n",
    "        \n",
    "        info_dict = {}\n",
    "        info_dict[\"inputs size\"] = self.inputs_size\n",
    "        info_dict[\"labels size\"] = self.labels_size\n",
    "        info_dict[\"loss\"] = self.lossFunction.method\n",
    "        info_dict[\"regularizer\"] = {\"rate\" : self.regulariz_rate,\n",
    "                                    \"margin\" : self.regulariz_margin\n",
    "                                   }\n",
    "        info_dict[\"tolerance\"] = self.hit_tolerance\n",
    "        info_dict[\"num units\"] = []\n",
    "        info_dict[\"activation function\"] = []\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            info_dict[\"num units\"].append(self.hiddenLayerList[l].num_unit)\n",
    "            info_dict[\"activation function\"].append(self.hiddenLayerList[l].activationFunction)\n",
    "        \n",
    "        info_dict[\"output function\"] = self.outputFunction\n",
    "        info_dict[\"terminator\"] = self.terminator\n",
    "        np.save(target_folder + \"model_info.npy\", info_dict)\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            np.save(target_folder + \"L%d.npy\" %(l), self.hiddenLayerList[l].linear.value)\n",
    "            np.save(target_folder + \"B%d.npy\" %(l), self.hiddenLayerList[l].bias.value)\n",
    "        \n",
    "        np.save(target_folder + \"Lo.npy\" , self.outputLayer.linear.value)\n",
    "        np.save(target_folder + \"Bo.npy\" , self.outputLayer.bias.value)\n",
    "    \n",
    "    def load_model(self, target_folder):\n",
    "        if len(set([self.trainData.GetDatumSize()[0],\n",
    "                    self.validData.GetDatumSize()[0],\n",
    "                    self.testData.GetDatumSize()[0]\n",
    "                   ])) == 1:\n",
    "            self.inputs_size = self.trainData.GetDatumSize()[0]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same inputs size\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[1],\n",
    "                    self.validData.GetDatumSize()[1],\n",
    "                    self.testData.GetDatumSize()[1]\n",
    "                   ])) == 1:\n",
    "            self.labels_size = self.trainData.GetDatumSize()[1]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same labels size\")\n",
    "        \n",
    "        if type(target_folder) != str:\n",
    "            raise TypeError(\"target_folder should be a str\")\n",
    "        \n",
    "        while target_folder[-1] ==\"/\":\n",
    "            target_folder = target_folder[:-1]\n",
    "        \n",
    "        target_folder += \"/\"\n",
    "        \n",
    "        self.has_build = False\n",
    "        \n",
    "        info_dict = np.load(target_folder + \"model_info.npy\").item()\n",
    "        if self.trainData.GetDatumSize() != (info_dict[\"inputs size\"], info_dict[\"labels size\"]):\n",
    "            raise ValueError(\"Datum size error, (inputs size, output size) for this model should be (%d, %d)\" %self.trainData.GetDatumSize())\n",
    "        \n",
    "        self.SetLossFunction(info_dict[\"loss\"])\n",
    "        self.SetRegularizer(info_dict[\"regularizer\"][\"rate\"], info_dict[\"regularizer\"][\"margin\"])\n",
    "        self.hit_tolerance = info_dict[\"tolerance\"]\n",
    "        \n",
    "        self.hiddenLayerList = []\n",
    "        for l in range(len(info_dict[\"num units\"])):\n",
    "            self.AddHiddenLayer(info_dict[\"num units\"][l], info_dict[\"activation function\"][l])\n",
    "        \n",
    "        self.SetOutputFunction(info_dict[\"output function\"])\n",
    "        self.terminator = info_dict[\"terminator\"]\n",
    "        self.Build()\n",
    "        \n",
    "        for l in range(len(info_dict[\"num units\"])):\n",
    "            self.hiddenLayerList[l].linear.value = np.load(target_folder + \"L%d.npy\" %(l))\n",
    "            self.hiddenLayerList[l].bias.value = np.load(target_folder + \"B%d.npy\" %(l))\n",
    "        \n",
    "        self.outputLayer.linear.value = np.load(target_folder + \"Lo.npy\")\n",
    "        self.outputLayer.bias.value = np.load(target_folder + \"Bo.npy\")\n",
    "    \n",
    "    def GetDimension(self):\n",
    "        output = self.outputLayer.GetDimension()\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            output += self.hiddenLayerList[l].GetDimension()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def GetInference(self, inputs):\n",
    "        if inputs.shape[1] != self.inputs_size:\n",
    "            raise ValueError(\"inputs size shoud be %d, get %d\" %(self.inputs_size, inputs.shape[1]))\n",
    "        \n",
    "        if self.GetNumHiddenLayers() > 0:\n",
    "            self.hiddenLayerList[0].Forward(inputs)\n",
    "            for l in range(1, self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].Forward(self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            self.outputLayer.Forward(self.hiddenLayerList[-1].flow_out)\n",
    "        else:\n",
    "            self.outputLayer.Forward(inputs)\n",
    "        \n",
    "        return self.outputLayer.flow_out\n",
    "    \n",
    "    def GetLoss(self, inference, target, weight = None):\n",
    "        if inference.shape != target.shape:\n",
    "            raise ValueError(\"shape between inference and target non-equal\")\n",
    "        \n",
    "        return self.lossFunction.GetLoss(inference, target, weight)\n",
    "    \n",
    "    def GetTrainLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.trainData.inputs), self.trainData.labels, self.trainData.weight)\n",
    "    \n",
    "    def GetValidLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.validData.inputs), self.validData.labels, self.validData.weight)\n",
    "    \n",
    "    def GetTestLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.testData.inputs), self.testData.labels, self.testData.weight)\n",
    "    \n",
    "    def GetAccuracy(self, inference, target, decimals=4, tolerance = 0.1):\n",
    "        # tolerance : for regression(r2) model, given a tolerance to verify hit or miss,\n",
    "        #             this variable is meaningless for classifycation\n",
    "        if self.lossFunction.method == \"cross entropy\":\n",
    "            output = (inference.argmax(axis=1) == target.argmax(axis=1)).mean()\n",
    "        elif self.lossFunction.method == \"r2\":\n",
    "            output = np.square(inference - target).sum(axis=1)\n",
    "            output = (output < tolerance).mean()\n",
    "        \n",
    "        return np.round(output, decimals)\n",
    "    \n",
    "    def GetTrainAccuracy(self, tolerance = None, decimals=4):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.trainData.inputs), self.trainData.labels, decimals, tolerance)\n",
    "    \n",
    "    def GetValidAccuracy(self, tolerance = None, decimals=4):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.validData.inputs), self.validData.labels, decimals, tolerance)\n",
    "    \n",
    "    def GetTestAccuracy(self, tolerance = None, decimals=4):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.testData.inputs), self.testData.labels, decimals, tolerance)\n",
    "    \n",
    "    def Backward(self, inputs, labels, weight):\n",
    "        deri = self.lossFunction.Backward(self.GetInference(inputs), labels, weight)\n",
    "        if self.GetNumHiddenLayers() > 0:\n",
    "            deri = self.outputLayer.Backward(deri, self.hiddenLayerList[-1].flow_out)\n",
    "            for l in range(self.GetNumHiddenLayers()-1, 0, -1):\n",
    "                deri = self.hiddenLayerList[l].Backward(deri, self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            deri = self.hiddenLayerList[0].Backward(deri, inputs)\n",
    "        else:\n",
    "            deri = self.outputLayer.Backward(deri, inputs)\n",
    "        \n",
    "    def ZeroDeri(self):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            deri = self.hiddenLayerList[l].ZeroDeri()\n",
    "        \n",
    "        deri = self.outputLayer.ZeroDeri()\n",
    "    \n",
    "    def ResetCwiseStep(self, new_cwise_step=0.1):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            deri = self.hiddenLayerList[l].ResetCwiseStep(new_cwise_step)\n",
    "        \n",
    "        deri = self.outputLayer.ResetCwiseStep(new_cwise_step)\n",
    "    \n",
    "    def Descent(self, step = 1., method = \"normal\"):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            self.hiddenLayerList[l].Descent(step, method)\n",
    "        \n",
    "        self.outputLayer.Descent(step, method)\n",
    "    \n",
    "    def BatchFit(self, batch_inputs, batch_labels, batch_weight, step = 1., method = \"normal\"):\n",
    "        batch_weight /= batch_weight.sum()\n",
    "        self.Backward(batch_inputs, batch_labels, batch_weight)\n",
    "        self.Descent(step, method)\n",
    "    \n",
    "    def EpochFit(self, batch_size = None, step = 1., method = \"normal\"):\n",
    "        if type(batch_size) == type(None):\n",
    "            self.BatchFit(self.trainData.inputs, self.trainData.labels, self.trainData.weight, step, method)\n",
    "        elif type(batch_size) == int:\n",
    "            if batch_size > 0:\n",
    "                for b in range(np.ceil(self.trainData.GetNumDatums()/ batch_size).astype(np.int)):\n",
    "                    self.BatchFit(self.trainData.inputs[b*batch_size: (b+1)*batch_size],\n",
    "                                  self.trainData.labels[b*batch_size: (b+1)*batch_size],\n",
    "                                  self.trainData.weight[b*batch_size: (b+1)*batch_size],\n",
    "                                  step,\n",
    "                                  method\n",
    "                                 )\n",
    "            else:\n",
    "                raise ValueError(\"batch_size should be positive int\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"batch_size should be positive int\")\n",
    "    \n",
    "    def Train(self, times, batch_size = None, step = 1., method = \"normal\", is_termination = False):\n",
    "        self.ResetCwiseStep()\n",
    "        self.terminator.Clear()\n",
    "        for t in range(times):\n",
    "            self.EpochFit(batch_size, step, method)\n",
    "            if is_termination:\n",
    "                if self.terminator.Hit(10*np.log10(self.GetValidLoss() + 0.000000001)):\n",
    "                # 0.000000001, bias for prevent error when log(0)\n",
    "                    return t+1\n",
    "        \n",
    "        return times\n",
    "    \n",
    "    def AddUnit(self, layer_index, num_added = 1, output_linear_bound = 1., cwise_step_initial = 0.1, drop_times=5):\n",
    "        if layer_index not in range(self.GetNumHiddenLayers()):\n",
    "            raise ValueError(\"layer_index should be an int from 0 to (#layer-1) for hiddenlayer\")\n",
    "        \n",
    "        if type(num_added) != int:\n",
    "            raise ValueError(\"num_added should be positive int\")\n",
    "        elif num_added <= 0:\n",
    "            raise ValueError(\"num_added should be positive int\")\n",
    "        \n",
    "        try:\n",
    "            if output_linear_bound < 0.:\n",
    "                raise ValueError(\"output_weight_bound should be non-negative\")\n",
    "        except:\n",
    "            raise ValueError(\"output_weight_bound should a non-negative real value\")\n",
    "        \n",
    "        new_linear = self.hiddenLayerList[layer_index].linear.value\n",
    "        new_bias = self.hiddenLayerList[layer_index].bias.value\n",
    "        for u in range(num_added):\n",
    "            new_linear = McmcColExtend(new_linear, drop_times)\n",
    "            new_bias = McmcColExtend(new_bias, drop_times)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].linear.SetValue(new_linear, cwise_step_initial)\n",
    "        self.hiddenLayerList[layer_index].bias.SetValue(new_bias, cwise_step_initial)\n",
    "        \n",
    "        if layer_index < self.GetNumHiddenLayers() - 1: # if not final hiddenlayer\n",
    "            new_output_linear = self.hiddenLayerList[layer_index+1].linear.value\n",
    "            new_output_linear = np.append(new_output_linear,\n",
    "                                         output_linear_bound*2*np.random.rand(num_added, self.hiddenLayerList[layer_index+1].num_unit),\n",
    "                                         axis=0\n",
    "                                         )\n",
    "            self.hiddenLayerList[layer_index+1].linear.SetValue(new_output_linear, cwise_step_initial)\n",
    "        \n",
    "        else: # if final hiddenlayer\n",
    "            new_output_linear = self.outputLayer.linear.value\n",
    "            new_output_linear = np.append(new_output_linear,\n",
    "                                         output_linear_bound*2*np.random.rand(num_added, self.outputLayer.num_unit),\n",
    "                                         axis=0\n",
    "                                         )\n",
    "            self.outputLayer.linear.SetValue(new_output_linear, cwise_step_initial)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].num_unit += num_added\n",
    "        \"\"\"    \n",
    "            def EliminationUnit(self, layer_index):\n",
    "                if self.hiddenLayerList[layer_index].num_unit == 1:\n",
    "                    raise ValueError(\"layer %d has only 1 unit, pass this process\"%(layer_index))\n",
    "\n",
    "                if layer_index == self.GetNumHiddenLayers() - 1:\n",
    "                    col_index, linear_trans, mse = LinearRefine(self.hiddenLayerList[layer_index].flow_out,\n",
    "                                                                self.outputLayer.flow_in\n",
    "                                                               )\n",
    "\n",
    "                    self.hiddenLayerList[layer_index].\n",
    "\n",
    "\n",
    "                self.hiddenLayerList[layer_index]\n",
    "        \"\"\"\n",
    "    def UnitsRefined(self, layer_index, reference_data = None, weight = None, method=\"remove\", threshold = 1):\n",
    "        # threshold : threshold for information contained of dimension be remaind\n",
    "        if self.hiddenLayerList[layer_index].num_unit == 1:\n",
    "            print(\"WARNING : Units Refining failed, layer %d has only 1 unit, pass this process\"%(layer_index))\n",
    "            return None\n",
    "        \n",
    "        if type(method) != str:\n",
    "            raise ValueError(\"method must be a str, and equal to 'remain', 'remove', 'info' or 'info ratio'.\")\n",
    "        \n",
    "        if type(layer_index) != int:\n",
    "            raise TypeError(\"layer_index should be an int between 0 to (# hidden layers - 1)\")\n",
    "        elif (layer_index > self.GetNumHiddenLayers() - 1) or (layer_index < 0):\n",
    "            raise ValueError(\"layer_index should be an int between 0 to (# hidden layers - 1)\")\n",
    "        \n",
    "        if (type(threshold) == int) and (method == \"remove\"):\n",
    "            if (threshold > self.hiddenLayerList[layer_index].num_unit-1):\n",
    "                raise ValueError(\"int threshold error : removed #neuron should less than currently #neuron\")\n",
    "            elif (threshold < -self.hiddenLayerList[layer_index].num_unit) or (threshold==0):\n",
    "                return None\n",
    "                # do nothing if remove no #neuron (threshold=0) or want to remain #neuron more than currently\n",
    "            \n",
    "        elif ((threshold > 1) and (threshold < 0)):\n",
    "            raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "        \n",
    "        if IsNone(reference_data):\n",
    "            reference_data = self.trainData.inputs\n",
    "            weight = self.trainData.weight\n",
    "        \n",
    "        self.GetInference(reference_data)\n",
    "        \n",
    "        if IsNone(weight):\n",
    "            weight = np.ones(reference_data.shape[0])\n",
    "            weight /= weight.size\n",
    "        \n",
    "        info = self.hiddenLayerList[layer_index].GetPCVar(weight)\n",
    "        if method in [\"remove\", \"remain\"]:\n",
    "            if type(threshold) != int:\n",
    "                raise TypeError(\"when using method 'remove' or 'remain', threshold represent the num of units want to remove, should be an int\")\n",
    "            \n",
    "            if method == \"remove\":\n",
    "                if (threshold < 0) or (threshold >= len(info)):\n",
    "                    raise ValueError(\"when using method 'remove', threshold should be in [0, #units - 1]\")\n",
    "                \n",
    "                cut_index = threshold\n",
    "            else: # case \"remain\"\n",
    "                if (threshold <= 0) or (threshold > len(info)):\n",
    "                    raise ValueError(\"when using method 'remain', threshold should be in [1, #units]\")\n",
    "                \n",
    "                cut_index = self.hiddenLayerList[layer_index].num_unit - threshold\n",
    "        \n",
    "        elif method in [\"info\", \"info ratio\"]:\n",
    "            if method == \"info ratio\":\n",
    "                threshold *= info.sum()\n",
    "            \n",
    "            cut_index = self.hiddenLayerList[layer_index].num_unit - (info >= threshold).sum()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"method must be 'remain', 'remove', 'info' or 'info ratio'.\")\n",
    "        \n",
    "        if layer_index < self.GetNumHiddenLayers() - 1: # case not final hidden layer\n",
    "            units_remain, new_linear, new_bias, mse = LinearRefine(self.hiddenLayerList[layer_index].flow_out,\n",
    "                                                                   self.hiddenLayerList[layer_index+1].flow_in,\n",
    "                                                                   num_elimination=cut_index,\n",
    "                                                                   regularizer = 10**-10\n",
    "                                                                  )\n",
    "            self.hiddenLayerList[layer_index+1].linear.SetValue(new_linear)\n",
    "            self.hiddenLayerList[layer_index+1].bias.SetValue(new_bias)\n",
    "        else:\n",
    "            units_remain, new_linear, new_bias, mse = LinearRefine(self.hiddenLayerList[layer_index].flow_out,\n",
    "                                                                   self.outputLayer.flow_in,\n",
    "                                                                   num_elimination=cut_index,\n",
    "                                                                   regularizer = 10**-10\n",
    "                                                                  )\n",
    "            self.outputLayer.linear.SetValue(new_linear)\n",
    "            self.outputLayer.bias.SetValue(new_bias)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].linear.SetValue(self.hiddenLayerList[layer_index].linear.value[:, units_remain])\n",
    "        self.hiddenLayerList[layer_index].bias.SetValue(self.hiddenLayerList[layer_index].bias.value[:, units_remain])\n",
    "        self.hiddenLayerList[layer_index].num_unit -= cut_index\n",
    "        \n",
    "# --------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "        cut_components = principal_components[:cut_index]\n",
    "        coordinate_representation, pivots = RowOperate(principal_components[cut_index:], large_element_alert=enhance_alert)\n",
    "        \n",
    "        if layer_index < self.GetNumHiddenLayers() - 1: # case not final hidden layer\n",
    "            new_linear_value = np.dot(coordinate_representation, self.hiddenLayerList[layer_index + 1].linear.value)\n",
    "            self.hiddenLayerList[layer_index + 1].bias.SetValue(self.hiddenLayerList[layer_index + 1].bias.value\n",
    "                                                                + np.dot(mean_flow_out, self.hiddenLayerList[layer_index + 1].linear.value)\n",
    "                                                                - np.dot(mean_flow_out[pivots], new_linear_value)\n",
    "                                                               )\n",
    "            self.hiddenLayerList[layer_index + 1].linear.SetValue(new_linear_value)\n",
    "        else:\n",
    "            new_linear_value = np.dot(coordinate_representation, self.outputLayer.linear.value)\n",
    "            self.outputLayer.bias.SetValue(self.outputLayer.bias.value\n",
    "                                           + np.dot(mean_flow_out, self.outputLayer.linear.value)\n",
    "                                           - np.dot(mean_flow_out[pivots], new_linear_value)\n",
    "                                          )\n",
    "            self.outputLayer.linear.SetValue(new_linear_value)\n",
    "        \n",
    "        if enhance_alert:\n",
    "            if len(cut_components) > 0:\n",
    "                trans_cut_components = np.dot(cut_components[:, pivots], coordinate_representation)\n",
    "                trans_cut_components_magnitude = np.linalg.norm(trans_cut_components, axis=1)\n",
    "                if trans_cut_components_magnitude.max() > 1.01:\n",
    "                    print(\"WARAING : noise after refine will be enhance, \", trans_cut_components_magnitude)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].linear.SetValue(self.hiddenLayerList[layer_index].linear.value[:, pivots])\n",
    "        self.hiddenLayerList[layer_index].bias.SetValue(self.hiddenLayerList[layer_index].bias.value[:, pivots])\n",
    "        self.hiddenLayerList[layer_index].num_unit = len(pivots)\n",
    "\n",
    "    def UnitsRefined(self, layer_index, reference_data = None, weight = None, method=\"remain\", threshold = 0.01, enhance_alert=False):\n",
    "        # threshold : threshold for information contained of dimension be remaind\n",
    "        if self.hiddenLayerList[layer_index].num_unit == 1:\n",
    "            print(\"WARNING : Units Refining failed, layer %d has only 1 unit, pass this process\"%(layer_index))\n",
    "            return None\n",
    "        \n",
    "        if type(method) != str:\n",
    "            raise ValueError(\"method must be a str, and equal to 'remain', 'remove', 'info' or 'info ratio'.\")\n",
    "        \n",
    "        if type(layer_index) != int:\n",
    "            raise TypeError(\"layer_index should be an int between 0 to (# hidden layers - 1)\")\n",
    "        elif (layer_index > self.GetNumHiddenLayers() - 1) or (layer_index < 0):\n",
    "            raise ValueError(\"layer_index should be an int between 0 to (# hidden layers - 1)\")\n",
    "        \n",
    "        if (type(threshold) == int) and (method == \"remove\"):\n",
    "            if (threshold > self.hiddenLayerList[layer_index].num_unit-1):\n",
    "                raise ValueError(\"int threshold error : removed #neuron should less than currently #neuron\")\n",
    "            elif (threshold < -self.hiddenLayerList[layer_index].num_unit) or (threshold==0):\n",
    "                return None\n",
    "                # do nothing if remove no #neuron (threshold=0) or want to remain #neuron more than currently\n",
    "            \n",
    "        elif ((threshold > 1) and (threshold < 0)):\n",
    "            raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "        \n",
    "        if IsNone(reference_data):\n",
    "            reference_data = self.trainData.inputs\n",
    "            weight = self.trainData.weight\n",
    "        \n",
    "        self.GetInference(reference_data)\n",
    "        \n",
    "        if IsNone(weight):\n",
    "            weight = np.ones(reference_data.shape[0])\n",
    "            weight /= weight.size\n",
    "        \n",
    "        info, principal_components, mean_flow_out, centered_flow_out = self.hiddenLayerList[layer_index].GetPCA(weight)\n",
    "        if method in [\"remove\", \"remain\"]:\n",
    "            if type(threshold) != int:\n",
    "                raise TypeError(\"when using method 'remove' or 'remain', threshold represent the num of units want to remove, should be an int\")\n",
    "            \n",
    "            if method == \"remove\":\n",
    "                if (threshold < 0) or (threshold >= len(info)):\n",
    "                    raise ValueError(\"when using method 'remove', threshold should be in [0, #units - 1]\")\n",
    "                \n",
    "                cut_index = threshold\n",
    "            else: # case \"remain\"\n",
    "                if (threshold <= 0) or (threshold > len(info)):\n",
    "                    raise ValueError(\"when using method 'remain', threshold should be in [1, #units]\")\n",
    "                \n",
    "                cut_index = -threshold\n",
    "            \n",
    "        elif method in [\"info\", \"info ratio\"]:\n",
    "            if method == \"info ratio\":\n",
    "                threshold *= info.sum()\n",
    "            \n",
    "            cut_index = -(info >= threshold).sum()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"method must be 'remain', 'remove', 'info' or 'info ratio'.\")\n",
    "        \n",
    "        cut_components = principal_components[:cut_index]\n",
    "        coordinate_representation, pivots = RowOperate(principal_components[cut_index:], large_element_alert=enhance_alert)\n",
    "        \n",
    "        if layer_index < self.GetNumHiddenLayers() - 1: # case not final hidden layer\n",
    "            new_linear_value = np.dot(coordinate_representation, self.hiddenLayerList[layer_index + 1].linear.value)\n",
    "            self.hiddenLayerList[layer_index + 1].bias.SetValue(self.hiddenLayerList[layer_index + 1].bias.value\n",
    "                                                                + np.dot(mean_flow_out, self.hiddenLayerList[layer_index + 1].linear.value)\n",
    "                                                                - np.dot(mean_flow_out[pivots], new_linear_value)\n",
    "                                                               )\n",
    "            self.hiddenLayerList[layer_index + 1].linear.SetValue(new_linear_value)\n",
    "        else:\n",
    "            new_linear_value = np.dot(coordinate_representation, self.outputLayer.linear.value)\n",
    "            self.outputLayer.bias.SetValue(self.outputLayer.bias.value\n",
    "                                           + np.dot(mean_flow_out, self.outputLayer.linear.value)\n",
    "                                           - np.dot(mean_flow_out[pivots], new_linear_value)\n",
    "                                          )\n",
    "            self.outputLayer.linear.SetValue(new_linear_value)\n",
    "        \n",
    "        if enhance_alert:\n",
    "            if len(cut_components) > 0:\n",
    "                trans_cut_components = np.dot(cut_components[:, pivots], coordinate_representation)\n",
    "                trans_cut_components_magnitude = np.linalg.norm(trans_cut_components, axis=1)\n",
    "                if trans_cut_components_magnitude.max() > 1.01:\n",
    "                    print(\"WARAING : noise after refine will be enhance, \", trans_cut_components_magnitude)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].linear.SetValue(self.hiddenLayerList[layer_index].linear.value[:, pivots])\n",
    "        self.hiddenLayerList[layer_index].bias.SetValue(self.hiddenLayerList[layer_index].bias.value[:, pivots])\n",
    "        self.hiddenLayerList[layer_index].num_unit = len(pivots)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "\"\"\"\n",
    "    def inter_layer_linear_regression(self, layer_interval):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                if ls == 0:\n",
    "                    ri = np.array(self.px.T) # regression input\n",
    "                else:\n",
    "                    ri = np.array(self.ly[ls-1].y)\n",
    "                \n",
    "                ri = np.append(ri, np.ones((1, ri.shape[1])), axis=0) # append 1. for each datum as bias\n",
    "                ro = np.array(self.ly[le].x)\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        rr = np.linalg.lstsq(ri.T, ro.T) # regression result (matrix, residuals, rank of ri, singuler values of ri)\n",
    "        if len(rr[1]) == 0:\n",
    "            raise ValueError(\"output data of layer\" + str(ls-1) + \"(= -1, for input data) should be full rank, try self.nruron_refine first\")\n",
    "        \n",
    "        return rr[0], rr[1]/ri.shape[1]\n",
    "    \n",
    "    def find_linearist_layers(self, reference_data = None):\n",
    "        output = (0, 0, np.inf, np.array([[]]), np.zeros((0,0)))\n",
    "        if type(reference_data) == type(None):\n",
    "            self.prediction(self.tx)\n",
    "        else:\n",
    "            self.prediction(reference_data)\n",
    "        \n",
    "        for l1 in range(self.ln-1):\n",
    "            for l2 in range(i+1, self.ln):\n",
    "                rr = self.inter_layer_linear_regression((l1,l2))\n",
    "                if np.sqrt(rr[1].sum()) < output[2]:\n",
    "                    output = (l1, l2, np.sqrt(rr[1].sum()), rr[0])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def layer_filled(self, layer_interval, weights, bias):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        \n",
    "        if weights.shape[0] != bias.shape[0]:\n",
    "            raise ValueError(\"weights.shape[0] doesn't match bias.shape[0]\")\n",
    "        \n",
    "        if weights.shape[0] != self.ly[le].nn:\n",
    "            raise ValueError(\"weights.shape[0] doesn't match #neuron of layer at end of layer_interval\")\n",
    "        \n",
    "        self.ly[le].w.assign_values(weights)\n",
    "        self.ly[le].b.assign_values(bias)\n",
    "        self.ly = self.ly[:ls] + self.ly[le:]\n",
    "        self.ln = len(self.ly)\n",
    "    \n",
    "    def linear_filled(self, layer_interval):\n",
    "        try:\n",
    "            ls = layer_interval[0] # layer start\n",
    "            le = layer_interval[1] # layer end\n",
    "            if (ls < le) and (ls >= 0) and (le < self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "        except:\n",
    "            raise ValueError(\"layer_interval should be list-like, two int (a, b), with 0 <= a < b < total layer\")\n",
    "            \n",
    "        rr = self.inter_layer_linear_regression(layer_interval)\n",
    "        self.layer_filled(layer_interval, rr[0].T[:,:-1], rr[0].T[:,-1:])\n",
    "    \n",
    "    def insert_layer(self, position, weights, bias, activation_function, next_layer_weights, next_layer_bias):\n",
    "        if type(position) == int:\n",
    "            if position in range(self.ln):\n",
    "                pass\n",
    "        else:\n",
    "            raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        \n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        ilo, ili = weights.shape # input and output size of inserted layer\n",
    "        nlo, nli = next_layer_weights.shape # input and output size of next layer\n",
    "        \n",
    "        if position == 0:\n",
    "            if ili != self.xs:\n",
    "                raise ValueError(\"weights.shape error, cheak input and output size for this new layer\")\n",
    "        else:\n",
    "            if ili != self.ly[position-1].nn:\n",
    "                raise ValueError(\"weights.shape error, cheak input and output size for this new layer\")\n",
    "        \n",
    "        if (ilo != bias.shape[0]) or (ilo != nli):\n",
    "            raise ValueError(\"to define #neuron of new layer, all related weighs and bias size should be consistent\")\n",
    "        \n",
    "        if nlo != self.ly[position].nn:\n",
    "            raise ValueError(\"next_layer_weights.shape error, cheak #neuron of next layer\")\n",
    "        \n",
    "        if next_layer_bias.shape[0] != self.ly[position].nn:\n",
    "            raise ValueError(\"next_layer_bias.shape error, cheak #neuron of next layer\")\n",
    "        \n",
    "        if (bias.shape[1] != 1) or (next_layer_bias.shape[1] != 1):\n",
    "            raise ValueError(\"bias shape should be (#neuron, 1)\")\n",
    "        \n",
    "        l = position\n",
    "        \n",
    "        self.ly.insert(l, Layer(ilo, activation_function))\n",
    "        self.ly[l].w.assign_values(weights)\n",
    "        self.ly[l].b.assign_values(bias)\n",
    "        self.ly[l+1].w.assign_values(next_layer_weights)\n",
    "        self.ly[l+1].b.assign_values(next_layer_bias)\n",
    "        \n",
    "        self.ln = len(self.ly)\n",
    "    \n",
    "    def identity_dig(self, position, activation_function):\n",
    "        if type(position) == int:\n",
    "            if position in range(self.ln):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        else:\n",
    "            raise ValueError(\"position should be int between 0 to self.ln\")\n",
    "        \n",
    "        if type(activation_function) == type:\n",
    "            raise TypeError(\"activation_function should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        l = position\n",
    "        # ids : size of identity transform, input size of new layer\n",
    "        if l == 0:\n",
    "            ids = self.xs\n",
    "        else:\n",
    "            ids = self.ly[l-1].nn\n",
    "        \n",
    "        if type(activation_function) in [Relu, SoftPlus]:\n",
    "            liw = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 0)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 1)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) == LeakyRelu:\n",
    "            liw = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 0)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.concatenate((np.identity(ids), -np.identity(ids)), axis = 1) / (1.+activation_function.alpha)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) == Identity:\n",
    "            liw = np.identity(ids)\n",
    "            lib = np.zeros((2*ids, 1))\n",
    "            low = np.identity(ids)\n",
    "            lob = np.zeros((ids, 1))\n",
    "        elif type(activation_function) in [Sigmoid, Hypertan, Selu]:\n",
    "            # li : input of new layer\n",
    "            if l == 0:\n",
    "                li = np.array(self.tx.T)\n",
    "            else:\n",
    "                li = np.array(self.ly[l-1].y)\n",
    "            \n",
    "            lim = li.mean(axis=1)\n",
    "            lis = li.std(axis=1) + 1.\n",
    "            \n",
    "            liw = np.diag(1./lis)\n",
    "            if type(activation_function) == Selu:\n",
    "                lib = 1.-(lim/lis).reshape(-1,1) # let mean become one before transform by activation function\n",
    "            else:\n",
    "                lib = -(lim/lis).reshape(-1,1) # let mean become zero before transform by activation function\n",
    "            \n",
    "            lo = activation_function.trans(np.dot(liw, li)+lib)\n",
    "            lo = np.append(lo, np.ones((1, lo.shape[1])), axis=0) # append 1. for each datum as bias\n",
    "            rr = np.linalg.lstsq(lo.T, li.T) # regression result (matrix, residuals, rank of ri, singuler values of ri)\n",
    "            # since the goal is construct identity, try to find linear transform form layer output to layer input\n",
    "            low = rr[0].T[:,:-1]\n",
    "            lob = rr[0].T[:,-1:]\n",
    "        else:\n",
    "            raise TypeError(\"activation_function type error\")\n",
    "        \n",
    "        nlw = np.dot(self.ly[l].w.v, low)\n",
    "        nlb = np.dot(self.ly[l].w.v, lob) + self.ly[l].b.v\n",
    "        \n",
    "        self.insert_layer(l,\n",
    "                          liw,\n",
    "                          lib,\n",
    "                          activation_function,\n",
    "                          nlw,\n",
    "                          nlb\n",
    "                         )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ((np.arange(400)-200)/100).reshape(-1,1)\n",
    "Y = np.zeros((X.shape[0], 2))\n",
    "Y[:, :1] = 1*(X>-1.)\n",
    "Y[:, :1] *= 1*(X<1.)\n",
    "Y[:, 1:] = 1 - Y[:, :1]\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X, Y)\n",
    "NN.SetValidData(X, Y)\n",
    "NN.SetTestData(X, Y)\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.01, 3.)\n",
    "NN.AddHiddenLayer(10, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,20,-0.01)\n",
    "NN.Build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NN.Train(times=500, method=\"Rprop\", is_termination=False), end = \", \")\n",
    "print(NN.hiddenLayerList[0].num_unit, NN.GetTrainLoss())\n",
    "\n",
    "for t in range(5):\n",
    "    NN.UnitsRefined(layer_index=0, method=\"info\", threshold=0.01)\n",
    "    print(NN.Train(times=500, method=\"Rprop\", is_termination=False), end = \", \")\n",
    "    print(NN.hiddenLayerList[0].num_unit, NN.GetTrainLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Odd or even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ((np.arange(800)-400)/100).reshape(-1,1)\n",
    "Y = np.zeros(X.shape)\n",
    "Y[:, :] = 1*((X%2)>=1.)\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X, Y)\n",
    "NN.SetValidData(X, Y)\n",
    "NN.SetTestData(X, Y)\n",
    "NN.SetLossFunction(\"r2\")\n",
    "NN.SetRegularizer(0.001, 0.)\n",
    "NN.AddHiddenLayer(10, Hypertan())\n",
    "NN.SetOutputFunction(Sigmoid())\n",
    "NN.SetTerminator(10,30,-0.01)\n",
    "NN.Build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(NN.Train(times=500, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "print(NN.hiddenLayerList[0].num_unit, \", \", NN.GetTrainLoss())\n",
    "\n",
    "plt.plot(X.reshape(-1), NN.GetInference(NN.trainData.inputs)[:,0], \"bp\")\n",
    "plt.show()\n",
    "\n",
    "for t in range(10):\n",
    "    NN.AddUnit(layer_index=0, num_added=3, output_linear_bound=0.1)\n",
    "    print(NN.Train(times=500, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    NN.UnitsRefined(layer_index=0, method=\"remain\", threshold=7)\n",
    "    print(NN.Train(times=500, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    print(NN.hiddenLayerList[0].num_unit, \", \", NN.GetTrainLoss())\n",
    "    if NN.GetTrainLoss()>1.:\n",
    "        break\n",
    "    else:\n",
    "        pre_input = NN.hiddenLayerList[0].flow_out\n",
    "        pre_output = NN.outputLayer.flow_in\n",
    "    \n",
    "    #plt.plot(X.reshape(-1), NN.GetInference(NN.trainData.inputs)[:,0], \"bp\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skew chessboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = 4*np.random.rand(20000,2) - 2\n",
    "Y = np.zeros((X.shape[0],2))\n",
    "Y[:, 0] = (((np.floor((X[:,0] + X[:,1])%2) + np.floor((X[:,0] - X[:,1])%2)) %2) == 1)\n",
    "Y[:, 1] = 1- Y[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:10000], Y[:10000])\n",
    "NN.SetValidData(X[10000:15000], Y[10000:15000])\n",
    "NN.SetTestData(X[15000:], Y[15000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 1.)\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.)\n",
    "\n",
    "times = 100\n",
    "record = np.zeros((times))\n",
    "for t in range(times):\n",
    "    NN.Build()\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "    record[t] = NN.GetTestAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.sort()\n",
    "record[-25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat add-fit-kill-fit for fix number of units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:10000], Y[:10000])\n",
    "NN.SetValidData(X[10000:15000], Y[10000:15000])\n",
    "NN.SetTestData(X[15000:], Y[15000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 1.)\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.01)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=1000, batch_size=5000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(20):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.AddUnit(layer_index=l, num_added=2, output_linear_bound=0.01)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"remove\", threshold=2)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "I = NN.GetInference(X)[:, 0]\n",
    "plt.plot(X[:, 0][I>0.5], X[:, 1][I>0.5], \"bp\")\n",
    "plt.plot(X[:, 0][I<0.5], X[:, 1][I<0.5], \"rp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find suitable units by info ratio of layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:10000], Y[:10000])\n",
    "NN.SetValidData(X[10000:15000], Y[10000:15000])\n",
    "NN.SetTestData(X[15000:], Y[15000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 1.)\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=1000, batch_size=5000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(20):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.AddUnit(layer_index=l, num_added=1, output_linear_bound=0.01)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"info ratio\", threshold=0.001)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "I = NN.GetInference(X)[:, 0]\n",
    "plt.plot(X[:, 0][I>0.5], X[:, 1][I>0.5], \"bp\")\n",
    "plt.plot(X[:, 0][I<0.5], X[:, 1][I<0.5], \"rp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Is Skin\" on UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.load(\"../UCI/Is-skin/data-npy/data.npy\").astype(np.float64)\n",
    "Y = np.load(\"../UCI/Is-skin/data-npy/label.npy\").astype(np.float64)\n",
    "X *= 2/255\n",
    "X -= 1\n",
    "shuffle = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffle)\n",
    "X = X[shuffle]\n",
    "Y = Y[shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:150000], Y[:150000])\n",
    "NN.SetValidData(X[150000:200000], Y[150000:200000])\n",
    "NN.SetTestData(X[200000:], Y[200000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 0.)\n",
    "NN.AddHiddenLayer(2, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.01)\n",
    "for t in range(10):\n",
    "    NN.Build()\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat add-fit-kill-fit for fix number of units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:150000], Y[:150000])\n",
    "NN.SetValidData(X[150000:200000], Y[150000:200000])\n",
    "NN.SetTestData(X[200000:], Y[200000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 1.)\n",
    "NN.AddHiddenLayer(2, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(10):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.AddUnit(layer_index=l, num_added=1, output_linear_bound=0.01)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"remain\", threshold=2, enhance_alert=False)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find suitable units by info ratio of layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:150000], Y[:150000])\n",
    "NN.SetValidData(X[150000:200000], Y[150000:200000])\n",
    "NN.SetTestData(X[200000:], Y[200000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 0.)\n",
    "NN.AddHiddenLayer(10, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.01)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(20):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.AddUnit(layer_index=l, num_added=1, output_linear_bound=0.01)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"info ratio\", threshold=0.0001)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background delete for nokia machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((300000, 900))\n",
    "Y_train = np.zeros((300000, 2))\n",
    "\n",
    "X_valid = np.zeros((20000, 900))\n",
    "Y_valid = np.zeros((20000, 2))\n",
    "\n",
    "\n",
    "name = [0, 650]\n",
    "for f in range(len(name)):\n",
    "    file = name[f]\n",
    "    shuffle = np.arange(450*600)\n",
    "    np.random.shuffle(shuffle)\n",
    "    shuffle = shuffle[:110000]\n",
    "    \n",
    "    X_train[100000*f:100000*(f+1)] = np.load(\"../../linker/Nokia/frame%d_inputs.npy\" %(file)).astype(np.float64)[shuffle[:100000]]\n",
    "    Y_train[100000*f:100000*(f+1),0]=np.load(\"../../linker/Nokia/frame%d_labels.npy\" %(file))[shuffle[:100000]]\n",
    "    Y_train[100000*f:100000*(f+1),1]=1-Y_train[100000*f:100000*(f+1),0]\n",
    "    \n",
    "    X_valid[10000*f:10000*(f+1)] = np.load(\"../../linker/Nokia/frame%d_inputs.npy\" %(file)).astype(np.float64)[shuffle[100000:]]\n",
    "    Y_valid[10000*f:10000*(f+1),0]=np.load(\"../../linker/Nokia/frame%d_labels.npy\" %(file))[shuffle[100000:]]\n",
    "    Y_valid[10000*f:10000*(f+1),1]=1-Y_valid[10000*f:10000*(f+1),0]\n",
    "\n",
    "shuffle = np.arange(180*500)\n",
    "np.random.shuffle(shuffle)\n",
    "shuffle = shuffle[:50000]\n",
    "\n",
    "X_train[200000:250000] = np.load(\"../../linker/Nokia/frame300_part_inputs.npy\").astype(np.float64)[:50000]\n",
    "Y_train[200000:250000, 0] = np.load(\"../../linker/Nokia/frame300_part_labels.npy\")[:50000]\n",
    "Y_train[200000:250000, 1] = 1 - Y_train[200000:250000, 0]\n",
    "\n",
    "X_train[250000:] = np.load(\"../../linker/Nokia/frame1335_part_inputs.npy\").astype(np.float64)[shuffle]\n",
    "Y_train[250000:, 0] = np.load(\"../../linker/Nokia/frame1335_part_labels.npy\")[shuffle]\n",
    "Y_train[250000:, 1] = 1 - Y_train[250000:, 0]\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X_train, Y_train)\n",
    "NN.SetValidData(X_valid, Y_valid)\n",
    "NN.SetTestData(X_valid, Y_valid)\n",
    "\n",
    "\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.01, 3.)\n",
    "NN.AddHiddenLayer(1000, Hypertan())\n",
    "NN.AddHiddenLayer(100, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(5,10,-0.0)\n",
    "NN.Build()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.zeros((10,900))\n",
    "Y_train = np.zeros((10,2))\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X_train, Y_train)\n",
    "NN.SetValidData(X_train, Y_train)\n",
    "NN.SetTestData(X_train, Y_train)\n",
    "NN.load_model(\"../../linker/Nokia/get\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.Train(times=200, method=\"Rprop\", is_termination=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(NN.GetTrainAccuracy())\n",
    "print(NN.GetValidAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "for f in [0, 300, 400, 650]:\n",
    "    img = np.load(\"../../linker/Nokia/frame%d.npy\" %(f))\n",
    "    img_array = np.load(\"../../linker/Nokia/frame%d_inputs.npy\" %(f))\n",
    "    mask = np.zeros((480,648), np.uint8)\n",
    "    mask[7:457, 7:607] = (NN.GetInference(img_array)[:,0]>0.5).reshape(450,600).astype(np.uint8)\n",
    "    \n",
    "    kernel = np.ones((7,7), np.uint8)\n",
    "    mask = cv2.erode(mask, kernel, iterations=3)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=6)\n",
    "    mask = cv2.erode(mask, kernel, iterations=4)\n",
    "    for c in range(3):\n",
    "        img[:,:,c] *= mask\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.save_model(\"../../linker/Nokia/get\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "vidcap = cv2.VideoCapture(\"../../linker/Nokia/movie-A.MP4\")\n",
    "\n",
    "success = True\n",
    "success, img = vidcap.read()\n",
    "\n",
    "counter = 0\n",
    "kernel = np.ones((9,9), np.uint8)\n",
    "while True:\n",
    "    success, img = vidcap.read()\n",
    "    \n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    if ((counter % 15) == 0):\n",
    "        print(counter)\n",
    "        img = img[:,100:-100]\n",
    "        info = np.zeros(img.shape[:2]+ (4,))\n",
    "        info[:,:,3] = img.mean(axis=2)/255\n",
    "        info[:,:,:3] = Softmax().Forward(img.reshape(-1,3)/10).reshape(img.shape)\n",
    "        data = np.zeros((450, 600, 15*15*4))\n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                data[:,:,60*i+4*j:60*i+4*j+4] = info[i:i+450, j:j+600]\n",
    "        \n",
    "        mask = np.zeros((680,848), np.uint8)\n",
    "        mask[107:557, 107:707] = (NN.GetInference(data.reshape(-1,15*15*4))[:,0]>0.5).reshape(450,600).astype(np.uint8)\n",
    "        \n",
    "        mask = cv2.dilate(mask, kernel, iterations=8)\n",
    "        mask = cv2.erode(mask, kernel, iterations=8)\n",
    "        \n",
    "        mask = mask[100:-100, 100:-100]\n",
    "        \n",
    "        png = np.zeros(img.shape[:2]+ (4,), dtype=np.uint8)\n",
    "        \n",
    "        png[:,:,:3] = img\n",
    "        png[:,:,3] = mask*255\n",
    "        \n",
    "        cv2.imwrite(\"../../linker/Nokia/pngs/movie-A/frame%s.png\" %(str(counter).zfill(4)), png[:500, 150:-50])\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "print(\"down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case movie-d\n",
    "\n",
    "rotate first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "vidcap = cv2.VideoCapture(\"../../linker/Nokia/movie-D.MP4\")\n",
    "\n",
    "success = True\n",
    "success, img = vidcap.read()\n",
    "\n",
    "counter = 0\n",
    "kernel = np.ones((9,9), np.uint8)\n",
    "while True:\n",
    "    success, img = vidcap.read()\n",
    "    \n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    if ((counter % 15) == 0):\n",
    "        print(counter)\n",
    "        img = img[:,100:-100]\n",
    "        img = np.rot90(img,3)\n",
    "        \"\"\"\n",
    "        info = np.zeros(img.shape[:2]+ (4,))\n",
    "        info[:,:,3] = img.mean(axis=2)/255\n",
    "        info[:,:,:3] = Softmax().Forward(img.reshape(-1,3)/10).reshape(img.shape)\n",
    "        data = np.zeros((600, 450, 15*15*4))\n",
    "        for i in range(15):\n",
    "            for j in range(15):\n",
    "                data[:,:,60*i+4*j:60*i+4*j+4] = info[i:i+600, j:j+450]\n",
    "        \n",
    "        mask = np.zeros((848, 680), np.uint8)\n",
    "        mask[107:707, 107:557] = (NN.GetInference(data.reshape(-1,15*15*4))[:,0]>0.5).reshape(600,450).astype(np.uint8)\n",
    "        \n",
    "        mask = cv2.dilate(mask, kernel, iterations=8)\n",
    "        mask = cv2.erode(mask, kernel, iterations=8)\n",
    "        \n",
    "        mask = mask[100:-100, 100:-100]\n",
    "        \n",
    "        png = np.zeros(img.shape[:2]+ (4,), dtype=np.uint8)\n",
    "        \"\"\"\n",
    "        png[:,:,:3] = img\n",
    "        png[:,:,3] = 255\n",
    "        cv2.imwrite(\"../../linker/Nokia/pngs/movie-D/frame%s.png\" %(str(counter).zfill(4)), png)\n",
    "        \n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "print(\"down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"../UCI/Is-skin/data-npy/label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2,4], [5,7,3]]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo = NN.hiddenLayerList[0].flow_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.lstsq(NN.hiddenLayerList[0].flow_out, NN.outputLayer.flow_in, rcond=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo -= foo.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo /= foo.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov = np.dot(foo.T, foo)/len(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.abs(cov).sum(axis=0)-1)/(7-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(cov, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(np.linalg.norm(np.linalg.lstsq(foo[:, np.arange(7)!=i], foo, rcond=0.000000001)[1]/len(foo)))\n",
    "\n",
    "NN.hiddenLayerList[0].linear.SetValue(NN.hiddenLayerList[0].linear.value[:, np.arange(7)!=0])\n",
    "NN.hiddenLayerList[0].bias.SetValue(NN.hiddenLayerList[0].bias.value[:, np.arange(7)!=0])\n",
    "NN.outputLayer.linear.SetValue(NN.outputLayer.linear.value[np.arange(7)!=0, :])\n",
    "NN.GetTrainLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.hiddenLayerList[0].linear.SetValue(NN.hiddenLayerList[0].linear.value[:, np.arange(7)!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.hiddenLayerList[0].bias.SetValue(NN.hiddenLayerList[0].bias.value[:, np.arange(7)!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.outputLayer.linear.SetValue(NN.outputLayer.linear.value[np.arange(7)!=0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.GetInference(NN.trainData.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.GetTrainLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo = np.random.normal(0, 1, (100000, 100))\n",
    "fooo = np.dot(foo, np.random.rand(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.linalg.lstsq(foo, fooo, rcond=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo = np.random.normal(1., 1., (100,3))\n",
    "#foo[:,2] = 0.5*foo[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRefine(foo, foo, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.linalg.lstsq(np.c_[foo[:,:2], np.ones(100)], foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((np.dot(np.c_[foo[:,:2], np.ones(100)], result[0]) - foo)**2).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
