{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsNone(target):\n",
    "    return (type(target) == type(None))\n",
    "\n",
    "def ArraySign(input_array):\n",
    "    # return +1, 0, -1 respect to positive, zero, negtive\n",
    "    return 1.*(input_array>0) - 1.*(input_array<0)\n",
    "\n",
    "def CutValue(input_array, cut_value):\n",
    "    output = np.abs(input_array)\n",
    "    output = ArraySign(input_array) * (output * (output < cut_value) + cut_value * (output >= cut_value))\n",
    "    return output\n",
    "\n",
    "def WeightedSum(input_array, weight):\n",
    "    try:\n",
    "        return (input_array.reshape(-1)*weight).sum()\n",
    "    except:\n",
    "        raise ValueError(\"weight should be an 1-d array with the same size with input_array\")\n",
    "\n",
    "def WeightedRow(input_array, weight):\n",
    "    try:\n",
    "        return input_array*weight.reshape(-1,1)\n",
    "    except:\n",
    "        raise ValueError(\"weight should be an 1-d array with the same length with first shape of input_array\")\n",
    "\n",
    "def OverPenalty(input_value, rate = 0.1, threshold=0.):\n",
    "    output = np.abs(input_value) - threshold\n",
    "    output *= (output > 0)\n",
    "    output *= rate * ArraySign(input_value)\n",
    "    return output\n",
    "\n",
    "def RowOperate(matrix, threshold = 0.000000000000001, large_element_alert=False):\n",
    "    # matrix : matrix with rows > columns\n",
    "    reduced_matrix = np.array(matrix)\n",
    "    filtered_matrix = np.array(matrix)\n",
    "    shape = matrix.shape # matrix size\n",
    "    mask = np.ones(shape)\n",
    "    pivots = -1*np.ones((min(shape)), dtype = np.int) # store pivots, (# of pivots) <= min(rows, columns)\n",
    "    for p in range(len(pivots)):\n",
    "        filtered_matrix = reduced_matrix * mask # filter\n",
    "        if np.abs(filtered_matrix).max() < threshold:\n",
    "            print(\"WARNING : input rows not independ for threshold %d when apply row operation\" %(threshold))\n",
    "            break\n",
    "        \n",
    "        pivot_row, pivot_col = np.unravel_index(np.abs(filtered_matrix).argmax(), shape) # pivot row, pivot column\n",
    "        reduced_matrix[pivot_row] /= reduced_matrix[pivot_row][pivot_col]\n",
    "        multi = np.array(reduced_matrix[:, pivot_col])\n",
    "        multi[pivot_row] = 0.\n",
    "        reduced_matrix -= np.dot(multi.reshape((-1, 1)), reduced_matrix[pivot_row].reshape((1, -1)))\n",
    "        mask[pivot_row] = 0.\n",
    "        mask[:, pivot_col] = 0.\n",
    "        pivots[pivot_row] = pivot_col # the column-index of pivot_row-th row is pivot_col\n",
    "    \n",
    "    reduced_matrix = reduced_matrix[pivots != -1,:]\n",
    "    pivots = pivots[pivots != -1]\n",
    "    if large_element_alert:\n",
    "        if np.abs(reduced_matrix).max() > 1.01:\n",
    "            print(\"WARNING : reduced matrix has large element %f\" %(np.abs(reduced_matrix).max()))\n",
    "        \n",
    "    \n",
    "    return reduced_matrix, pivots\n",
    "\n",
    "def LinearRefine(regressor, response, num_elimination=1, regularizer=0.):\n",
    "    if (regressor.ndim != 2) or (regressor.size == 0):\n",
    "        raise ValueError(\"regressor should be a non-empty numpy matrix\")\n",
    "    elif (response.ndim != 2) or (response.size == 0):\n",
    "        raise ValueError(\"response should be a non-empty numpy matrix\")\n",
    "    elif len(regressor) != len(response):\n",
    "        raise ValueError(\"len(regressor) != len(response)\")\n",
    "    \n",
    "    if regularizer < 0.:\n",
    "        regularizer = 0.\n",
    "        print(\"SetRegularizer error, regularizer must be non-negative, has been set to zero.\")\n",
    "    \n",
    "    num_var = regressor.shape[1]\n",
    "    regressor = np.append(regressor, np.ones((len(regressor), 1)), axis=1) # add bias\n",
    "    gram = np.dot(regressor.T, regressor)\n",
    "    gram += regularizer * regressor.shape[0] * np.identity(regressor.shape[1])\n",
    "    projected = np.dot(regressor.T, response)\n",
    "    is_leave = np.ones((gram.shape[0]), dtype=bool)\n",
    "    response_square = np.square(response).sum(axis=0)\n",
    "    for d in range(num_elimination):\n",
    "        square_err = np.inf * np.ones((gram.shape[0]), dtype=bool)\n",
    "        for i in range(num_var):\n",
    "            if is_leave[i]:\n",
    "                work_index = (is_leave * (np.arange(len(is_leave)) != i)).astype(bool)\n",
    "                coe = np.linalg.solve(gram[work_index][:, work_index], projected[work_index])\n",
    "                square_err[i] = (response_square\n",
    "                                 - (coe * projected[work_index]).sum(axis=0) \n",
    "                                 - regularizer * np.square(coe).sum(axis=0)\n",
    "                                ).sum()\n",
    "        \n",
    "        is_leave[np.argmin(square_err)] = False\n",
    "    \n",
    "    rms = square_err.min()/(regressor.shape[0] - is_leave.sum())\n",
    "    coe = np.linalg.solve(gram[is_leave][:, is_leave], projected[is_leave])\n",
    "    bias = coe[-1].reshape(1, -1)\n",
    "    return is_leave[:-1], coe[:-1], bias, rms\n",
    "\n",
    "def McmcNormal(points, drop_times = 10, mean=0., std=1.):\n",
    "    # Useing Markov chain Monte Carlo method to get a new point from normal distribution with given points\n",
    "    # each element is get from mean and std\n",
    "    output = np.random.normal(mean, std, points.shape[1:])\n",
    "    if drop_times > 1:\n",
    "        for t in range(1, drop_times):\n",
    "            candicate = np.random.normal(mean, std, points.shape[1:])\n",
    "            candicate_distance = np.sqrt(np.square(np.subtract(points, candicate)).sum(axis=tuple(np.arange(1, len(points.shape))))).min()\n",
    "            # distance of candicate to target\n",
    "            output_distance = np.sqrt(np.square(np.subtract(points, output)).sum(axis=tuple(np.arange(1, len(points.shape))))).min()\n",
    "            # distance of currently output to target\n",
    "            if np.random.rand()*output_distance < candicate_distance:\n",
    "                output = np.array(candicate)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def McmcColExtend(input_matrix, drop_times, extend_min=0.):\n",
    "    # return a matrix with a new col with same level by McmcNormal\n",
    "    input_matrix = input_matrix.T # transpose cols to rows\n",
    "    output = np.insert(input_matrix,\n",
    "                       len(input_matrix),\n",
    "                       McmcNormal(input_matrix, drop_times, 0, max(extend_min, np.sqrt(np.square(input_matrix).mean()))),\n",
    "                       axis=0\n",
    "                      ).T # transpose rows back to cols\n",
    "    return output\n",
    "\n",
    "# Data\n",
    "\n",
    "class Data():\n",
    "    def __init__(self, inputs=np.zeros((0,0)), labels=np.zeros((0,0)), weight = None):\n",
    "        self.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetData(self, inputs=np.zeros((0,0)), labels=np.zeros((0,0)), weight = None):\n",
    "        if len(inputs) != len(labels):\n",
    "            raise ValueError(\"num_datums error, #inputs != #labels.\")\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        if IsNone(weight):\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "        elif weight.shape != (len(inputs)):\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "            print(\"WARNING : weight shape error, set uniform weight.\")\n",
    "        elif weight.sum() <= 0:\n",
    "            self.weight = np.ones((inputs.shape[0]))\n",
    "            print(\"WARNING : get non-positive weight sum, set uniform weight.\")\n",
    "        else:\n",
    "            self.weight = weight\n",
    "        \n",
    "        self.weight /= self.weight.sum()\n",
    "    \n",
    "    def GetNumDatums(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def GetDatumSize(self):\n",
    "        # return size of input and label\n",
    "        return (self.inputs.shape[1], self.labels.shape[1])\n",
    "    \n",
    "    def Shuffle(self):\n",
    "        # shuffling datumds\n",
    "        new_index = np.arange(self.GetNumDatums())\n",
    "        np.random.shuffle(new_index)\n",
    "        self.inputs = self.inputs[new_index]\n",
    "        self.labels = self.labels[new_index]\n",
    "    \n",
    "    def IsClassification(self):\n",
    "        # cheaking if labels of this data is classification\n",
    "        # By cheaking:\n",
    "        # 1. labels have only two value : 0, 1\n",
    "        # 2. two or more classes\n",
    "        # 3. each datum has unique 1\n",
    "        output = ((self.labels == 0) + (self.labels == 1)).all()\n",
    "        output *= (self.labels.shape[1]>1)\n",
    "        output *= (self.labels.sum(axis=1)==1).all()\n",
    "        return output\n",
    "\n",
    "# Data end\n",
    "\n",
    "# VariableArray\n",
    "\n",
    "class VariableArray():\n",
    "    def __init__(self, size, cwise_step_initial=0.1):\n",
    "        self.value = np.random.normal(0., 1., size) # array value\n",
    "        self.total_deri = np.zeros(self.value.shape) # total derivative, used to descent\n",
    "        self.last_total_deri = np.zeros(self.value.shape) # last total derivative\n",
    "        self.moving = np.zeros(self.value.shape) # moving array\n",
    "        self.cwise_step = cwise_step_initial*np.ones(self.value.shape) # component-wise step\n",
    "        \n",
    "        self.regulariz_rate = 0.\n",
    "        self.regulariz_margin = 0.\n",
    "    \n",
    "    def SetValue(self, input_value, cwise_step_initial=0.1):\n",
    "        self.value = np.array(input_value) # array value\n",
    "        self.total_deri = np.zeros(self.value.shape) # total derivative, used to descent\n",
    "        self.last_total_deri = np.zeros(self.value.shape) # last total derivative\n",
    "        self.moving = np.zeros(self.value.shape) # moving array\n",
    "        self.cwise_step = cwise_step_initial*np.ones(self.value.shape) # component-wise step\n",
    "    \n",
    "    def SetDeri(self, input_value):\n",
    "        if input_value.shape != self.total_deri.shape:\n",
    "            raise ValueError(\"input_value shape error\")\n",
    "        \n",
    "        self.total_deri = np.array(input_value)\n",
    "    \n",
    "    def DeriModify(self, input_value):\n",
    "        if input_value.shape != self.total_deri.shape:\n",
    "            raise ValueError(\"input_value shape error\")\n",
    "        \n",
    "        self.total_deri += input_value\n",
    "    \n",
    "    def ZeroDeri(self):\n",
    "        self.total_deri *= 0\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        self.regulariz_rate = max(rate, 0.)\n",
    "        self.regulariz_margin = max(margin, 0.)\n",
    "    \n",
    "    def ResetCwiseStep(self, new_cwise_step):\n",
    "        self.cwise_step = new_cwise_step * np.ones(self.cwise_step.shape)\n",
    "    \n",
    "    def Regularize(self):\n",
    "        if self.regulariz_rate != 0:\n",
    "            self.total_deri += OverPenalty(self.value, self.regulariz_rate, self.regulariz_margin)\n",
    "    \n",
    "    def Descent(self, step=1., method=\"normal\", move_max=1.):\n",
    "        self.Regularize()\n",
    "        if method == \"normal\":\n",
    "            self.moving = self.total_deri * step\n",
    "            self.moving = -1*CutValue(self.moving, move_max)\n",
    "        elif method == \"Rprop\":\n",
    "            self.moving = ArraySign(self.total_deri)\n",
    "            self.movint_return = ArraySign(self.total_deri*self.last_total_deri)\n",
    "            self.cwise_step *= 1.2*(self.movint_return>0) + 1.*(self.movint_return==0) + 0.5*(self.movint_return<0)\n",
    "            self.cwise_step = CutValue(self.cwise_step, move_max)\n",
    "            self.moving *= -1*self.cwise_step\n",
    "        else:\n",
    "            raise ValueError(\"descent method error\")\n",
    "        \n",
    "        self.value += self.moving\n",
    "        \n",
    "        self.last_total_deri = np.array(self.total_deri)\n",
    "        self.ZeroDeri()\n",
    "\n",
    "# VariableArray end\n",
    "\n",
    "# Activation functions defined start\n",
    "\n",
    "class Identity():\n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return np.ones(flow_in.shape, dtype = np.float64)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Sigmoid():\n",
    "    def Forward(self, flow_in):\n",
    "        return expit(flow_in)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return expit(flow_in)*expit(-flow_in)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Hypertan():\n",
    "    def Forward(self, flow_in):\n",
    "        flow_in = CutValue(flow_in, 100)\n",
    "        return np.tanh(flow_in)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        flow_in = CutValue(flow_in, 100) # cut value out of [-100, 100] to 100, cosh(-100) = cosh(100)\n",
    "        return 1. / np.square(np.cosh(flow_in))\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class SoftSign():\n",
    "    def Forward(self, flow_in):\n",
    "        return ArraySign(flow_in)*(1. - 1./(np.abs(flow_in) + 1.))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1. / np.square(np.abs(flow_in) + 1.)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Relu():\n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in*(flow_in>0)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1.*(flow_in>0)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class LeakyRelu():\n",
    "    def __init__(self, alpha = 0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        return flow_in*(flow_in>0) + self.alpha*flow_in*(flow_in<0)\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return 1.*(flow_in>0) + self.alpha*(flow_in<0)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class SoftPlus():\n",
    "    def Forward(self, flow_in):\n",
    "        return np.log(1. + np.exp(flow_in))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return expit(flow_in)\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Selu():\n",
    "    def __init__(self):\n",
    "        self.ahpha = 1.05071\n",
    "        self.beta = 1.67326\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        return self.ahpha*(flow_in*(flow_in>=0) + self.beta*(np.exp(flow_in) - 1)*(flow_in<0))\n",
    "    \n",
    "    def Diff(self, flow_in):\n",
    "        return self.ahpha*(1.*(flow_in>=0) + self.beta*np.exp(flow_in)*(flow_in<0))\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        return self.Diff(flow_in) * back_flow\n",
    "\n",
    "class Softmax():\n",
    "    def Forward(self, flow_in):\n",
    "        output = flow_in - flow_in.max(axis=1).reshape(-1,1)\n",
    "        output = np.exp(output)\n",
    "        output /= output.sum(axis=1).reshape(-1,1)\n",
    "        return output\n",
    "    \n",
    "    def Backward(self, flow_in, back_flow):\n",
    "        flow_out = self.Forward(flow_in) # result of self.trans\n",
    "        return flow_out*back_flow - flow_out*((flow_out*back_flow).sum(axis=1).reshape(-1,1))\n",
    "\n",
    "# Activation functions defined end\n",
    "\n",
    "# Layer\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, num_unit, activationFunction):\n",
    "        if type(activationFunction) == type:\n",
    "            raise TypeError(\"activationFunction should be a class. eg: Use 'Sigmoid()', not 'Sigmoid'\")\n",
    "        \n",
    "        self.num_unit = num_unit\n",
    "        self.activationFunction = activationFunction\n",
    "        self.linear = VariableArray((0, self.num_unit)) # linear weights working before active function\n",
    "        self.bias = VariableArray((1, self.num_unit)) # bias working before active function\n",
    "        self.flow_in = np.zeros((0, self.num_unit))\n",
    "        self.flow_out = np.zeros((0, self.num_unit))\n",
    "    \n",
    "    def Forward(self, flow_in):\n",
    "        self.flow_in = np.dot(flow_in, self.linear.value) + self.bias.value\n",
    "        self.flow_out = self.activationFunction.Forward(self.flow_in)\n",
    "    \n",
    "    def Backward(self, back_flow, layer_source):\n",
    "        deri = self.activationFunction.Backward(self.flow_in, back_flow)\n",
    "        self.linear.DeriModify(np.dot(layer_source.T, deri))\n",
    "        self.bias.DeriModify(np.sum(deri, axis=0).reshape(1, -1))\n",
    "        deri = np.dot(deri, self.linear.value.T)\n",
    "        return deri\n",
    "    \n",
    "    def ZeroDeri(self):\n",
    "        self.linear.ZeroDeri()\n",
    "        self.bias.ZeroDeri()\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        self.linear.SetRegularizer(rate, margin)\n",
    "        self.bias.SetRegularizer(rate, margin)\n",
    "    \n",
    "    def Descent(self, step, method):\n",
    "        self.linear.Descent(step, method)\n",
    "        self.bias.Descent(step, method)\n",
    "    \n",
    "    def ResetCwiseStep(self, new_cwise_step):\n",
    "        self.linear.ResetCwiseStep(new_cwise_step)\n",
    "        self.bias.ResetCwiseStep(new_cwise_step)\n",
    "        \n",
    "    def GetPCVar(self, weight):\n",
    "        if IsNone(weight):\n",
    "            weight = np.ones((self.flow_out.shape[0])) / self.flow_out.shape[0]\n",
    "        \n",
    "        mean_flow_out = self.flow_out.mean(axis=0)\n",
    "        centered_flow_out = self.flow_out - mean_flow_out\n",
    "        cov = np.dot(centered_flow_out.T * weight, centered_flow_out) # covariance matrix\n",
    "        information= np.linalg.eigvalsh(cov) #information is sorted from small to large\n",
    "        return information\n",
    "    \n",
    "    def GetDimension(self):\n",
    "        return self.linear.value.size + self.bias.value.size\n",
    "\n",
    "# Layer end\n",
    "\n",
    "# Loss function\n",
    "\n",
    "class LossFunction():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "    \n",
    "    def SetMethod(self, method):\n",
    "        self.method = method\n",
    "    \n",
    "    def GetLoss(self, inference, target, weight):\n",
    "        if self.method == \"r2\":\n",
    "            output = WeightedSum(np.square(inference - target).sum(axis=1), weight)\n",
    "            output /= WeightedSum(np.square(target - target.mean(axis=0)).sum(axis=1), weight)\n",
    "        elif self.method == \"cross entropy\":\n",
    "            output = WeightedSum((-target*np.log(inference)).sum(axis=1), weight)\n",
    "        else:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def Backward(self, inference, target, weight):\n",
    "        if self.method == \"r2\":\n",
    "            output = WeightedRow(2.*(inference - target), weight)\n",
    "            output /= WeightedSum(np.square(target - target.mean(axis=0)).sum(axis=1), weight)\n",
    "        elif self.method == \"cross entropy\":\n",
    "            output = WeightedRow(-(target/inference), weight)\n",
    "        else:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "class Terminator():\n",
    "    def __init__(self, short_period = 5, long_period = 10, threshold = 0.):\n",
    "        try:\n",
    "            short_period = int(short_period)\n",
    "            long_period = int(long_period)\n",
    "        except:\n",
    "            raise ValueError(\"period should be a value, and will be transfer to int\")\n",
    "        \n",
    "        if short_period <= 0:\n",
    "            short_period = 1\n",
    "            print(\"WARNING : short_period <= 0, set 1\")\n",
    "        \n",
    "        if long_period <= short_period:\n",
    "            long_period = short_period + 1\n",
    "            print(\"WARNING : long_period <= short_period, set short_period + 1\")\n",
    "        \n",
    "        self.short_period = short_period\n",
    "        self.long_period = long_period\n",
    "        self.threshold = threshold\n",
    "        self.record = []\n",
    "    \n",
    "    def Reset(self, short_period, long_period, threshold = 0.):\n",
    "        try:\n",
    "            short_period = int(short_period)\n",
    "            long_period = int(long_period)\n",
    "        except:\n",
    "            raise ValueError(\"period should be a value, and will be transfer to int\")\n",
    "        \n",
    "        if short_period <= 0:\n",
    "            short_period = 1\n",
    "            print(\"WARNING : short_period <= 0, set 1\")\n",
    "        \n",
    "        if long_period <= short_period:\n",
    "            long_period = short_period + 1\n",
    "            print(\"WARNING : long_period <= short_period, set %d\" %(short_period + 1))\n",
    "        \n",
    "        self.short_period = short_period\n",
    "        self.long_period = long_period\n",
    "        self.threshold = threshold\n",
    "        self.record = []\n",
    "    \n",
    "    def Hit(self, input_value):\n",
    "        try:\n",
    "            input_value = float(input_value)\n",
    "        except:\n",
    "            raise ValueError(\"input_value should be a real value\")\n",
    "        \n",
    "        self.record = [input_value] + self.record[:self.long_period-1]\n",
    "        if len(self.record) == self.long_period:\n",
    "            return (np.mean(self.record[:self.short_period]) - self.threshold > np.mean(self.record))\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def Clear(self):\n",
    "        self.record = []\n",
    "\n",
    "class DogikoNeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.hiddenLayerList = []\n",
    "        self.outputFunction = None\n",
    "        self.outputLayer = None\n",
    "        self.lossFunction = None\n",
    "        self.trainData = Data()\n",
    "        self.validData = Data()\n",
    "        self.testData = Data()\n",
    "        self.regulariz_rate = 0.\n",
    "        self.regulariz_margin = 0.\n",
    "        self.has_build = False\n",
    "        self.hit_tolerance = 0.1\n",
    "        self.terminator = Terminator()\n",
    "    \n",
    "    def SetLossFunction(self, method):\n",
    "        if method not in [\"r2\", \"cross entropy\"]:\n",
    "            raise ValueError(\"loss function method should be 'r2', 'cross entropy'\")\n",
    "        \n",
    "        self.lossFunction = LossFunction(method)\n",
    "    \n",
    "    def SetRegularizer(self, rate, margin):\n",
    "        if rate < 0.:\n",
    "            print(\"WARNING : regulariz_rate error, get negative value, setting to 0.\")\n",
    "        \n",
    "        if margin < 0.:\n",
    "            print(\"WARNING : regulariz_margin error, get negative value, setting to 0.\")\n",
    "        \n",
    "        self.regulariz_rate = max(rate, 0.)\n",
    "        self.regulariz_margin = max(margin, 0.)\n",
    "        \n",
    "        if self.has_build:\n",
    "            for l in range(self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "            \n",
    "            self.outputLayer.SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "    \n",
    "    def SetTrainData(self, inputs, labels, weight = None):\n",
    "        if self.has_build:\n",
    "            if inputs.shape[1] != self.inputs_size:\n",
    "                raise ValueError(\"inputs size doesn't match with this model\")\n",
    "            \n",
    "            if labels.shape[1] != self.labels_size:\n",
    "                raise ValueError(\"labels size doesn't match with this model\")\n",
    "        \n",
    "        self.trainData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetValidData(self, inputs, labels, weight = None):\n",
    "        if self.has_build:\n",
    "            if inputs.shape[1] != self.inputs_size:\n",
    "                raise ValueError(\"inputs size doesn't match with this model\")\n",
    "            \n",
    "            if labels.shape[1] != self.labels_size:\n",
    "                raise ValueError(\"labels size doesn't match with this model\")\n",
    "        \n",
    "        self.validData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def SetTestData(self, inputs, labels, weight = None):\n",
    "        if self.has_build:\n",
    "            if inputs.shape[1] != self.inputs_size:\n",
    "                raise ValueError(\"inputs size doesn't match with this model\")\n",
    "            \n",
    "            if labels.shape[1] != self.labels_size:\n",
    "                raise ValueError(\"labels size doesn't match with this model\")\n",
    "        \n",
    "        self.testData.SetData(inputs, labels, weight)\n",
    "    \n",
    "    def AddHiddenLayer(self, num_unit, activationFunction):\n",
    "        self.hiddenLayerList.append(Layer(num_unit, activationFunction))\n",
    "        if self.has_build:\n",
    "            print(\"WARNING : a hidden layer added after build, please re-build model or set related value manually.\")\n",
    "    \n",
    "    def SetOutputFunction(self, activationFunction):\n",
    "        # units of output layer is fixed as same as label size\n",
    "        self.outputFunction = activationFunction\n",
    "        if self.has_build:\n",
    "            self.outputLayer.activationFunction = self.outputFunction\n",
    "    \n",
    "    def SetHitTolerance(self, tolerance):\n",
    "        try:\n",
    "            if float(tolerance) <= 0.:\n",
    "                print(\"setting tolerance failed, tolerance should be positive real value\")\n",
    "            \n",
    "            self.hit_tolerance = float(tolerance)\n",
    "        except:\n",
    "            print(\"setting tolerance failed, tolerance should be positive real value\")\n",
    "    \n",
    "    def SetTerminator(self, short_period, long_period, threshold = 0.):\n",
    "        self.terminator.Reset(short_period, long_period, threshold)\n",
    "        \n",
    "    def ClearTerminator(self):\n",
    "        self.terminator.Clear()\n",
    "    \n",
    "    def GetNumHiddenLayers(self):\n",
    "        return len(self.hiddenLayerList)\n",
    "    \n",
    "    def Build(self):\n",
    "        if IsNone(self.lossFunction):\n",
    "            raise ValueError(\"Set loss function before build.\")\n",
    "        \n",
    "        if self.lossFunction.method == \"cross entropy\":\n",
    "            if type(self.outputFunction) not in [Sigmoid, Softmax, SoftSign]:\n",
    "                print (\"WARNING : chosen loss function is cross entropy but the output of output layer function may out of (0, 1)\")\n",
    "            \n",
    "        \n",
    "        if IsNone(self.outputFunction):\n",
    "            self.outputFunction = Identity()\n",
    "            print (\"WARNING : doesn't set outputFunction before build, set Identity().\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[0],\n",
    "                    self.validData.GetDatumSize()[0],\n",
    "                    self.testData.GetDatumSize()[0]\n",
    "                   ])) == 1:\n",
    "            self.inputs_size = self.trainData.GetDatumSize()[0]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same inputs size\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[1],\n",
    "                    self.validData.GetDatumSize()[1],\n",
    "                    self.testData.GetDatumSize()[1]\n",
    "                   ])) == 1:\n",
    "            self.labels_size = self.trainData.GetDatumSize()[1]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same labels size\")\n",
    "        \n",
    "        # when hidden layer exist, set first layer value\n",
    "        if self.GetNumHiddenLayers() >0:\n",
    "            self.hiddenLayerList[0].linear.SetValue(np.random.normal(0.,\n",
    "                                                                     1.,\n",
    "                                                                     (self.inputs_size, self.hiddenLayerList[0].num_unit)\n",
    "                                                                    )\n",
    "                                                   )\n",
    "            self.hiddenLayerList[0].bias.SetValue(np.random.normal(0.,\n",
    "                                                                   1.,\n",
    "                                                                   (1, self.hiddenLayerList[0].num_unit)\n",
    "                                                                  )\n",
    "                                                 )\n",
    "            # normalize flow_in by modify layer variables\n",
    "            self.hiddenLayerList[0].Forward(self.trainData.inputs)\n",
    "            flow_in_mean = self.hiddenLayerList[0].flow_in.mean(axis=0)\n",
    "            flow_in_std = self.hiddenLayerList[0].flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "            self.hiddenLayerList[0].linear.value /= flow_in_std\n",
    "            self.hiddenLayerList[0].bias.value -= flow_in_mean\n",
    "            self.hiddenLayerList[0].bias.value /= flow_in_std\n",
    "            self.hiddenLayerList[0].Forward(self.trainData.inputs)\n",
    "            # when hidden layer num >= 2, set internal layer value\n",
    "            for l in range(1, self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].linear.SetValue(np.random.normal(0.,\n",
    "                                                                         1.,\n",
    "                                                                         (self.hiddenLayerList[l-1].num_unit,\n",
    "                                                                          self.hiddenLayerList[l].num_unit\n",
    "                                                                         )\n",
    "                                                                        )\n",
    "                                                       )\n",
    "                self.hiddenLayerList[l].bias.SetValue(np.random.normal(0.,\n",
    "                                                                       1.,\n",
    "                                                                       (1, self.hiddenLayerList[l].num_unit)\n",
    "                                                                      )\n",
    "                                                     )\n",
    "                # normalize flow_in by modify layer variables\n",
    "                self.hiddenLayerList[l].Forward(self.hiddenLayerList[l-1].flow_out)\n",
    "                flow_in_mean = self.hiddenLayerList[l].flow_in.mean(axis=0)\n",
    "                flow_in_std = self.hiddenLayerList[l].flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "                self.hiddenLayerList[l].linear.value /= flow_in_std\n",
    "                self.hiddenLayerList[l].bias.value -= flow_in_mean\n",
    "                self.hiddenLayerList[l].bias.value /= flow_in_std\n",
    "                self.hiddenLayerList[l].Forward(self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            # set output layer\n",
    "            self.outputLayer = Layer(self.labels_size, self.outputFunction)\n",
    "            self.outputLayer.linear.SetValue(np.random.normal(0.,\n",
    "                                                              1.,\n",
    "                                                              (self.hiddenLayerList[-1].num_unit,\n",
    "                                                               self.outputLayer.num_unit\n",
    "                                                              )\n",
    "                                                             )\n",
    "                                            )\n",
    "            # normalize flow_in by modify layer variables\n",
    "            self.outputLayer.Forward(self.hiddenLayerList[-1].flow_out)\n",
    "            flow_in_mean = self.outputLayer.flow_in.mean(axis=0)\n",
    "            flow_in_std = self.outputLayer.flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "            self.outputLayer.linear.value /= flow_in_std\n",
    "            self.outputLayer.bias.value -= flow_in_mean\n",
    "            self.outputLayer.bias.value /= flow_in_std\n",
    "            self.outputLayer.Forward(self.hiddenLayerList[-1].flow_out)\n",
    "            \n",
    "        else: # case no hiddenlayer\n",
    "            self.outputLayer.linear.SetValue(np.random.normal(0.,\n",
    "                                                              1.,\n",
    "                                                              (self.inputs_size,\n",
    "                                                               self.outputLayer.num_unit\n",
    "                                                              )\n",
    "                                                             )\n",
    "                                            )\n",
    "        \n",
    "            self.outputLayer.bias.SetValue(np.random.normal(0.,\n",
    "                                                            1.,\n",
    "                                                            (1, self.outputLayer.num_unit)\n",
    "                                                           )\n",
    "                                          )\n",
    "            # normalize flow out to fit labels by modify layer variables\n",
    "            self.outputLayer.Forward(self.trainData.inputs)\n",
    "            flow_in_mean = self.outputLayer.flow_in.mean(axis=0)\n",
    "            flow_in_std = self.outputLayer.flow_in.std(axis=0) + 0.000000001 # bias for prevent zero std\n",
    "            self.outputLayer.linear.value /= flow_in_std\n",
    "            self.outputLayer.bias.value -= flow_in_mean\n",
    "            self.outputLayer.bias.value /= flow_in_std\n",
    "            self.outputLayer.Forward(self.trainData.inputs)\n",
    "        \n",
    "        # Set regularizer\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            self.hiddenLayerList[l].SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "        \n",
    "        self.outputLayer.SetRegularizer(self.regulariz_rate, self.regulariz_margin)\n",
    "        \n",
    "        self.has_build = True\n",
    "    \n",
    "    def save_model(self, target_folder):\n",
    "        if type(target_folder) != str:\n",
    "            raise TypeError(\"target_folder should be a str\")\n",
    "        \n",
    "        while target_folder[-1] ==\"/\":\n",
    "            target_folder = target_folder[:-1]\n",
    "        \n",
    "        if not os.path.exists(target_folder):\n",
    "            os.makedirs(target_folder)\n",
    "        \n",
    "        target_folder += \"/\"\n",
    "        \n",
    "        info_dict = {}\n",
    "        info_dict[\"inputs size\"] = self.inputs_size\n",
    "        info_dict[\"labels size\"] = self.labels_size\n",
    "        info_dict[\"loss\"] = self.lossFunction.method\n",
    "        info_dict[\"regularizer\"] = {\"rate\" : self.regulariz_rate,\n",
    "                                    \"margin\" : self.regulariz_margin\n",
    "                                   }\n",
    "        info_dict[\"tolerance\"] = self.hit_tolerance\n",
    "        info_dict[\"num units\"] = []\n",
    "        info_dict[\"activation function\"] = []\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            info_dict[\"num units\"].append(self.hiddenLayerList[l].num_unit)\n",
    "            info_dict[\"activation function\"].append(self.hiddenLayerList[l].activationFunction)\n",
    "        \n",
    "        info_dict[\"output function\"] = self.outputFunction\n",
    "        info_dict[\"terminator\"] = self.terminator\n",
    "        np.save(target_folder + \"model_info.npy\", info_dict)\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            np.save(target_folder + \"L%d.npy\" %(l), self.hiddenLayerList[l].linear.value)\n",
    "            np.save(target_folder + \"B%d.npy\" %(l), self.hiddenLayerList[l].bias.value)\n",
    "        \n",
    "        np.save(target_folder + \"Lo.npy\" , self.outputLayer.linear.value)\n",
    "        np.save(target_folder + \"Bo.npy\" , self.outputLayer.bias.value)\n",
    "    \n",
    "    def load_model(self, target_folder):\n",
    "        if len(set([self.trainData.GetDatumSize()[0],\n",
    "                    self.validData.GetDatumSize()[0],\n",
    "                    self.testData.GetDatumSize()[0]\n",
    "                   ])) == 1:\n",
    "            self.inputs_size = self.trainData.GetDatumSize()[0]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same inputs size\")\n",
    "        \n",
    "        if len(set([self.trainData.GetDatumSize()[1],\n",
    "                    self.validData.GetDatumSize()[1],\n",
    "                    self.testData.GetDatumSize()[1]\n",
    "                   ])) == 1:\n",
    "            self.labels_size = self.trainData.GetDatumSize()[1]\n",
    "        else:\n",
    "            raise ValueError(\"train, valid, test data must have the same labels size\")\n",
    "        \n",
    "        if type(target_folder) != str:\n",
    "            raise TypeError(\"target_folder should be a str\")\n",
    "        \n",
    "        while target_folder[-1] ==\"/\":\n",
    "            target_folder = target_folder[:-1]\n",
    "        \n",
    "        target_folder += \"/\"\n",
    "        \n",
    "        self.has_build = False\n",
    "        \n",
    "        info_dict = np.load(target_folder + \"model_info.npy\").item()\n",
    "        if self.trainData.GetDatumSize() != (info_dict[\"inputs size\"], info_dict[\"labels size\"]):\n",
    "            raise ValueError(\"Datum size error, (inputs size, output size) for this model should be (%d, %d)\" %self.trainData.GetDatumSize())\n",
    "        \n",
    "        self.SetLossFunction(info_dict[\"loss\"])\n",
    "        self.SetRegularizer(info_dict[\"regularizer\"][\"rate\"], info_dict[\"regularizer\"][\"margin\"])\n",
    "        self.hit_tolerance = info_dict[\"tolerance\"]\n",
    "        \n",
    "        self.hiddenLayerList = []\n",
    "        for l in range(len(info_dict[\"num units\"])):\n",
    "            self.AddHiddenLayer(info_dict[\"num units\"][l], info_dict[\"activation function\"][l])\n",
    "        \n",
    "        self.SetOutputFunction(info_dict[\"output function\"])\n",
    "        self.terminator = info_dict[\"terminator\"]\n",
    "        self.Build()\n",
    "        \n",
    "        for l in range(len(info_dict[\"num units\"])):\n",
    "            self.hiddenLayerList[l].linear.value = np.load(target_folder + \"L%d.npy\" %(l))\n",
    "            self.hiddenLayerList[l].bias.value = np.load(target_folder + \"B%d.npy\" %(l))\n",
    "        \n",
    "        self.outputLayer.linear.value = np.load(target_folder + \"Lo.npy\")\n",
    "        self.outputLayer.bias.value = np.load(target_folder + \"Bo.npy\")\n",
    "    \n",
    "    def GetDimension(self):\n",
    "        output = self.outputLayer.GetDimension()\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            output += self.hiddenLayerList[l].GetDimension()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def GetInference(self, inputs):\n",
    "        if inputs.shape[1] != self.inputs_size:\n",
    "            raise ValueError(\"inputs size shoud be %d, get %d\" %(self.inputs_size, inputs.shape[1]))\n",
    "        \n",
    "        if self.GetNumHiddenLayers() > 0:\n",
    "            self.hiddenLayerList[0].Forward(inputs)\n",
    "            for l in range(1, self.GetNumHiddenLayers()):\n",
    "                self.hiddenLayerList[l].Forward(self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            self.outputLayer.Forward(self.hiddenLayerList[-1].flow_out)\n",
    "        else:\n",
    "            self.outputLayer.Forward(inputs)\n",
    "        \n",
    "        return self.outputLayer.flow_out\n",
    "    \n",
    "    def GetLoss(self, inference, target, weight = None):\n",
    "        if inference.shape != target.shape:\n",
    "            raise ValueError(\"shape between inference and target non-equal\")\n",
    "        \n",
    "        return self.lossFunction.GetLoss(inference, target, weight)\n",
    "    \n",
    "    def GetTrainLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.trainData.inputs), self.trainData.labels, self.trainData.weight)\n",
    "    \n",
    "    def GetValidLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.validData.inputs), self.validData.labels, self.validData.weight)\n",
    "    \n",
    "    def GetTestLoss(self):\n",
    "        return self.GetLoss(self.GetInference(self.testData.inputs), self.testData.labels, self.testData.weight)\n",
    "    \n",
    "    def GetAccuracy(self, inference, target, decimals=4, tolerance = 0.1):\n",
    "        # tolerance : for regression(r2) model, given a tolerance to verify hit or miss,\n",
    "        #             this variable is meaningless for classifycation\n",
    "        if self.lossFunction.method == \"cross entropy\":\n",
    "            output = (inference.argmax(axis=1) == target.argmax(axis=1)).mean()\n",
    "        elif self.lossFunction.method == \"r2\":\n",
    "            output = np.square(inference - target).sum(axis=1)\n",
    "            output = (output < tolerance).mean()\n",
    "        \n",
    "        return np.round(output, decimals)\n",
    "    \n",
    "    def GetTrainAccuracy(self, tolerance = None, decimals=4):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.trainData.inputs), self.trainData.labels, decimals, tolerance)\n",
    "    \n",
    "    def GetValidAccuracy(self, tolerance = None, decimals=4):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.validData.inputs), self.validData.labels, decimals, tolerance)\n",
    "    \n",
    "    def GetTestAccuracy(self, tolerance = None, decimals=4):\n",
    "        if IsNone(tolerance):\n",
    "            tolerance = self.hit_tolerance\n",
    "        \n",
    "        return self.GetAccuracy(self.GetInference(self.testData.inputs), self.testData.labels, decimals, tolerance)\n",
    "    \n",
    "    def Backward(self, inputs, labels, weight):\n",
    "        deri = self.lossFunction.Backward(self.GetInference(inputs), labels, weight)\n",
    "        if self.GetNumHiddenLayers() > 0:\n",
    "            deri = self.outputLayer.Backward(deri, self.hiddenLayerList[-1].flow_out)\n",
    "            for l in range(self.GetNumHiddenLayers()-1, 0, -1):\n",
    "                deri = self.hiddenLayerList[l].Backward(deri, self.hiddenLayerList[l-1].flow_out)\n",
    "            \n",
    "            deri = self.hiddenLayerList[0].Backward(deri, inputs)\n",
    "        else:\n",
    "            deri = self.outputLayer.Backward(deri, inputs)\n",
    "        \n",
    "    def ZeroDeri(self):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            deri = self.hiddenLayerList[l].ZeroDeri()\n",
    "        \n",
    "        deri = self.outputLayer.ZeroDeri()\n",
    "    \n",
    "    def ResetCwiseStep(self, new_cwise_step=0.1):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            deri = self.hiddenLayerList[l].ResetCwiseStep(new_cwise_step)\n",
    "        \n",
    "        deri = self.outputLayer.ResetCwiseStep(new_cwise_step)\n",
    "    \n",
    "    def Descent(self, step = 1., method = \"normal\"):\n",
    "        for l in range(self.GetNumHiddenLayers()):\n",
    "            self.hiddenLayerList[l].Descent(step, method)\n",
    "        \n",
    "        self.outputLayer.Descent(step, method)\n",
    "    \n",
    "    def BatchFit(self, batch_inputs, batch_labels, batch_weight, step = 1., method = \"normal\"):\n",
    "        batch_weight /= batch_weight.sum()\n",
    "        self.Backward(batch_inputs, batch_labels, batch_weight)\n",
    "        self.Descent(step, method)\n",
    "    \n",
    "    def EpochFit(self, batch_size = None, step = 1., method = \"normal\"):\n",
    "        if type(batch_size) == type(None):\n",
    "            self.BatchFit(self.trainData.inputs, self.trainData.labels, self.trainData.weight, step, method)\n",
    "        elif type(batch_size) == int:\n",
    "            if batch_size > 0:\n",
    "                for b in range(np.ceil(self.trainData.GetNumDatums()/ batch_size).astype(np.int)):\n",
    "                    self.BatchFit(self.trainData.inputs[b*batch_size: (b+1)*batch_size],\n",
    "                                  self.trainData.labels[b*batch_size: (b+1)*batch_size],\n",
    "                                  self.trainData.weight[b*batch_size: (b+1)*batch_size],\n",
    "                                  step,\n",
    "                                  method\n",
    "                                 )\n",
    "            else:\n",
    "                raise ValueError(\"batch_size should be positive int\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"batch_size should be positive int\")\n",
    "    \n",
    "    def Train(self, times, batch_size = None, step = 1., method = \"normal\", is_termination = False, shuffling = False):\n",
    "        self.terminator.Clear()\n",
    "        for t in range(times):\n",
    "            if shuffling:\n",
    "                self.trainData.Shuffle()\n",
    "            \n",
    "            self.EpochFit(batch_size, step, method)\n",
    "            if is_termination:\n",
    "                if self.terminator.Hit(10*np.log10(self.GetValidLoss() + 0.000000001)):\n",
    "                # 0.000000001, bias for prevent error when log(0)\n",
    "                    return t+1\n",
    "        \n",
    "        return times\n",
    "    \n",
    "    def AddUnit(self, layer_index, num_added = 1, output_linear_bound = 1., cwise_step_initial = 0.1, drop_times=5):\n",
    "        if layer_index not in range(self.GetNumHiddenLayers()):\n",
    "            raise ValueError(\"layer_index should be an int from 0 to (#layer-1) for hiddenlayer\")\n",
    "        \n",
    "        if type(num_added) != int:\n",
    "            raise ValueError(\"num_added should be positive int\")\n",
    "        elif num_added <= 0:\n",
    "            raise ValueError(\"num_added should be positive int\")\n",
    "        \n",
    "        try:\n",
    "            if output_linear_bound < 0.:\n",
    "                raise ValueError(\"output_weight_bound should be non-negative\")\n",
    "        except:\n",
    "            raise ValueError(\"output_weight_bound should a non-negative real value\")\n",
    "        \n",
    "        new_linear = self.hiddenLayerList[layer_index].linear.value\n",
    "        new_bias = self.hiddenLayerList[layer_index].bias.value\n",
    "        for u in range(num_added):\n",
    "            new_linear = McmcColExtend(new_linear, drop_times)\n",
    "            new_bias = McmcColExtend(new_bias, drop_times)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].linear.SetValue(new_linear, cwise_step_initial)\n",
    "        self.hiddenLayerList[layer_index].bias.SetValue(new_bias, cwise_step_initial)\n",
    "        \n",
    "        if layer_index < self.GetNumHiddenLayers() - 1: # if not final hiddenlayer\n",
    "            new_output_linear = self.hiddenLayerList[layer_index+1].linear.value\n",
    "            new_output_linear = np.append(new_output_linear,\n",
    "                                          output_linear_bound * (2 * np.random.rand(num_added, self.hiddenLayerList[layer_index+1].num_unit) - 1),\n",
    "                                          axis=0\n",
    "                                         )\n",
    "            self.hiddenLayerList[layer_index+1].linear.SetValue(new_output_linear, cwise_step_initial)\n",
    "        \n",
    "        else: # if final hiddenlayer\n",
    "            new_output_linear = self.outputLayer.linear.value\n",
    "            new_output_linear = np.append(new_output_linear,\n",
    "                                          output_linear_bound * (2 * np.random.rand(num_added, self.outputLayer.num_unit) - 1),\n",
    "                                          axis=0\n",
    "                                         )\n",
    "            self.outputLayer.linear.SetValue(new_output_linear, cwise_step_initial)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].num_unit += num_added\n",
    "    \n",
    "    def UnitsRefined(self, layer_index, reference_data = None, weight = None, method=\"remove\", threshold = 1):\n",
    "        if self.hiddenLayerList[layer_index].num_unit == 1:\n",
    "            print(\"WARNING : Units Refining failed, layer %d has only 1 unit, pass this process\"%(layer_index))\n",
    "            return None\n",
    "        \n",
    "        if type(method) != str:\n",
    "            raise ValueError(\"method must be a str, and equal to 'remain', 'remove', 'info' or 'info ratio'.\")\n",
    "        \n",
    "        if type(layer_index) != int:\n",
    "            raise TypeError(\"layer_index should be an int between 0 to (# hidden layers - 1)\")\n",
    "        elif (layer_index > self.GetNumHiddenLayers() - 1) or (layer_index < 0):\n",
    "            raise ValueError(\"layer_index should be an int between 0 to (# hidden layers - 1)\")\n",
    "        \n",
    "        if (type(threshold) == int) and (method == \"remove\"):\n",
    "            if (threshold > self.hiddenLayerList[layer_index].num_unit-1):\n",
    "                raise ValueError(\"int threshold error : removed #neuron should less than currently #neuron\")\n",
    "            elif (threshold < -self.hiddenLayerList[layer_index].num_unit) or (threshold==0):\n",
    "                return None\n",
    "                # do nothing if remove no #neuron (threshold=0) or want to remain #neuron more than currently\n",
    "            \n",
    "        elif ((threshold > 1) and (threshold < 0)):\n",
    "            raise ValueError(\"threshold : a value in (0, 1), or an nonzero int\")\n",
    "        \n",
    "        if IsNone(reference_data):\n",
    "            reference_data = self.trainData.inputs\n",
    "            weight = self.trainData.weight\n",
    "        \n",
    "        self.GetInference(reference_data)\n",
    "        \n",
    "        if IsNone(weight):\n",
    "            weight = np.ones(reference_data.shape[0])\n",
    "            weight /= weight.size\n",
    "        \n",
    "        info = self.hiddenLayerList[layer_index].GetPCVar(weight)\n",
    "        if method in [\"remove\", \"remain\"]:\n",
    "            if type(threshold) != int:\n",
    "                raise TypeError(\"when using method 'remove' or 'remain', threshold represent the num of units want to remove, should be an int\")\n",
    "            \n",
    "            if method == \"remove\":\n",
    "                if (threshold < 0) or (threshold >= len(info)):\n",
    "                    raise ValueError(\"when using method 'remove', threshold should be in [0, #units - 1]\")\n",
    "                \n",
    "                cut_index = threshold\n",
    "            else: # case \"remain\"\n",
    "                if (threshold <= 0) or (threshold > len(info)):\n",
    "                    raise ValueError(\"when using method 'remain', threshold should be in [1, #units]\")\n",
    "                \n",
    "                cut_index = self.hiddenLayerList[layer_index].num_unit - threshold\n",
    "        \n",
    "        elif method in [\"info\", \"info ratio\"]:\n",
    "            if method == \"info ratio\":\n",
    "                threshold *= info.sum()\n",
    "            \n",
    "            cut_index = self.hiddenLayerList[layer_index].num_unit - (info >= threshold).sum()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"method must be 'remain', 'remove', 'info' or 'info ratio'.\")\n",
    "        \n",
    "        if layer_index < self.GetNumHiddenLayers() - 1: # case not final hidden layer\n",
    "            units_remain, new_linear, new_bias, mse = LinearRefine(self.hiddenLayerList[layer_index].flow_out,\n",
    "                                                                   self.hiddenLayerList[layer_index+1].flow_in,\n",
    "                                                                   num_elimination=cut_index,\n",
    "                                                                   regularizer = 10**-8\n",
    "                                                                  )\n",
    "            self.hiddenLayerList[layer_index+1].linear.SetValue(new_linear)\n",
    "            self.hiddenLayerList[layer_index+1].bias.SetValue(new_bias)\n",
    "        else:\n",
    "            units_remain, new_linear, new_bias, mse = LinearRefine(self.hiddenLayerList[layer_index].flow_out,\n",
    "                                                                   self.outputLayer.flow_in,\n",
    "                                                                   num_elimination=cut_index,\n",
    "                                                                   regularizer = 10**-8\n",
    "                                                                  )\n",
    "            self.outputLayer.linear.SetValue(new_linear)\n",
    "            self.outputLayer.bias.SetValue(new_bias)\n",
    "        \n",
    "        self.hiddenLayerList[layer_index].linear.SetValue(self.hiddenLayerList[layer_index].linear.value[:, units_remain])\n",
    "        self.hiddenLayerList[layer_index].bias.SetValue(self.hiddenLayerList[layer_index].bias.value[:, units_remain])\n",
    "        self.hiddenLayerList[layer_index].num_unit -= cut_index\n",
    "    \n",
    "    def UnitsModify(self,\n",
    "                    layer_index,\n",
    "                    set_num_unit,\n",
    "                    reference_data = None,\n",
    "                    weight = None,\n",
    "                    output_linear_bound = 1.,\n",
    "                    cwise_step_initial = 0.1,\n",
    "                    drop_times = 5\n",
    "                   ):\n",
    "        if type(set_num_unit) != int:\n",
    "            raise ValueError(\"set_num_unit should be int\")\n",
    "        elif set_num_unit <= 0:\n",
    "            raise ValueError(\"set_num_unit should be positive\")\n",
    "        \n",
    "        num_diff = set_num_unit - self.hiddenLayerList[layer_index].num_unit\n",
    "        if num_diff < 0:\n",
    "            self.UnitsRefined(layer_index, reference_data, weight, \"remain\",\n",
    "                              threshold = set_num_unit\n",
    "                             )\n",
    "            return \"remove %d units\"%(-num_diff)\n",
    "        elif num_diff > 0:\n",
    "            self.AddUnit(layer_index, num_diff, output_linear_bound, cwise_step_initial, drop_times)\n",
    "            return \"add %d units\"%(num_diff)\n",
    "        else:\n",
    "            return \"nothing changed\"\n",
    "\"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient cheak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(0, 1, (1000,2))\n",
    "Y = np.dot(X, np.random.normal(0, 2, (2,1)))\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X, Y)\n",
    "NN.SetValidData(X, Y)\n",
    "NN.SetTestData(X, Y)\n",
    "NN.SetLossFunction(\"r2\")\n",
    "NN.SetRegularizer(0., 0.)\n",
    "NN.AddHiddenLayer(2, Identity())\n",
    "NN.AddHiddenLayer(2, Sigmoid())\n",
    "NN.AddHiddenLayer(2, Hypertan())\n",
    "NN.AddHiddenLayer(2, Relu())\n",
    "NN.AddHiddenLayer(2, LeakyRelu())\n",
    "NN.AddHiddenLayer(2, SoftPlus())\n",
    "NN.SetOutputFunction(Selu())\n",
    "NN.SetTerminator(10,20,-0.)\n",
    "NN.Build()\n",
    "\n",
    "step = 0.000001\n",
    "\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    for j in range(NN.hiddenLayerList[l].linear.value.shape[1]):\n",
    "        for i in range(NN.hiddenLayerList[l].linear.value.shape[0]):\n",
    "            b = NN.GetTrainLoss()\n",
    "            NN.ZeroDeri()\n",
    "            NN.Backward(X, Y, NN.trainData.weight)\n",
    "            partial_deri = NN.hiddenLayerList[l].linear.total_deri[i, j]\n",
    "            NN.hiddenLayerList[l].linear.value[i, j] += step\n",
    "            a = NN.GetTrainLoss()\n",
    "            if np.abs(((a-b)/step)/partial_deri - 1) > 0.001:\n",
    "                print(((a-b)/step)/partial_deri - 1)\n",
    "        \n",
    "        b = NN.GetTrainLoss()\n",
    "        NN.ZeroDeri()\n",
    "        NN.Backward(X, Y, NN.trainData.weight)\n",
    "        partial_deri = NN.hiddenLayerList[l].bias.total_deri[0, j]\n",
    "        NN.hiddenLayerList[l].bias.value[0, j] += step\n",
    "        a = NN.GetTrainLoss()\n",
    "        if np.abs(((a-b)/step)/partial_deri - 1) > 0.001:\n",
    "            print(((a-b)/step)/partial_deri - 1)\n",
    "\n",
    "for j in range(NN.outputLayer.linear.value.shape[1]):\n",
    "    for i in range(NN.outputLayer.linear.value.shape[0]):\n",
    "        b = NN.GetTrainLoss()\n",
    "        NN.ZeroDeri()\n",
    "        NN.Backward(X, Y, NN.trainData.weight)\n",
    "        partial_deri = NN.outputLayer.linear.total_deri[i, j]\n",
    "        NN.outputLayer.linear.value[i, j] += step\n",
    "        a = NN.GetTrainLoss()\n",
    "        if np.abs(((a-b)/step)/partial_deri - 1) > 0.001:\n",
    "            print(((a-b)/step)/partial_deri - 1)\n",
    "\n",
    "    b = NN.GetTrainLoss()\n",
    "    NN.ZeroDeri()\n",
    "    NN.Backward(X, Y, NN.trainData.weight)\n",
    "    partial_deri = NN.outputLayer.bias.total_deri[0, j]\n",
    "    NN.outputLayer.bias.value[0, j] += step\n",
    "    a = NN.GetTrainLoss()\n",
    "    if np.abs(((a-b)/step)/partial_deri - 1) > 0.001:\n",
    "        print(((a-b)/step)/partial_deri - 1)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ((np.arange(400)-200)/100).reshape(-1,1)\n",
    "Y = np.zeros((X.shape[0], 2))\n",
    "Y[:, :1] = 1*(X>-1.)\n",
    "Y[:, :1] *= 1*(X<1.)\n",
    "Y[:, 1:] = 1 - Y[:, :1]\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X, Y)\n",
    "NN.SetValidData(X, Y)\n",
    "NN.SetTestData(X, Y)\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.01, 3.)\n",
    "NN.AddHiddenLayer(10, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,20,-0.01)\n",
    "NN.Build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(NN.Train(times=500, method=\"Rprop\", is_termination=False), end = \", \")\n",
    "print(NN.hiddenLayerList[0].num_unit, NN.GetTrainLoss())\n",
    "\n",
    "for t in range(5):\n",
    "    NN.UnitsRefined(layer_index=0, method=\"info\", threshold=0.01)\n",
    "    print(NN.Train(times=500, method=\"Rprop\", is_termination=False), end = \", \")\n",
    "    print(NN.hiddenLayerList[0].num_unit, NN.GetTrainLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Odd or even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ((np.arange(800)-400)/100).reshape(-1,1)\n",
    "Y = np.zeros(X.shape)\n",
    "Y[:, :] = 1*((X%2)>=1.)\n",
    "\n",
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X, Y)\n",
    "NN.SetValidData(X, Y)\n",
    "NN.SetTestData(X, Y)\n",
    "NN.SetLossFunction(\"r2\")\n",
    "NN.SetRegularizer(0.001, 0.)\n",
    "NN.AddHiddenLayer(2, Hypertan())\n",
    "NN.SetOutputFunction(Sigmoid())\n",
    "NN.SetTerminator(10,30,-0.01)\n",
    "NN.Build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NN.Train(times=500, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "print(NN.hiddenLayerList[0].num_unit, \", \", NN.GetTrainLoss())\n",
    "\n",
    "for t in range(20):\n",
    "    NN.UnitsModify(layer_index=0, set_num_unit=7, output_linear_bound=0.01)\n",
    "    NN.UnitsModify(layer_index=0, set_num_unit=8, output_linear_bound=0.01)\n",
    "    print(NN.Train(times=500, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    print(NN.hiddenLayerList[0].num_unit, \", \", NN.GetTrainLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skew chessboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = 4*np.random.rand(20000,2) - 2\n",
    "Y = np.zeros((X.shape[0],2))\n",
    "Y[:, 0] = (((np.floor((X[:,0] + X[:,1])%2) + np.floor((X[:,0] - X[:,1])%2)) %2) == 1)\n",
    "Y[:, 1] = 1- Y[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:10000], Y[:10000])\n",
    "NN.SetValidData(X[10000:15000], Y[10000:15000])\n",
    "NN.SetTestData(X[15000:], Y[15000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 1.)\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.)\n",
    "\n",
    "times = 100\n",
    "record = np.zeros((times))\n",
    "for t in range(times):\n",
    "    NN.Build()\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "    record[t] = NN.GetTestAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record.sort()\n",
    "record[-25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat add-fit-kill-fit for fix number of units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:10000], Y[:10000])\n",
    "NN.SetValidData(X[10000:15000], Y[10000:15000])\n",
    "NN.SetTestData(X[15000:], Y[15000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.001, 2.)\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(5,10, -0.)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=500, batch_size=5000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(30):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"remove\", threshold=2)\n",
    "        NN.AddUnit(layer_index=l, num_added=2, output_linear_bound=0.001)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.01)\n",
    "    print(NN.Train(times=500, batch_size=5000, method=\"Rprop\", is_termination=True, shuffling=True), end = \", \")    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "I = NN.GetInference(X)[:, 0]\n",
    "plt.plot(X[:, 0][I>0.5], X[:, 1][I>0.5], \"bp\")\n",
    "plt.plot(X[:, 0][I<0.5], X[:, 1][I<0.5], \"rp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find suitable units by info ratio of layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:10000], Y[:10000])\n",
    "NN.SetValidData(X[10000:15000], Y[10000:15000])\n",
    "NN.SetTestData(X[15000:], Y[15000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 1.)\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.AddHiddenLayer(20, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=1000, batch_size=5000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(20):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.AddUnit(layer_index=l, num_added=1, output_linear_bound=0.01)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"info ratio\", threshold=0.001)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=1000, batch_size=10000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "I = NN.GetInference(X)[:, 0]\n",
    "plt.plot(X[:, 0][I>0.5], X[:, 1][I>0.5], \"bp\")\n",
    "plt.plot(X[:, 0][I<0.5], X[:, 1][I<0.5], \"rp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Is Skin\" on UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.load(\"../UCI/Is-skin/data-npy/data.npy\").astype(np.float64)\n",
    "Y = np.load(\"../UCI/Is-skin/data-npy/label.npy\").astype(np.float64)\n",
    "X *= 2/255\n",
    "X -= 1\n",
    "shuffle = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffle)\n",
    "X = X[shuffle]\n",
    "Y = Y[shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:150000], Y[:150000])\n",
    "NN.SetValidData(X[150000:200000], Y[150000:200000])\n",
    "NN.SetTestData(X[200000:], Y[200000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 0.)\n",
    "NN.AddHiddenLayer(2, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.01)\n",
    "for t in range(10):\n",
    "    NN.Build()\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat add-fit-kill-fit for fix number of units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:150000], Y[:150000])\n",
    "NN.SetValidData(X[150000:200000], Y[150000:200000])\n",
    "NN.SetTestData(X[200000:], Y[200000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 1.)\n",
    "NN.AddHiddenLayer(2, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(10):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.AddUnit(layer_index=l, num_added=1, output_linear_bound=0.01)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"remain\", threshold=2, enhance_alert=False)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find suitable units by info ratio of layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = DogikoNeuralNetwork()\n",
    "NN.SetTrainData(X[:150000], Y[:150000])\n",
    "NN.SetValidData(X[150000:200000], Y[150000:200000])\n",
    "NN.SetTestData(X[200000:], Y[200000:])\n",
    "NN.SetLossFunction(\"cross entropy\")\n",
    "NN.SetRegularizer(0.0001, 0.)\n",
    "NN.AddHiddenLayer(10, Hypertan())\n",
    "NN.SetOutputFunction(Softmax())\n",
    "NN.SetTerminator(10,30, -0.01)\n",
    "\n",
    "NN.Build()\n",
    "\n",
    "print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "for l in range(NN.GetNumHiddenLayers()):\n",
    "    print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "\n",
    "print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())\n",
    "\n",
    "for t in range(20):\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.AddUnit(layer_index=l, num_added=1, output_linear_bound=0.01)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        NN.UnitsRefined(layer_index=l, method=\"info ratio\", threshold=0.0001)\n",
    "    \n",
    "    NN.ResetCwiseStep(0.001)\n",
    "    print(NN.Train(times=2000, batch_size=50000, method=\"Rprop\", is_termination=True), end = \", \")\n",
    "    for l in range(NN.GetNumHiddenLayers()):\n",
    "        print(NN.hiddenLayerList[l].num_unit, end = \", \")\n",
    "    \n",
    "    print(NN.GetTrainAccuracy(), NN.GetValidAccuracy(), NN.GetTestAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
